<p data-pid="hHrGZ3UY"><b>大家多多点赞、转发和关注</b> <a class="member_mention" href="http://www.zhihu.com/people/fe459e428af0b5f8abb53f2f4f3c732c" data-hash="fe459e428af0b5f8abb53f2f4f3c732c" data-hovercard="p$b$fe459e428af0b5f8abb53f2f4f3c732c">@清川</a><b>，这样答主才有动力分享更多好文章！</b></p><h2>写在前面</h2><p data-pid="F93teLMQ">麻省理工的韩松实验室（<a href="http://link.zhihu.com/?target=https%3A//hanlab.mit.edu/" class=" wrap external" target="_blank" rel="nofollow noreferrer">MIT HAN Lab</a>），是端智能系统领域最为著名的研究组之一。2015年，韩松首次提出了神经网络的剪枝算法<sup data-text="Learning both Weights and Connections for Efficient Neural Network" data-url="https://proceedings.neurips.cc/paper/2015/file/ae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf" data-draft-node="inline" data-draft-type="reference" data-numero="1">[1]</sup>，掀起了模型压缩和端侧推理加速的研究热潮，直到2020年左右才陷入饱和。</p><p data-pid="KTru8kqW">5年时间里，学术界对模型压缩的讨论渐入佳境，方法从剪枝逐渐扩展到量化<sup data-text="Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding" data-url="https://arxiv.org/pdf/1510.00149.pdf" data-draft-node="inline" data-draft-type="reference" data-numero="2">[2]</sup><sup data-text="Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference" data-url="https://openaccess.thecvf.com/content_cvpr_2018/papers/Jacob_Quantization_and_Training_CVPR_2018_paper.pdf" data-draft-node="inline" data-draft-type="reference" data-numero="3">[3]</sup>、蒸馏<sup data-text="Distilling the Knowledge in a Neural Network" data-url="https://arxiv.org/pdf/1503.02531" data-draft-node="inline" data-draft-type="reference" data-numero="4">[4]</sup>、低秩分解<sup data-text="Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation" data-url="https://proceedings.neurips.cc/paper/2014/file/2afe4567e1bf64d32a5527244d104cea-Paper.pdf" data-draft-node="inline" data-draft-type="reference" data-numero="5">[5]</sup>、结构重参数化<sup data-text="ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks" data-url="https://openaccess.thecvf.com/content_ICCV_2019/papers/Ding_ACNet_Strengthening_the_Kernel_Skeletons_for_Powerful_CNN_via_Asymmetric_ICCV_2019_paper.pdf" data-draft-node="inline" data-draft-type="reference" data-numero="6">[6]</sup>、动态结构路由<sup data-text="Dynamic Neural Networks: A Survey" data-url="https://ieeexplore.ieee.org/iel7/34/4359286/09560049.pdf?casa_token=svXizAkcczIAAAAA:TLET2OzwnN5inrDME5qE9bHiGsc4QZ4zTaUECrgwCknHgxIeiMuW6Qk5Q_k8hhh4vlALk-IOn96O" data-draft-node="inline" data-draft-type="reference" data-numero="7">[7]</sup>等，模型的部署环境从最开始的智能手机逐渐发展到智能手表、手环，再到最后的DSP和ARM单片机<sup data-text="Squeezing Deep Learning into Mobile and Embedded Devices" data-url="https://akhilmathurs.github.io/papers/lane_pervasive2017.pdf" data-draft-node="inline" data-draft-type="reference" data-numero="8">[8]</sup>。</p><p data-pid="d3dYV1vw">现有的工作中，模型大都通过云上集中式训练或类似联邦学习的端云协同训练得到。其后，模型压缩考虑如何对现有模型进行改造，使其在端侧设备部署后能有更好的推断性能。此前我们始终认为端侧的资源不足以支撑训练过程：智能手机的资源已经是捉襟见肘，更何况是内存、硬盘资源极低的ARM微控制器。而下面要介绍的这篇文章则让这件事由不可能变为可能。</p><h2>On-Device Training under 256KB Memory</h2><p data-pid="rK_mZYSt">这篇文章发表在2022年的NeurIPS上。文章一公开就引起了大家的高度重视，在22年下学期我助教的TinyML课程上有同学尝试复现了这篇文章，23年春天我在组会上做了40分钟的相关报告。组会报告PPT<a href="http://link.zhihu.com/?target=https%3A//pan.baidu.com/s/14_DGAfqLpCn_Aeh5Q6C8Ug%3Fpwd%3Dr5ta" class=" wrap external" target="_blank" rel="nofollow noreferrer">链接</a>，提取码r5ta。其他传送门：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/2206.15472" class=" wrap external" target="_blank" rel="nofollow noreferrer">论文原文</a>、<a href="http://link.zhihu.com/?target=https%3A//tinytraining.mit.edu/" class=" wrap external" target="_blank" rel="nofollow noreferrer">官方网站</a>、<a href="http://link.zhihu.com/?target=https%3A//github.com/mit-han-lab/tiny-training" class=" wrap external" target="_blank" rel="nofollow noreferrer">源代码</a>、<a href="http://link.zhihu.com/?target=https%3A//tinytraining.mit.edu/assets/on-device-training.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">官方PPT</a>、<a href="http://link.zhihu.com/?target=https%3A//youtu.be/XaDCO8YtmBw" class=" wrap external" target="_blank" rel="nofollow noreferrer">Demo</a>、<a href="http://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DyboIhr_lamY" class=" wrap external" target="_blank" rel="nofollow noreferrer">讲解视频</a>。本文的介绍依照我自己的理解进行组织，加入了一些私货，可能和网上的翻译类、新闻类介绍不同。</p><h3>研究背景</h3><p data-pid="N7rfHKI1">IoT设备通常具有极低的硬件资源，比如320KB的SRAM和1MB的Flash存储，和云端数据中心的高性能显卡差了5个数量级。此前韩松实验室提出了MCUNetv1/v2，为单片机设备上的模型推理提供了支持。</p><ul><li data-pid="FqJOm_UD"><b>v1提出了TinyNAS</b>，一种两阶段的硬件资源感知的神经网络结构搜索算法。第一阶段，自动搜索神经网络空间来适配不同硬件的资源限制，找到可以满足硬件资源限制的最高精度的搜索空间；第二阶段，根据不同搜索空间中神经网络计算量的分布，选择特定单片机的自由搜索空间进行网络架构的搜索。此外，<b>v1还提出一个配套的内存高效推理库TinyEngine</b>。参考这篇更详细的讲解：<a href="https://zhuanlan.zhihu.com/p/423354176" class="internal" target="_blank">诸葛灬孔暗：微型机器学习时代已经到来了</a>。</li><li data-pid="vfjfBXyc"><b>v2提出了Patch-based推理</b>：在内存密集阶段，模型每次仅在特征图的一小块区域上进行计算，完成计算后释放内存缓冲区，从而有效降低峰值内存占用。这个方法也被用于本文的正向推理过程。详细介绍参考：<a href="https://zhuanlan.zhihu.com/p/428014053" class="internal" target="_blank">韩松团队提出MCUNetV2：解锁MCU端新纪录！</a></li></ul><h3>研究动机</h3><ol><li data-pid="z6jPtUo2">在IoT设备的局部环境中，模型需要<b>个性化微调</b>，同时由于局部环境的变化，需要<b>持续学习</b></li><li data-pid="wxidSOYC">现有的云侧训练方案会引发<b>隐私安全问题</b>，同时海量终端集中式训练会对服务器和网络链路造成<b>较大负载</b></li><li data-pid="dua0GgX1">神经网络的训练阶段比推断阶段<b>需要更多的内存</b>，是阻碍端侧训练的主要因素</li></ol><p data-pid="mdV4bTNe"><b>补充</b>：训练时我们需要保存的信息包括模型信息和缓存信息，前者包括模型参数、梯度值、一阶和二阶动量（对于使用动量的优化器而言），后者主要是求解梯度时所依赖的正向传播过程中各层的激活值。再乘上batchsize这个因子，我们可以估算训练时的内存开销理论值。实际值会由于临时区缓存和内存碎片而更大。参考<a href="https://zhuanlan.zhihu.com/p/520898642" class="internal" target="_blank">Maxliu：深度学习模型训练时的内存开销</a>。</p><h3>现有方法</h3><ol><li data-pid="J8j6X9gz">现有神经网络框架要么<b>完全缺乏端侧推理支持</b>，比如PyTorch、TensorFlow、JAX、MXNet等，要么只支持端侧推理<b>不支持端侧反向传播</b>，比如TVM、TF-Lite、NCNN等。同时，<b>单片机没有操作系统，缺乏支持现有框架的运行时环境</b>。</li><li data-pid="46uSIoUl">还有一类神经网络框架，通过序列化反序列化以及内存分页方法实现了微控制器上的训练，但<b>需要非常大的外部存储（硬盘）支持</b>。</li><li data-pid="pT7lrxGP">现有的低开销迁移学习训练方法只<b>重训练调节十分有限的神经网络结构</b>，比如只更新最后一层、只更新偏置值、只更新BN层，只更新一个平行的残差结构（韩松的学生蔡涵提出的Tiny Transfer Learning方法<sup data-text="TinyTL: Reduce Memory, Not Parameters for Efficient On-Device Learning" data-url="https://proceedings.neurips.cc/paper/2020/file/81f7acabd411274fcf65ce2070ed568a-Paper.pdf" data-draft-node="inline" data-draft-type="reference" data-numero="9">[9]</sup>）。这些方法只减少了参数量，没有考虑内存问题，也没有在端侧真正做实验。</li></ol><h3>核心设计</h3><p data-pid="5JmP27sE"><b>1 量化感知缩放（降低参数精度）</b></p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-0e670c7e718f717183b3b2bf2de975e9_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="1184" data-rawheight="342" class="origin_image zh-lightbox-thumb" width="1184" data-original="https://pic1.zhimg.com/v2-0e670c7e718f717183b3b2bf2de975e9_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1184'%20height='342'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1184" data-rawheight="342" class="origin_image zh-lightbox-thumb lazy" width="1184" data-original="https://pic1.zhimg.com/v2-0e670c7e718f717183b3b2bf2de975e9_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-0e670c7e718f717183b3b2bf2de975e9_720w.jpg?source=d16d100b"></figure><p data-pid="C6OxvWlx">本文希望<b>在低精度量化的计算图上直接进行训练</b>，降低参数和算子的精度从而降低训练开销<b>。</b></p><p data-pid="Cg131-X8">传统量化的目的是为了推断时期的加速，比如在量化感知训练中会使用一个伪量化计算图来仿真推断时的效果，这样可以将量化缩放的截断误差考虑进去，同时可以调整出一套合适的量化超参数。这种方法（1）在训练时引入批量归一化层，而在推断时没有，会造成准确率损失（2）使用浮点数算子，不能达到低开销训练的目的。所以作者另起炉灶，抛弃掉现有框架，直接在真实量化计算图上训练。</p><p data-pid="pLOAYc7R">实验表明，相比于浮点训练，<b>直接在真实量化计算图上训练会造成10%的准确率下降</b>。这是为什么呢？作者认为是量化导致梯度的计算出现偏差。文章计算了权重的范数与梯度范数的比值，发现存在数值上的漂移。下图的横轴代表的是神经网络参数的序号，总体上是weight与bias交替的。深蓝色线是浮点训练的效果，绿色的线是量化训练的效果。可以看出绿线普遍比蓝线高，而且出现了规律的抖动（zig-zag）。</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-98178263c8cfbfd5b543e49eb290394a_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="1006" data-rawheight="364" class="origin_image zh-lightbox-thumb" width="1006" data-original="https://picx.zhimg.com/v2-98178263c8cfbfd5b543e49eb290394a_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1006'%20height='364'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1006" data-rawheight="364" class="origin_image zh-lightbox-thumb lazy" width="1006" data-original="https://picx.zhimg.com/v2-98178263c8cfbfd5b543e49eb290394a_720w.jpg?source=d16d100b" data-actualsrc="https://pic1.zhimg.com/v2-98178263c8cfbfd5b543e49eb290394a_720w.jpg?source=d16d100b"></figure><p data-pid="NyZ40U8u">这种现象和量化算法本有的问题。这里我先介绍一下数值量化一般是怎么做的。以最基础的<b>非对称线性量化</b>为例：</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-54150e709e497af93fc3ada42a990090_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="647" data-rawheight="281" class="origin_image zh-lightbox-thumb" width="647" data-original="https://pica.zhimg.com/v2-54150e709e497af93fc3ada42a990090_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='647'%20height='281'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="647" data-rawheight="281" class="origin_image zh-lightbox-thumb lazy" width="647" data-original="https://pica.zhimg.com/v2-54150e709e497af93fc3ada42a990090_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-54150e709e497af93fc3ada42a990090_720w.jpg?source=d16d100b"></figure><p data-pid="e_HBcr_x">假如我们要将一个权重矩阵从浮点数映射到int8。首先统计浮点数值的分布范围，并取得浮点的最大值和最小值。有的时候会有过大或过小的离群值，所以这个边界会作为一个超参数去调节，满足绝大多数权重分布在两个边界之间。对于整数数轴，int8的范围从-128到+127，如果是uint8，那么范围就是从0到255。接下来计算一个放缩因子s，可以将实数域上的每一个值都映射到整数数轴上，那么显然</p><p data-pid="eYNUggxY"><img src="https://www.zhihu.com/equation?tex=s%3D%28q_%7Bmax%7D-q_%7Bmin%7D%29%2F%28r_%7Bmax%7D-r_%7Bmin%7D%29%5C%5C" alt="s=(q_{max}-q_{min})/(r_{max}-r_{min})\\" eeimg="1">注意这样直接放缩会导致某些值被映射到两个整数之间，我们使用四舍五入的方式将它们约束在整数值上。如果是对称量化，浮点数的0会刚好对应整数的0，坐标轴两侧范围大小相等，只需要乘上这个因子就完成了量化。但非对称量化考虑一种更一般的情况，将浮点数域上的0映射得到的整数称为零点，即zero point。根据图上的推导，易知：</p><p data-pid="X0-G6iHb"><img src="https://www.zhihu.com/equation?tex=z%3Dq_%7Bmin%7D-r_%7Bmin%7D%5Ccdot+s+%3D+q_%7Bmax%7D-r_%7Bmax%7D%5Ccdot+s%5C%5C" alt="z=q_{min}-r_{min}\cdot s = q_{max}-r_{max}\cdot s\\" eeimg="1">最后量化的完整公式为</p><p data-pid="U9wRUP-t"><img src="https://www.zhihu.com/equation?tex=q%3D%5Ctext%7Bround%7D%28r%5Ccdot+s%29+%2B+z%5C%5C" alt="q=\text{round}(r\cdot s) + z\\" eeimg="1">其中r为原始浮点数，q为量化后的结果。</p><p data-pid="27vXsnqa">接着说这篇论文， 它以一个简单的线性层的量化为例</p><p data-pid="HAgPQdNP"><img src="https://www.zhihu.com/equation?tex=%5Cbar%7By%7D_%7Bint8%7D%3D%5Ctext%7Bcast2int8%7D%5Bs_%7Bfp32%7D%5Ccdot%28%5Cbar+W_%7Bint8%7D%5Cbar+x_%7Bint8%7D%2B%5Cbar+b_%7Bint32%7D%29%5D%5C%5C" alt="\bar{y}_{int8}=\text{cast2int8}[s_{fp32}\cdot(\bar W_{int8}\bar x_{int8}+\bar b_{int32})]\\" eeimg="1">其中偏置值之所以是int32类型，是因为它在精度上要和Wx对齐。两个int8相乘的结果最大需要int16才能存下，又由于矩阵和向量相乘中存在对这个乘积的累加，所以需要int32才能存下。根据以上介绍，量化过程将浮点数权重值放大了很多倍，不考虑四舍五入以及零点的影响，有 <img src="https://www.zhihu.com/equation?tex=W%5Capprox+s_W%5Ccdot+%5Cbar+W%5C%5C" alt="W\approx s_W\cdot \bar W\\" eeimg="1"></p><p data-pid="gpF64eJN">注意这里的放缩因子和上面的s定义不同，是s的倒数，值小于1。W代表浮点数值，上面加一横的代表量化后的数值。根据梯度的链式法则</p><p data-pid="rSnIkxT8"><img src="https://www.zhihu.com/equation?tex=y%3DWx%2Bb%5CRightarrow+%5Cnabla+W%3Dx%5ET%5Cnabla+y%5C%5C" alt="y=Wx+b\Rightarrow \nabla W=x^T\nabla y\\" eeimg="1">又因为</p><p data-pid="PEvVL2ep"><img src="https://www.zhihu.com/equation?tex=y%5Capprox+s_W%5Cbar+Wx%2Bb%5CRightarrow+%5Cnabla+%5Cbar+W%5Capprox+s_Wx%5ET%5Cnabla+y%5C%5C" alt="y\approx s_W\bar Wx+b\Rightarrow \nabla \bar W\approx s_Wx^T\nabla y\\" eeimg="1">所以梯度被缩小了很多倍</p><p data-pid="mrsg_lE-"><img src="https://www.zhihu.com/equation?tex=%5Cnabla+%5Cbar+W%5Capprox+s_W+%5Cnabla+W%5C%5C" alt="\nabla \bar W\approx s_W \nabla W\\" eeimg="1">于是权重的范数除以梯度的范数的比值被放大了 <img src="https://www.zhihu.com/equation?tex=1%2Fs_W" alt="1/s_W" eeimg="1"> 的平方倍</p><p data-pid="dvDwbP56"><img src="https://www.zhihu.com/equation?tex=%5CVert+%5Cbar+W%5CVert%2F%5CVert+%5Cnabla+%5Cbar+W%5CVert+%5Capprox+s_W%5E%7B-2%7D+%5CVert+W%5CVert%2F%5CVert+%5Cnabla+W%5CVert%5C%5C" alt="\Vert \bar W\Vert/\Vert \nabla \bar W\Vert \approx s_W^{-2} \Vert W\Vert/\Vert \nabla W\Vert\\" eeimg="1">因此只需要在计算的梯度上将这个偏差乘回去即可</p><p data-pid="2IDUzQQC"><img src="https://www.zhihu.com/equation?tex=%5Cnabla+%5Cbar+W%27%3D%5Cnabla+%5Cbar++W%5Ccdot+s_W%5E%7B-2%7D%2C+%5C+%5Cnabla+%5Cbar+b%27%3D%5Cnabla+%5Cbar+b%5Ccdot+s_W%5E%7B-2%7D%5Ccdot+s_x%5E%7B-2%7D%5C%5C" alt="\nabla \bar W'=\nabla \bar  W\cdot s_W^{-2}, \ \nabla \bar b'=\nabla \bar b\cdot s_W^{-2}\cdot s_x^{-2}\\" eeimg="1">W和x同时受量化的影响产生了数值的缩放，要和Wx对齐，偏置值的梯度就要乘以两个因子。这也解释了为什么<b>量化训练的范数比图像出现了锯齿，因为偏置的数值放大比权重更严重</b>。完成对梯度的重缩放之后，可以看到下图中黄色曲线已经和浮点数训练时的曲线几乎重合。</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-70e1cbd087b4273bbb91b9c6d8c61a75_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="1457" data-rawheight="447" class="origin_image zh-lightbox-thumb" width="1457" data-original="https://pica.zhimg.com/v2-70e1cbd087b4273bbb91b9c6d8c61a75_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1457'%20height='447'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1457" data-rawheight="447" class="origin_image zh-lightbox-thumb lazy" width="1457" data-original="https://pica.zhimg.com/v2-70e1cbd087b4273bbb91b9c6d8c61a75_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-70e1cbd087b4273bbb91b9c6d8c61a75_720w.jpg?source=d16d100b"></figure><p data-pid="Sq5_QDh_">我们可以从对比实验中看出加入量化感知缩放（QAS）的优化方式能够得到和浮点数最接近的效果，甚至在某些数据集上能够超过浮点训练。</p><figure data-size="normal"><noscript><img src="https://pica.zhimg.com/v2-db8d8036339d316f6253ce2a3555c857_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="1685" data-rawheight="417" class="origin_image zh-lightbox-thumb" width="1685" data-original="https://pic1.zhimg.com/v2-db8d8036339d316f6253ce2a3555c857_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1685'%20height='417'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1685" data-rawheight="417" class="origin_image zh-lightbox-thumb lazy" width="1685" data-original="https://pic1.zhimg.com/v2-db8d8036339d316f6253ce2a3555c857_720w.jpg?source=d16d100b" data-actualsrc="https://pica.zhimg.com/v2-db8d8036339d316f6253ce2a3555c857_720w.jpg?source=d16d100b"></figure><p data-pid="IORpGmRJ"><b>2 稀疏参数更新（降低参数数量）</b></p><p data-pid="zaYjmq58">除了降低参数精度以外，本文通过三个角度降低参与训练的参数数量：<b>冻结某些网络层权重、冻结某些层的偏置值、冻结特定层的某些通道</b>。为了选择最合适的策略，本文把不同参数的重要性按照一种边际贡献进行建模。</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-9a93d3a70677a8cbda482305a7612fa8_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="1626" data-rawheight="338" class="origin_image zh-lightbox-thumb" width="1626" data-original="https://pica.zhimg.com/v2-9a93d3a70677a8cbda482305a7612fa8_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1626'%20height='338'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1626" data-rawheight="338" class="origin_image zh-lightbox-thumb lazy" width="1626" data-original="https://pica.zhimg.com/v2-9a93d3a70677a8cbda482305a7612fa8_720w.jpg?source=d16d100b" data-actualsrc="https://pic1.zhimg.com/v2-9a93d3a70677a8cbda482305a7612fa8_720w.jpg?source=d16d100b"></figure><p data-pid="QP4gPa1U"><b>对于偏置值的选择</b>：将只训练最后一层的效果作为基准，从最后一层开始依次加入k个偏置值进行训练，评估其对网络准确率的贡献，绘制出偏置值数量与累积贡献的关系曲线。在MobileNetv2上的结果如下图左侧所示，可以看出当数量增加到25时开始出现准确率的饱和。</p><p data-pid="UPY9-hiG"><b>对于权重值的选择</b>：将只训练所有的偏置值的效果作为基准，每次加入一层权重值，评估准确率的提升。如下图右侧所示，横轴是所加入的网络层的编号，不同颜色的曲线表示不同比例的通道冻结。可以看出，权重的位置越靠近网络的输出，对准确率的贡献越大。</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-d451e8fccfb8a82bdba700684398df91_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="1373" data-rawheight="309" class="origin_image zh-lightbox-thumb" width="1373" data-original="https://pic1.zhimg.com/v2-d451e8fccfb8a82bdba700684398df91_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1373'%20height='309'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1373" data-rawheight="309" class="origin_image zh-lightbox-thumb lazy" width="1373" data-original="https://pic1.zhimg.com/v2-d451e8fccfb8a82bdba700684398df91_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-d451e8fccfb8a82bdba700684398df91_720w.jpg?source=d16d100b"></figure><p data-pid="-O6asGCB">最后将所有的边际贡献累加得到一个优化的目标函数</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-186cd62f8bd5f9a71da45dab46fab5a6_720w.png?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="1249" data-rawheight="128" class="origin_image zh-lightbox-thumb" width="1249" data-original="https://picx.zhimg.com/v2-186cd62f8bd5f9a71da45dab46fab5a6_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1249'%20height='128'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1249" data-rawheight="128" class="origin_image zh-lightbox-thumb lazy" width="1249" data-original="https://picx.zhimg.com/v2-186cd62f8bd5f9a71da45dab46fab5a6_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-186cd62f8bd5f9a71da45dab46fab5a6_720w.png?source=d16d100b"></figure><p data-pid="qlKslGuy">在内存资源的约束下，动态的选择偏置值的数量、权重层的编号以及冻结通道的比例。文章使用<b>进化搜索算法</b>（类似遗传算法）对这个问题进行求解，附录中给出了与随机搜索的效果对比，说明了优化的有效性。</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-cc7c819ad209f9f70f93aa4e6658c9c0_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="1782" data-rawheight="512" class="origin_image zh-lightbox-thumb" width="1782" data-original="https://pica.zhimg.com/v2-cc7c819ad209f9f70f93aa4e6658c9c0_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1782'%20height='512'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1782" data-rawheight="512" class="origin_image zh-lightbox-thumb lazy" width="1782" data-original="https://pica.zhimg.com/v2-cc7c819ad209f9f70f93aa4e6658c9c0_720w.jpg?source=d16d100b" data-actualsrc="https://pic1.zhimg.com/v2-cc7c819ad209f9f70f93aa4e6658c9c0_720w.jpg?source=d16d100b"></figure><p data-pid="q2hGBhBv">上图是搜索算法结果的一个用例。可以看出网络不同层次对内存资源的消耗不是均匀的，而是呈现一种U形排布，<b>中间的网络层内存消耗最小，浅层和深层网络内存消耗较高</b>。19层以前的网络参数完全冻结，之后的偏置值全部参与训练。中间某些层的权重值使用全部通道，靠近输出的网络层使用更小的通道比例参与训练。算法搜索的结果与网络对内存的消耗规律是相吻合的。</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-cc99c5b8dc1b3f1c7bd5bcc36f713708_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="1590" data-rawheight="464" class="origin_image zh-lightbox-thumb" width="1590" data-original="https://picx.zhimg.com/v2-cc99c5b8dc1b3f1c7bd5bcc36f713708_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1590'%20height='464'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1590" data-rawheight="464" class="origin_image zh-lightbox-thumb lazy" width="1590" data-original="https://picx.zhimg.com/v2-cc99c5b8dc1b3f1c7bd5bcc36f713708_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-cc99c5b8dc1b3f1c7bd5bcc36f713708_720w.jpg?source=d16d100b"></figure><p data-pid="jqLdwhEA">上面每张图中蓝色曲线的第一个点代表只使用最后一层进行训练，最后一个点代表使用全部网络参数进行训练。可以看出红色曲线代表的稀疏参数更新方法达到了甚至比全量训练更好的效果，同时很大程度削减了峰值内存开销。</p><p data-pid="uqBYrkqg"><b>3 轻量训练引擎（底层框架支撑）</b></p><p data-pid="LE3EzBoy">传统的训练框架为了更好的动态性，使用运行时的动态计算图，牺牲了系统的高效性。运算中具有大量冗余的中间结果缓存，并且由于对语言环境的依赖，编译得到的二进制文件更加庞大。此外，传统训练框架中的算子不是面向低资源设备优化的，也不具备对稀疏参数更新的支持。</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-957c30e9ee7f6be1d46af39ddf8a5520_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="1688" data-rawheight="311" class="origin_image zh-lightbox-thumb" width="1688" data-original="https://pica.zhimg.com/v2-957c30e9ee7f6be1d46af39ddf8a5520_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1688'%20height='311'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1688" data-rawheight="311" class="origin_image zh-lightbox-thumb lazy" width="1688" data-original="https://pica.zhimg.com/v2-957c30e9ee7f6be1d46af39ddf8a5520_720w.jpg?source=d16d100b" data-actualsrc="https://pic1.zhimg.com/v2-957c30e9ee7f6be1d46af39ddf8a5520_720w.jpg?source=d16d100b"></figure><p data-pid="rbR4l1b0">本文首先将模型的反向传播过程记录下来，组成一个编译时的<b>静态计算图</b>。之后，考虑到由于稀疏参数更新，很多参数不参与训练，某些层的输出激活值不需要进行缓存。引擎将这部分<b>冗余计算从计算图中裁剪掉</b>。</p><p data-pid="0J2eG06p">然后，<b>对图中的算子进行等价重排</b>。传统的框架，例如Pytorch在反向传播时，先计算网络所有的梯度值，再统一进行梯度的更新，这就造成很多梯度对象在内存中驻留较长的时间。通过算子重排，可以让已经计算好的梯度及时进行更新，从而降低峰值内存消耗。最后，将计算图的中间表示IR编译成对应平台的<b>二进制机器码</b>，消除对编程语言和工具库的依赖关系。</p><p data-pid="5XjELyK5">如果对神经网络反向传播、自动计算梯度的过程不了解，可以参考我之前的文章：<a href="https://zhuanlan.zhihu.com/p/426087140" class="internal" target="_blank">深入浅出张量自动求导机制</a>。</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-6b0bf393f9251582c6f718e0a7359534_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="1140" data-rawheight="307" class="origin_image zh-lightbox-thumb" width="1140" data-original="https://picx.zhimg.com/v2-6b0bf393f9251582c6f718e0a7359534_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1140'%20height='307'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1140" data-rawheight="307" class="origin_image zh-lightbox-thumb lazy" width="1140" data-original="https://picx.zhimg.com/v2-6b0bf393f9251582c6f718e0a7359534_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-6b0bf393f9251582c6f718e0a7359534_720w.jpg?source=d16d100b"></figure><p data-pid="LjCVBr_W">在轻量训练引擎的支持下，量化感知缩放和稀疏参数更新的效果才真正在硬件上体现出来。上图左侧对比了稀疏参数更新和全量训练的内存消耗，中间对比了现有的TF-Lite框架和本文提出的TTE框架的内存消耗，右侧展示了算子重排对内存的优化效果。</p><h3>实验设置</h3><p data-pid="sZ4kpZlX">本文选用了MobileNetv2、ProxylessNAS、MCUNet三种网络结构，在ImageNet上进行预训练，并在8个不同的下游数据集上进行微调：Cars、CIFAR-10、CIFAR-100、CUB、Flowers、Food、Pets、VWW。训练的过程首先<b>在STM32F746（320KB SRAM，1MB Flash）上进行验证</b>，但之后的大规模实验在GPU上进行仿真完成。在VWW数据集上，系统的真实训练速度大致为1.7帧每秒，推断速度为3.2帧每秒，在100张样本上训练收敛需要5到10分钟。</p><h2>对这篇工作的评价</h2><h3><b>正面评价</b>：</h3><ul><li data-pid="blnIVMF5">在资源极低的IoT设备上首次实现了深度网络（40层）的大规模（微调多层权重）重训练。第一次实现量化计算图的直接训练，提出的量化感知缩放和稀疏参数更新方法原理简单，效果出色。</li></ul><h3><b>负面评价</b>：</h3><ul><li data-pid="W3MTOWs7">网络微调策略搜索时，为了确定边际效益需要数据参与，而这是轻量训练引擎编译优化的前置条件。搜索过程和编译优化过程耗时较长，内存开销较大，而文章没有指明这部分在什么地方进行，只说明了搜索过程在10分钟之内完成。</li><li data-pid="dJ7L-FXl">文章的研究动机是IoT设备的隐私性，然而这不是很符合实际情况。数据虽然在传感器附近产生，但是标注过程往往在更高一级的网络节点上。比较合理的假设是在网关节点进行本地训练，而网关节点的计算和存储能力未必有如文章中所说的那么弱。所以<b>极低资源设备的本地训练意义不是很大</b>，至少不像当初端侧训练的必要性那么强。</li></ul>