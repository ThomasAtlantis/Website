<p data-pid="fFE9rbZL">题目中其实描述了两个不同的问题，分别是参数初始化与训练样本集对于神经网络梯度的影响。</p><p data-pid="kr7buAIF">我所了解的讨论初始化的文章大多旨在<b>寻找一种合理的初始化方案来提升收敛速度与稳定性</b>，关注的是收敛时梯度的方差或者上界，没有看到有关相似度的建模。</p><p data-pid="7-utojkt">就像初始簇中心点会影响聚类算法的收敛过程，初始参数也会极大地影响神经网络的收敛过程。举最简单的例子，假如我们对所有神经元的权重使用相同的常数初始化，那么在反向传播时，每层所有的神经元都会计算出相同的梯度，因而模型不能学习到任何有效的知识。另外根据链式法则，浅层梯度近似于其后各层权重以及激活函数梯度的累乘，权重过小会导致梯度消失，过大会导致梯度爆炸，参考这里的<a href="https://link.zhihu.com/?target=https%3A//www.deeplearning.ai/ai-notes/initialization/index.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">演示</a>。尽管后续引入了ReLU激活函数、BN层、weight decay等方法调整梯度和权重，适当大小的初始化总是有益的。至于题目中所说的初始化不同就很模糊了，是同一分布下不同地采样还是不同的分布？我们需要约束一下讨论的范畴。</p><p data-pid="53vbxiP4">因为神经网络本身是非线性、非凸的，参数初始化相关的理论研究一般是基于理想假设，想办法稳定网络的某些性质或保证收敛时权重有界、梯度趋于0。例如最经典的文章<sup data-text="Understanding the difficulty of training deep feedforward neural networks" data-url="" data-draft-node="inline" data-draft-type="reference" data-numero="1">[1]</sup>就旨在稳定正向传播中的特征方差和反向传播中的梯度方差：假设激活函数在0附近趋近于 <img src="https://www.zhihu.com/equation?tex=f%28x%29%3Dx" alt="f(x)=x" eeimg="1"> ，那么对于第 <img src="https://www.zhihu.com/equation?tex=k" alt="k" eeimg="1"> 层的权重矩阵 <img src="https://www.zhihu.com/equation?tex=W" alt="W" eeimg="1"> ，正向传播相当于输入左乘一个 <img src="https://www.zhihu.com/equation?tex=W" alt="W" eeimg="1"> ，反向传播相当于上一层的梯度右乘一个 <img src="https://www.zhihu.com/equation?tex=W" alt="W" eeimg="1">。设 <img src="https://www.zhihu.com/equation?tex=W" alt="W" eeimg="1"> 为维度 <img src="https://www.zhihu.com/equation?tex=d_k%5Ctimes+d_%7Bk%2B1%7D" alt="d_k\times d_{k+1}" eeimg="1"> 且服从 <img src="https://www.zhihu.com/equation?tex=N%280%2C1%29" alt="N(0,1)" eeimg="1"> 的独立随机矩阵，用它左乘和右乘一个向量分别将该向量的方差扩大 <img src="https://www.zhihu.com/equation?tex=d_%7Bk%2B1%7D" alt="d_{k+1}" eeimg="1"> 和 <img src="https://www.zhihu.com/equation?tex=d_k" alt="d_k" eeimg="1"> 倍。因此想要维持特征和梯度的方差不扩大，最简单的方法就是让权重服从分布 <img src="https://www.zhihu.com/equation?tex=N%280%2C%5Csigma%5E2%29" alt="N(0,\sigma^2)" eeimg="1"> ，其中 <img src="https://www.zhihu.com/equation?tex=%5Csigma%3D%5Csqrt%7B2%2F%28d_k%2Bd_%7Bk%2B1%7D%29%7D" alt="\sigma=\sqrt{2/(d_k+d_{k+1})}" eeimg="1"> 。后续的研究<sup data-text="Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification" data-url="" data-draft-node="inline" data-draft-type="reference" data-numero="2">[2]</sup><sup data-text="Revise Saturated Activation Functions" data-url="" data-draft-node="inline" data-draft-type="reference" data-numero="3">[3]</sup>与此类似。</p><p data-pid="9MOdjdH7">题主也许可以假设一个函数族作为参数初始化，比如高斯分布，当 <img src="https://www.zhihu.com/equation?tex=%5Csigma" alt="\sigma" eeimg="1"> 变化 <img src="https://www.zhihu.com/equation?tex=%5CDelta+%5Csigma+" alt="\Delta \sigma " eeimg="1"> 时，将以上方差的推导替换成相似度的推导。除此之外，还有一些依赖训练集的参数初始化方法<sup data-text="GradInit: Learning to Initialize Neural Networks for Stable and Efficient Training" data-url="" data-draft-node="inline" data-draft-type="reference" data-numero="4">[4]</sup><sup data-text="Weight Initialization of Deep Neural Networks(DNNs) using Data Statistics" data-url="" data-draft-node="inline" data-draft-type="reference" data-numero="5">[5]</sup>，复用其他预训练模型参数作为初始参数的方法<sup data-text="Discriminability-Based Transfer between Neural Networks " data-url="" data-draft-node="inline" data-draft-type="reference" data-numero="6">[6]</sup><sup data-text="ModelKeeper: Accelerating DNN Training via Automated Training Warmup" data-url="" data-draft-node="inline" data-draft-type="reference" data-numero="7">[7]</sup>。</p><p data-pid="lZVThrRQ">至于不同的数据集对于训练后梯度的影响，其实也是我们组最近在思考的一个问题。如果测试数据与训练数据不同分布，在Meta Learning的文章中或许会有一些理论分析。如果新的数据来源于某个训练集，也许多任务学习研究梯度规范化（解决任务间梯度冲突）的文章中有所涉及。</p><p data-pid="BE-JzVkE">最后，可能还有一些领域与此相关——数据筛选和压缩。前者（Coreset Selection）研究如何从一堆数据中选出最有价值的子集进行训练以提升测试时的性能，后者（Data Condensation）研究如何将一堆数据压缩成少量数据，使训练后与全集上的性能相当。这些工作都涉及到对不同数据集训练效果的评估，其中不乏有从梯度稳定性的角度出发的，或许可以借鉴。</p><p data-pid="PWYvgVLC">因为我也不做这些方向，所以只希望这个回答能够抛砖引玉。</p>