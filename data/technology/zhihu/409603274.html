<p data-pid="N3roJ4r_">最近在关注联邦学习的通信与计算效率优化，读到谷歌Jakub Konecny的两件套挺有意思：</p><ul><li data-pid="FCvrInnc"><u><a href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1610.05492" class=" wrap external" target="_blank" rel="nofollow noreferrer">Federated learning: Strategies for improving communication efficiency</a></u></li><li data-pid="GDgNdNlO"><a href="https://zhuanlan.zhihu.com/p/409603274/%3C/u%3Eh%3Cu%3Ettp%3C/u%3Es%3Cu%3E://ar%3C/u%3Ex%3Cu%3Eiv%3C/u%3E.%3Cu%3Eorg/abs/1812.07210%3C/u%3E" class="internal" target="_blank">Expanding the reach of federated learning by reducing client resource requirements</a></li></ul><p data-pid="zLGUkJvn">第一篇文章给出的研究动机如下：实际生活中上传链路带宽往往比下载链路小很多，而且服务器广播给客户端的模型，可以通过模型压缩进一步减小数据量，这就使得两者的传输速度不平衡，于是这篇文章关注对上传瓶颈通信效率的优化。第二篇文章话风一转，说上传链路已经进行了优化，下载部分还未进行尝试，同时通信数据压缩对端上计算加速也没有帮助，于是对这两个方面又进行修补。真的是啥话都让你说了！论文读的越多越能感受到讲故事是有话术的！</p><p data-pid="qMnV0RiA">这两篇文章在对数据进行压缩时都尝试了基于概率的数值量化方法，前者对客户端上传的梯度更新进行量化，后者对下载的全局模型的数值进行量化。<b>基于概率的数值量化方法，英文原文是probabilistic quantization，是个常用的数值压缩手段</b>，网上好像很少提到。下面以梯度更新的数值量化为例记录一下原理。</p><p data-pid="KZqswCWD">设第 <img src="https://www.zhihu.com/equation?tex=t" alt="t" eeimg="1"> 轮第 <img src="https://www.zhihu.com/equation?tex=i" alt="i" eeimg="1"> 个客户端的梯度更新矩阵为 <img src="https://www.zhihu.com/equation?tex=H_t%5Ei%5Cin+%5Cmathbb%7BR%7D%5E%7Bd_1%5Ctimes+d_2%7D" alt="H_t^i\in \mathbb{R}^{d_1\times d_2}" eeimg="1">。将这个矩阵按照元素顺序展开成向量</p><p data-pid="6mpFcmqd"><img src="https://www.zhihu.com/equation?tex=h+%3D+%28h_1%2C%5Cdots%2Ch_%7Bd_1%5Ctimes+d_2%7D%29%3D%5Ctext%7Bvec%7D%28H_t%5Ei%29%5C%5C" alt="h = (h_1,\dots,h_{d_1\times d_2})=\text{vec}(H_t^i)\\" eeimg="1"> </p><p data-pid="WEGrmjvU">计算该向量中元素的最大值和最小值</p><p data-pid="HxLKBfi4"><img src="https://www.zhihu.com/equation?tex=h_%7Bmax%7D%3D%5Cmax_j+%5C+%28h_j%29%2C+%5Cquad+h_%7Bmin%7D%3D%5Cmin_j+%5C+%28h_j%29%5C%5C" alt="h_{max}=\max_j \ (h_j), \quad h_{min}=\min_j \ (h_j)\\" eeimg="1"> </p><p data-pid="IOgGMMKF">则向量1比特量化压缩后的元素值为</p><p data-pid="d6mQWa-D"><img src="https://www.zhihu.com/equation?tex=%5Cwidetilde%7Bh%7D_j%3D%5Cleft%5C%7B%5Cbegin%7Baligned%7D+%26h_%7Bmax%7D%2C+%26%5Ctext%7Bwith+probability+%7D+%5Cfrac%7Bh_j-h_%7Bmin%7D%7D%7Bh_%7Bmax%7D-h_%7Bmin%7D%7D%5C%5C+%26h_%7Bmin%7D%2C+%26%5Ctext%7Bwith+probability+%7D+%5Cfrac%7Bh_%7Bmax%7D-h_j%7D%7Bh_%7Bmax%7D-h_%7Bmin%7D%7D+%5Cend%7Baligned%7D%5Cright.%5C%5C" alt="\widetilde{h}_j=\left\{\begin{aligned} &amp;h_{max}, &amp;\text{with probability } \frac{h_j-h_{min}}{h_{max}-h_{min}}\\ &amp;h_{min}, &amp;\text{with probability } \frac{h_{max}-h_j}{h_{max}-h_{min}} \end{aligned}\right.\\" eeimg="1"> </p><p data-pid="HGhHeaL-">显然 <img src="https://www.zhihu.com/equation?tex=%5Ctilde%7Bh%7D" alt="\tilde{h}" eeimg="1"> 是 <img src="https://www.zhihu.com/equation?tex=h" alt="h" eeimg="1"> 的无偏估计，由全概率公式可得</p><p data-pid="JhfMI5cM"><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+E%5B%5Ctilde%7Bh%7D_j%5D%26%3D%5Cfrac%7Bh_j-h_%7Bmin%7D%7D%7Bh_%7Bmax%7D-h_%7Bmin%7D%7D%5Ccdot+h_%7Bmax%7D+%2B+%5Cfrac%7Bh_%7Bmax%7D-h_j%7D%7Bh_%7Bmax%7D-h_%7Bmin%7D%7D%5Ccdot+h_%7Bmin%7D%5C%5C+%26%3D%5Cfrac%7B1%7D%7Bh_%7Bmax%7D-h_%7Bmin%7D%7D%5Ccdot+%28h_%7Bmax%7Dh_j-h_%7Bmax%7Dh_%7Bmin%7D+%2B+h_%7Bmax%7Dh_%7Bmin%7D-h_%7Bmin%7Dh_j%29%5C%5C+%26%3D%5Cfrac%7Bh_%7Bmax%7D-h_%7Bmin%7D%7D%7Bh_%7Bmax%7D-h_%7Bmin%7D%7D%5Ccdot+h_j%3Dh_j+%5Cend%7Baligned%7D%5C%5C" alt="\begin{aligned} E[\tilde{h}_j]&amp;=\frac{h_j-h_{min}}{h_{max}-h_{min}}\cdot h_{max} + \frac{h_{max}-h_j}{h_{max}-h_{min}}\cdot h_{min}\\ &amp;=\frac{1}{h_{max}-h_{min}}\cdot (h_{max}h_j-h_{max}h_{min} + h_{max}h_{min}-h_{min}h_j)\\ &amp;=\frac{h_{max}-h_{min}}{h_{max}-h_{min}}\cdot h_j=h_j \end{aligned}\\" eeimg="1"></p><p data-pid="DOy3VO-W">推广到 <img src="https://www.zhihu.com/equation?tex=b" alt="b" eeimg="1"> 比特量化。先将区间 <img src="https://www.zhihu.com/equation?tex=%5Bh_%7Bmin%7D%2Ch_%7Bmax%7D%5D" alt="[h_{min},h_{max}]" eeimg="1"> 等分为 <img src="https://www.zhihu.com/equation?tex=2%5E%7Bb%7D" alt="2^{b}" eeimg="1"> 段，设 <img src="https://www.zhihu.com/equation?tex=h_j" alt="h_j" eeimg="1"> 的值落在其中某一段，该段的两个端点分别为 <img src="https://www.zhihu.com/equation?tex=h%5E%5Cprime" alt="h^\prime" eeimg="1"> 和 <img src="https://www.zhihu.com/equation?tex=h%5E%7B%5Cprime%5Cprime%7D" alt="h^{\prime\prime}" eeimg="1"> ，则接下来只需要用这两个值分别去替代上述公式中的 <img src="https://www.zhihu.com/equation?tex=h_%7Bmin%7D" alt="h_{min}" eeimg="1"> 和 <img src="https://www.zhihu.com/equation?tex=h_%7Bmax%7D" alt="h_{max}" eeimg="1"> 即可。除此之外论文中介绍了一种情况，会导致这种估计方法带来较大的误差：</p><blockquote data-pid="MnNotoWd">when max = 1 and min = −1 and most of values are 0, the 1-bit quantization will lead to a large error.</blockquote><p data-pid="RRPbT_gL">相反，如果原向量在各个维度上的数值分布比较均匀，效果将会最好。文中介绍了一种将数值分布变得均匀的神奇方法，那就是<b>对原向量右乘一个随机正交矩阵</b>。我们可以将这个正交矩阵提前规定好，在服务器端和客户端达成共识，那么在客户端上传更新的压缩过程之前先右乘这个矩阵，在服务器端解压之后再次右乘这个矩阵，根据正交原理，就可以把数据恢复出来。</p><blockquote data-pid="9lJU26Jf">We note that applying a random rotation on h before the quantization (multiplying h by a random orthogonal matrix) solves this issue.</blockquote><p data-pid="WgNLoXQi">接下来我们用Python实现一下1比特量化，复现一下文中提到的这种极端情况看看效果。首先我们从正态分布中随机采样生成一个向量，然后将+1和-1添加到向量中。为了达到绝大多数数值都接近0的效果，我们将正态分布的均值设为0，方差取一个很小的值。这样也可以从经验上保证 <img src="https://www.zhihu.com/equation?tex=h_%7Bmax%7D%3D%2B1" alt="h_{max}=+1" eeimg="1"> 并且 <img src="https://www.zhihu.com/equation?tex=h_%7Bmin%7D%3D-1" alt="h_{min}=-1" eeimg="1"> 。</p><div class="highlight"><pre><code class="language-python"><span></span><span class="n">h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.00001</span><span class="p">,</span> <span class="n">dimension</span> <span class="o">-</span> <span class="mi">2</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])])</span>
</code></pre></div><p data-pid="LaaF13Dr">其中dimension是h的维度。接下来实现1比特量化的功能：</p><div class="highlight"><pre><code class="language-python"><span></span><span class="k">def</span> <span class="nf">approximate_h</span><span class="p">(</span><span class="n">h</span><span class="p">):</span>
    <span class="n">approx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">h</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
    <span class="n">h_min</span><span class="p">,</span> <span class="n">h_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">h</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">h_i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">h</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">h_i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">approx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">h_max</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">approx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">h_min</span>
    <span class="k">return</span> <span class="n">approx</span>
</code></pre></div><p data-pid="JIyDp0Bg">为了能够看出无偏估计的效果，我们设置一个检验次数，即对h的每个元素进行多次量化估计，最终返回均值，用多次实验的均值去逼近期望。改造上面的函数：</p><div class="highlight"><pre><code class="language-python"><span></span><span class="k">def</span> <span class="nf">approximate_h</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">approx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">h</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
    <span class="n">h_min</span><span class="p">,</span> <span class="n">h_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">h</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">h_i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">h</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">h_i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">approx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">h_max</span> <span class="o">-</span> <span class="n">approx</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">approx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">h_min</span> <span class="o">-</span> <span class="n">approx</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">approx</span>
</code></pre></div><p data-pid="hlwsKwRF">这里均值更新的公式为：</p><p data-pid="o-t0duVC"><img src="https://www.zhihu.com/equation?tex=avg%5E%7Bt%2B1%7D%3D%5Cfrac%7Bt%5Ccdot+avg%5E%7Bt%7D%2Bh_j%5E%7Bt%2B1%7D%7D%7Bt%2B1%7D%3Davg%5E%7Bt%7D-%5Cfrac%7Bavg%5Et-h_j%5E%7Bt%2B1%7D%7D%7Bt%2B1%7D%5C%5C" alt="avg^{t+1}=\frac{t\cdot avg^{t}+h_j^{t+1}}{t+1}=avg^{t}-\frac{avg^t-h_j^{t+1}}{t+1}\\" eeimg="1"> </p><p data-pid="x0DoTDuy">我们可以通过scipy的ortho_group方法生成正交矩阵</p><div class="highlight"><pre><code class="language-python"><span></span><span class="n">o</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">ortho_group</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dimension</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div><p data-pid="fuUDeIlh">我们可以观察到右乘正交矩阵对于向量数值的影响，以维度为10为例，原始向量：</p><div class="highlight"><pre><code class="language-python"><span></span><span class="p">[</span><span class="o">-</span><span class="mf">2.82279401e-06</span> <span class="o">-</span><span class="mf">7.37954895e-07</span>  <span class="mf">1.05849056e-05</span> <span class="o">-</span><span class="mf">1.01327544e-05</span>
 <span class="o">-</span><span class="mf">7.15114114e-07</span>  <span class="mf">1.07201417e-05</span> <span class="o">-</span><span class="mf">1.01558722e-06</span> <span class="o">-</span><span class="mf">2.00573958e-05</span>
 <span class="o">-</span><span class="mf">1.00000000e+00</span>  <span class="mf">1.00000000e+00</span><span class="p">]</span>
</code></pre></div><p data-pid="vz-caaPc">右乘正交矩阵后的结果：</p><div class="highlight"><pre><code class="language-python"><span></span><span class="p">[</span><span class="o">-</span><span class="mf">0.15043699</span> <span class="o">-</span><span class="mf">0.25237392</span>  <span class="mf">0.25132182</span>  <span class="mf">0.2553367</span>   <span class="mf">0.36880325</span>  <span class="mf">0.52117845</span>
  <span class="mf">0.41356199</span>  <span class="mf">0.8316785</span>   <span class="mf">0.00980367</span> <span class="o">-</span><span class="mf">0.71753402</span><span class="p">]</span>
</code></pre></div><p data-pid="OWHrKrQL">效果十分明显啊，均匀了好多！接下来我们做重复试验，每次分别使用原始向量量化和正交右乘后的向量量化，以L1残差为误差评价指标，看看两者效果的差距。</p><p data-pid="OhCA5jWw"><img src="https://www.zhihu.com/equation?tex=Error%3D%5Csum_j+%7C%5Ctilde%7Bh%7D_j-h_j%7C%5C%5C" alt="Error=\sum_j |\tilde{h}_j-h_j|\\" eeimg="1"> </p><div class="highlight"><pre><code class="language-python"><span></span><span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">300</span><span class="p">):</span>
    <span class="n">o</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">ortho_group</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dimension</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.00001</span><span class="p">,</span> <span class="n">dimension</span> <span class="o">-</span> <span class="mi">2</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])])</span>
    <span class="n">a</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">h</span> <span class="o">-</span> <span class="n">approximate_h</span><span class="p">(</span><span class="n">h</span><span class="p">))))</span>
    <span class="n">b</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">h</span> <span class="o">@</span> <span class="n">o</span> <span class="o">-</span> <span class="n">approximate_h</span><span class="p">(</span><span class="n">h</span> <span class="o">@</span> <span class="n">o</span><span class="p">))))</span>
</code></pre></div><p data-pid="GmpBNgP9">绘制直方图统计误差在这300次试验中的分布。epoch是每次试验里，用来产生平均估计值的量化操作次数。下图中从上到下，这个次数依次为5、10、20、40、80次。</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-1a342007a291daebade8a57aa3e9fb60_720w.jpg?source=d16d100b" data-size="normal" data-rawwidth="1920" data-rawheight="841" class="origin_image zh-lightbox-thumb" width="1920" data-original="https://picx.zhimg.com/v2-1a342007a291daebade8a57aa3e9fb60_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1920'%20height='841'&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="1920" data-rawheight="841" class="origin_image zh-lightbox-thumb lazy" width="1920" data-original="https://picx.zhimg.com/v2-1a342007a291daebade8a57aa3e9fb60_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-1a342007a291daebade8a57aa3e9fb60_720w.jpg?source=d16d100b"><figcaption>epoch对原始向量与正交右乘后向量量化误差的影响</figcaption></figure><p data-pid="R4pXqAmR">可以看出，量化估计次数较少的情况下，使用正交矩阵右乘后的向量量化估计更准确。这一点很好理解，因为原始向量绝大多数元素趋近于零时，每个数值被分配为+1和-1的概率几乎相等，它们的真实值距离最大最小值很远，所以估计误差大；正交右乘后的向量数值更加集中，均值离最值更近一些。</p><p data-pid="MUxGK_ud">在量化估计操作重复多次后，原始趋近于零的元素的估计值在+1和-1两极摇摆，均值逐渐逼近期望，也就是真实值。这时由于原始向量的分布更加对称，所以更容易显示出统计规律。这就好比一枚质地均匀的硬币，不需要很多次试验，就可以较为精确的知道正反面概率都是0.5，但是对于那种一二三等奖分布不均匀的幸运大转盘，就需要很多次试验才能知道每个奖项区域所占的比例。</p><p data-pid="Hmssf3lk">接下来我们固定量化次数为10次，仍然进行300次试验，但改变向量的维度数。下图中向量的维度从上到下分别为5、10、20、40、80。</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-a82471f8cf32decb1f3f18aace50a3f4_720w.jpg?source=d16d100b" data-size="normal" data-rawwidth="1920" data-rawheight="687" class="origin_image zh-lightbox-thumb" width="1920" data-original="https://pic1.zhimg.com/v2-a82471f8cf32decb1f3f18aace50a3f4_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1920'%20height='687'&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="1920" data-rawheight="687" class="origin_image zh-lightbox-thumb lazy" width="1920" data-original="https://pic1.zhimg.com/v2-a82471f8cf32decb1f3f18aace50a3f4_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-a82471f8cf32decb1f3f18aace50a3f4_720w.jpg?source=d16d100b"><figcaption>dimension对原始向量与正交右乘后向量量化误差的影响</figcaption></figure><p data-pid="Hb9fdaXk">可以看出向量的维度数越高，正交右乘带来的改善越明显。换句话说，提高向量的维度数可以延后epoch升高时正交右乘处于劣势的转折点。在实际的联邦学习中，量化epoch可以理解为上传更新的频率，在一个局部时间内，本地的更新变化不大看做不变的话，上传了的多次量化结果就可以看做epoch，这注定是一个不大的值。同时由于实际神经网络的参数矩阵不会是100这么小的维度，所以从上述实验规律来看，epoch很小、dimension很大，右乘正交矩阵确实可以让量化估计变得更准。这里是以1比特量化为例，实际上也可以用更多的存储位数，那么也必然使估计的粒度更小，从而估计的很准。</p><p data-pid="OYw3MlTX">这种依概率量化的方式不是联邦学习的独创，这也是为什么这几篇文章在我看来目前意义不大。联邦学习里通信效率的优化是个老生常谈的问题，而这几篇文章给出的解决方法没有联邦学习场景下的特殊性（虽然它可能要论证收敛性），是一般的数据压缩方法。记录这篇文章纯属觉得好玩儿，大家感兴趣也可以去看看文章里其他的压缩方法，比如强制矩阵低秩和随机掩码，也都挺有意思。</p>