<h2>通俗理解</h2><p data-pid="JGug3s3y">主成分分析（即PCA，Principal Component Analysis）是一种常用的<b>无监督降维方法</b>。既然讨论降维，一定是假设原始数据的信息表示存在冗余，并且PCA可以甄别和摒弃这种冗余。</p><p data-pid="kpr-6W0Q">首先，数据在采集时<b>很难保证各特征维度完全独立</b>。比如，当我们想按照大小划分一些西瓜，却并不了解圆周率。我们切开西瓜，既测量了直径又测量了周长，但二者任取其一都不会损失信息。</p><p data-pid="qqvg6Z4l">其次，数据的<b>某些特征在样本间存在很大分野，而有些特征则模棱两可</b>。比如，我们想把西瓜分成沙瓤的和脆瓤的，测了水分、糖分、直径和颜色。众所周知，从水分和糖分上已经基本可以区分两种西瓜，那么直径、颜色这种高度重叠的特征就相对冗余。</p><p data-pid="itYe7nct">主成分是什么？是指<b>对原始数据进行投影变换得到的一系列正交向量</b>。把原始数据的各个维度想象成坐标轴。一开始，这些坐标轴不仅没有相互垂直，而且有些轴上数据挤在一起。PCA要做的就是把这些轴重新规划一下，旋转、拉伸，把没必要的轴拍扁。</p><p data-pid="j_vHzcHv">新的维度之间互不相关，都是数据信息的有效组成成分，按照数据点在轴上分布的方差降序排列，就是主成分。<b>方差越大，就越容易划分，或者说信息量越大，那就越主要。</b></p><h2>原理推导</h2><p data-pid="qoS3XKiG">我们观测到的数据 <img src="https://www.zhihu.com/equation?tex=x_1%2Cx_2%2C%5Cdots%2C+x_N" alt="x_1,x_2,\dots, x_N" eeimg="1"> 具有原始的特征空间 <img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BR%7D%5Ep" alt="\mathbb{R}^p" eeimg="1"> （即 <img src="https://www.zhihu.com/equation?tex=x_i" alt="x_i" eeimg="1"> 是一个 <img src="https://www.zhihu.com/equation?tex=p" alt="p" eeimg="1"> 维列向量）。现在尝试用更紧致的 <img src="https://www.zhihu.com/equation?tex=q" alt="q" eeimg="1"> 维（ <img src="https://www.zhihu.com/equation?tex=q%3Cp" alt="q&lt;p" eeimg="1"> ）向量 <img src="https://www.zhihu.com/equation?tex=%5Clambda+_i" alt="\lambda _i" eeimg="1"> 来表示原始数据 <img src="https://www.zhihu.com/equation?tex=x_i" alt="x_i" eeimg="1"> ，然后用线性模型 <img src="https://www.zhihu.com/equation?tex=f%3A%5Cmathbb%7BR%7D%5Eq%5Crightarrow+%5Cmathbb%7BR%7D%5Ep" alt="f:\mathbb{R}^q\rightarrow \mathbb{R}^p" eeimg="1"> 将其“解压”（重建）回去：</p><p data-pid="04Jdl6Y8"><img src="https://www.zhihu.com/equation?tex=%5Chat+x_i%3Df%28%5Clambda_i%29%3D%5Cmu+%2B+V_q+%5Clambda_i%5C%5C" alt="\hat x_i=f(\lambda_i)=\mu + V_q \lambda_i\\" eeimg="1">其中，<img src="https://www.zhihu.com/equation?tex=%5Cmu" alt="\mu" eeimg="1"> 表示原始数据在特征空间中的位置。 <img src="https://www.zhihu.com/equation?tex=V_q" alt="V_q" eeimg="1"> 是 <img src="https://www.zhihu.com/equation?tex=p%5Ctimes+q" alt="p\times q" eeimg="1"> 维矩阵，由 <img src="https://www.zhihu.com/equation?tex=q" alt="q" eeimg="1"> 个单位正交的 <img src="https://www.zhihu.com/equation?tex=p" alt="p" eeimg="1"> 维向量组成这意味着 <img src="https://www.zhihu.com/equation?tex=V_q" alt="V_q" eeimg="1"> 的任意两个不同的列向量内积为0，相同的列向量内积为1（ <img src="https://www.zhihu.com/equation?tex=V_q%5ETV_q%3DI" alt="V_q^TV_q=I" eeimg="1"> ）。这 <img src="https://www.zhihu.com/equation?tex=q" alt="q" eeimg="1"> 个向量组成了 <img src="https://www.zhihu.com/equation?tex=q" alt="q" eeimg="1"> 个轴，而 <img src="https://www.zhihu.com/equation?tex=%5Clambda_i" alt="\lambda_i" eeimg="1"> 则是新坐标系下的点的坐标。</p><p data-pid="OLexPS_R"><b>为了减小信息量的损失，最朴素的想法是要求重建后的 <img src="https://www.zhihu.com/equation?tex=%5Chat+x_i" alt="\hat x_i" eeimg="1"> 和原始 <img src="https://www.zhihu.com/equation?tex=x_i" alt="x_i" eeimg="1"> 最接近</b>。于是最小化重建误差：</p><p data-pid="lwzlY1dz"><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cmin+%5C+g%28%5Cmu%2CV_q%2C%5Clambda%29+%26%3D+%5Csum_%7Bi%3D1%7D%5EN+%5CVert+x_i+-+%5Cmu+-+V_q+%5Clambda_i+%5CVert+%5E2+%5C%5C+%26%3D+%5Csum_%7Bi%3D1%7D%5EN%28x_i+-+%5Cmu+-+V_q+%5Clambda_i%29%5ET+%28x_i+-+%5Cmu+-+V_q+%5Clambda_i%29%5Cend%7Balign%2A%7D%5C%5C" alt="\begin{align*} \min \ g(\mu,V_q,\lambda) &amp;= \sum_{i=1}^N \Vert x_i - \mu - V_q \lambda_i \Vert ^2 \\ &amp;= \sum_{i=1}^N(x_i - \mu - V_q \lambda_i)^T (x_i - \mu - V_q \lambda_i)\end{align*}\\" eeimg="1"></p><p data-pid="PoiUdW1I">其中 <img src="https://www.zhihu.com/equation?tex=x" alt="x" eeimg="1"> 是给定的，其余的是优化中的决策变量。由于极值处各变量偏导为0：</p><p data-pid="tOJcatE-"><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bcases%7D+%5Cfrac%7B%5Cpartial+g%7D%7B%5Cpartial+%5Cmu%7D+%3D+-2%5Csum_%7Bi%3D1%7D%5EN+%28x_i-%5Cmu-V_q%5Clambda_i%29%3D0%5C%5C+%5Cfrac%7B%5Cpartial+g%7D%7B%5Cpartial+%5Clambda_i%7D%3D-2V_q%5ET%28x_i-%5Cmu-V_q%5Clambda_i%29%3D0+%5Cend%7Bcases%7D%5C%5C" alt="\begin{cases} \frac{\partial g}{\partial \mu} = -2\sum_{i=1}^N (x_i-\mu-V_q\lambda_i)=0\\ \frac{\partial g}{\partial \lambda_i}=-2V_q^T(x_i-\mu-V_q\lambda_i)=0 \end{cases}\\" eeimg="1"></p><p data-pid="VmTqmhs6">用 <img src="https://www.zhihu.com/equation?tex=%5Cbar+x%3D%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5EN+x_i" alt="\bar x=\frac{1}{N}\sum_{i=1}^N x_i" eeimg="1"> 表示观测数据的均值，得到：</p><p data-pid="Jz6rpkRW"><img src="https://www.zhihu.com/equation?tex=%28I-V_qV_q%5ET%29%28%5Cbar+x+-+%5Cmu%29%3D0%5C%5C" alt="(I-V_qV_q^T)(\bar x - \mu)=0\\" eeimg="1"></p><p data-pid="Woyx6BKj">显然 <img src="https://www.zhihu.com/equation?tex=%5Cmu+%3D+%5Cbar+x" alt="\mu = \bar x" eeimg="1"> 是该方程的一个解，即选择观测数据的均值作为原始特征空间中新坐标系的原点，进而可以得到新的坐标 <img src="https://www.zhihu.com/equation?tex=%5Clambda_i+%3D+V_q%5ET%28x_i+-+%5Cbar+x%29" alt="\lambda_i = V_q^T(x_i - \bar x)" eeimg="1">。由于 <img src="https://www.zhihu.com/equation?tex=I-V_qV_q%5ET" alt="I-V_qV_q^T" eeimg="1"> 是个 <img src="https://www.zhihu.com/equation?tex=p%5Ctimes+p" alt="p\times p" eeimg="1"> 的方阵，并且具有幂等性质：</p><p data-pid="3urG3Alp"><img src="https://www.zhihu.com/equation?tex=%28I-V_qV_q%5ET%29%28I-V_qV_q%5ET%29%3DI-2V_qV_q%5ET%2BV_qV_q%5ETV_qV_q%5ET%3DI-V_qV_q%5ET%5C%5C" alt="(I-V_qV_q^T)(I-V_qV_q^T)=I-2V_qV_q^T+V_qV_q^TV_qV_q^T=I-V_qV_q^T\\" eeimg="1">所以矩阵的秩等于矩阵的迹<sup data-text='Proving: "The trace of an idempotent matrix equals the rank of the matrix"' data-url="https://math.stackexchange.com/questions/101512/proving-the-trace-of-an-idempotent-matrix-equals-the-rank-of-the-matrix#answer-2345818" data-draft-node="inline" data-draft-type="reference" data-numero="1">[1]</sup>。由于 <img src="https://www.zhihu.com/equation?tex=V_qV_q%5ET" alt="V_qV_q^T" eeimg="1"> 的迹就是 <img src="https://www.zhihu.com/equation?tex=V_q" alt="V_q" eeimg="1"> 所有元素的平方和，等于 <img src="https://www.zhihu.com/equation?tex=q" alt="q" eeimg="1"> ，所以 <img src="https://www.zhihu.com/equation?tex=I-V_qV_q%5ET" alt="I-V_qV_q^T" eeimg="1"> 的秩小于 <img src="https://www.zhihu.com/equation?tex=p" alt="p" eeimg="1"> ，不满秩，那么上述方程有无穷多个解，解空间就是这个方阵的零空间（证明过程参考<sup data-text="The Elements of Statistical Learning Ex. 14.7" data-url="https://github.com/szcf-weiya/ESL-CN/issues/45" data-draft-node="inline" data-draft-type="reference" data-numero="2">[2]</sup>）。这其实也合理，当我们将新的坐标系平移一下，让它的原点不与 <img src="https://www.zhihu.com/equation?tex=%5Cbar+x" alt="\bar x" eeimg="1"> 对齐，那么只要调整坐标和投影矩阵，总能得到相同的表示。</p><p data-pid="cfKcSoUZ">通常我们假设原始数据的均值为0，如果不为0，我们就在预处理时进行规范化。于是，重建误差的目标函数 <img src="https://www.zhihu.com/equation?tex=g" alt="g" eeimg="1"> 可以表示为：</p><p data-pid="_DMpuULD"><img src="https://www.zhihu.com/equation?tex=%5Cmin+%5C+g%28V_q%29+%3D+%5Csum_%7Bi%3D1%7D%5EN+%5CVert+x_i+-+V_q+V_q%5ETx_i%5CVert+%5E2%5C%5C" alt="\min \ g(V_q) = \sum_{i=1}^N \Vert x_i - V_q V_q^Tx_i\Vert ^2\\" eeimg="1">接下来，我们对这个函数进行展开和简化：<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5C+g%28V_q%29+%26%3D+%5Csum_i+%28x_i+-+V_q+V_q%5ETx_i%29%5ET%28x_i+-+V_q+V_q%5ETx_i%29%5C%5C+%26%3D%5Csum_i%28x_i%5ET-x_i%5ETV_qV_q%5ET%29%28x_i+-+V_q+V_q%5ETx_i%29%5C%5C+%26%3D%5Csum_i%28x_i%5ETx_i+-2x_i%5ETV_qV_q%5ETx_i%2Bx_i%5ETV_qV_q%5ETV_qV_q%5ETx_i%29%5C%5C+%26%3D-%5Csum_ix_i%5ETV_qV_q%5ETx_i+%2B+%5Ctext%7Bconstant%7D+%5C%5C+%26%3D-%5Csum_i%28V_q%5ETx_i%29%5ET%28V_q%5ETx_i%29+%2B+%5Ctext%7Bconstant%7D+%5C%5C+%26%3D-%5Csum_i%5Ctext%7Btr%7D%5C%7B%28V_q%5ETx_i%29%28V_q%5ETx_i%29%5ET%5C%7D+%2B%5Ctext%7Bconstant%7D+%5C%5C+%26%3D-%5Ctext%7Btr%7D%5C%7BV_q%5ET%5Csum_i%28x_ix_i%5ET%29V_q%5C%7D++%2B+%5Ctext%7Bconstant%7D+%5C%5C+%26%3D-%5Ctext%7Btr%7D%28V_q%5ETX%5ETXV_q%29++%2B+%5Ctext%7Bconstant%7D+%5Cend%7Balign%2A%7D%5C%5C" alt="\begin{align*} \ g(V_q) &amp;= \sum_i (x_i - V_q V_q^Tx_i)^T(x_i - V_q V_q^Tx_i)\\ &amp;=\sum_i(x_i^T-x_i^TV_qV_q^T)(x_i - V_q V_q^Tx_i)\\ &amp;=\sum_i(x_i^Tx_i -2x_i^TV_qV_q^Tx_i+x_i^TV_qV_q^TV_qV_q^Tx_i)\\ &amp;=-\sum_ix_i^TV_qV_q^Tx_i + \text{constant} \\ &amp;=-\sum_i(V_q^Tx_i)^T(V_q^Tx_i) + \text{constant} \\ &amp;=-\sum_i\text{tr}\{(V_q^Tx_i)(V_q^Tx_i)^T\} +\text{constant} \\ &amp;=-\text{tr}\{V_q^T\sum_i(x_ix_i^T)V_q\}  + \text{constant} \\ &amp;=-\text{tr}(V_q^TX^TXV_q)  + \text{constant} \end{align*}\\" eeimg="1"></p><p data-pid="XhYIS_mz">其中 <img src="https://www.zhihu.com/equation?tex=X" alt="X" eeimg="1"> 是 <img src="https://www.zhihu.com/equation?tex=N" alt="N" eeimg="1"> 个行向量 <img src="https://www.zhihu.com/equation?tex=x_i%5ET" alt="x_i^T" eeimg="1"> 组成的 <img src="https://www.zhihu.com/equation?tex=N%5Ctimes+p" alt="N\times p" eeimg="1"> 维矩阵。于是，目标函数变形为：</p><p data-pid="KWWorlnk"><img src="https://www.zhihu.com/equation?tex=%5Cmax+%5C+%5Ctext%7Btr%7D%28V_q%5ETX%5ETXV_q%29+%5Cquad+%5Ctext%7Bs.t.%7D+%5C+V_q%5ETV_q%3DI%5C%5C" alt="\max \ \text{tr}(V_q^TX^TXV_q) \quad \text{s.t.} \ V_q^TV_q=I\\" eeimg="1">由于新坐标系下的坐标 <img src="https://www.zhihu.com/equation?tex=%5Clambda+_i%3DV_q%5ETx_i" alt="\lambda _i=V_q^Tx_i" eeimg="1"> ，均值 <img src="https://www.zhihu.com/equation?tex=%5Cbar+%5Clambda_i" alt="\bar \lambda_i" eeimg="1"> 仍然为0，所以 <img src="https://www.zhihu.com/equation?tex=V_q%5ETX%5ETXV_q" alt="V_q^TX^TXV_q" eeimg="1"> 实际是 <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"> 的协方差矩阵，而矩阵的迹就是投影后样本的方差。于是，最小化重建误差等价于最大化样本在新坐标系各轴上的方差。</p><p data-pid="ap4roZlr">使用拉格朗日乘子法将该问题转化为无约束优化：</p><p data-pid="U3uW0J7c"><img src="https://www.zhihu.com/equation?tex=%5Cmax+%5C+tr%28V_q%5ETX%5ETXV_q%29-%5Csum_%7Bij%7D%28H+%5Codot+%28V_q%5ETV_q-I%29%29_%7Bij%7D%5C%5C" alt="\max \ tr(V_q^TX^TXV_q)-\sum_{ij}(H \odot (V_q^TV_q-I))_{ij}\\" eeimg="1"></p><p data-pid="dcUCcY-z">其中 <img src="https://www.zhihu.com/equation?tex=H" alt="H" eeimg="1"> 是乘子矩阵，与 <img src="https://www.zhihu.com/equation?tex=V_q%5ETV_q-I" alt="V_q^TV_q-I" eeimg="1"> 逐元素相乘，再将结果的所有元素累加为一个标量。在实际操作中，为了降低优化的复杂度，往往只对矩阵的主对角元素进行约束，即只要求 <img src="https://www.zhihu.com/equation?tex=V_q" alt="V_q" eeimg="1"> 的列向量是单位向量，而不要求它们正交。于是将 <img src="https://www.zhihu.com/equation?tex=H" alt="H" eeimg="1"> 矩阵的非主对角元素置零，乘子项变为 <img src="https://www.zhihu.com/equation?tex=%5Ctext%7Btr%7D%28%28V_q%5ETV_q-I%29H%29" alt="\text{tr}((V_q^TV_q-I)H)" eeimg="1">。目标函数对 <img src="https://www.zhihu.com/equation?tex=V_q" alt="V_q" eeimg="1"> 求梯度并使梯度为0（迹的梯度公式参考<a href="http://link.zhihu.com/?target=https%3A//www.math.uwaterloo.ca/%7Ehwolkowi/matrixcookbook.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">Matrix Cookbook</a>公式111），得到：</p><p data-pid="0HsJ-zYD"><img src="https://www.zhihu.com/equation?tex=X%5ETXV_q%3DV_qH%5C%5C" alt="X^TXV_q=V_qH\\" eeimg="1"></p><p data-pid="_cAVDYrH">可以看到 <img src="https://www.zhihu.com/equation?tex=V_q" alt="V_q" eeimg="1"> 的列向量 <img src="https://www.zhihu.com/equation?tex=v_j" alt="v_j" eeimg="1"> 其实就是 <img src="https://www.zhihu.com/equation?tex=X%5ETX" alt="X^TX" eeimg="1"> 的特征向量，而对应的 <img src="https://www.zhihu.com/equation?tex=H" alt="H" eeimg="1"> 矩阵的主对角元素 <img src="https://www.zhihu.com/equation?tex=%5Ceta_j" alt="\eta_j" eeimg="1"> 就是其特征值。将上式带入到优化目标函数中，得到： <img src="https://www.zhihu.com/equation?tex=%5Cmax+%5C+%5Ctext%7Btr%7D%28V_q%5ETX%5ETXV_q%29-0%3D%5Ctext%7Btr%7D%28V_q%5ETV_qH%29%3D%5Ctext%7Btr%7D%28H%29%3D%5Csum_%7Bj%3D1%7D%5Eq%5Ceta_j%5C%5C" alt="\max \ \text{tr}(V_q^TX^TXV_q)-0=\text{tr}(V_q^TV_qH)=\text{tr}(H)=\sum_{j=1}^q\eta_j\\" eeimg="1"></p><p data-pid="0YFxXBNR">在实际应用中，一般对降维后的维度 <img src="https://www.zhihu.com/equation?tex=q" alt="q" eeimg="1"> 进行参数选择。要最大化不同 <img src="https://www.zhihu.com/equation?tex=q" alt="q" eeimg="1"> 值下的目标函数，就要将 <img src="https://www.zhihu.com/equation?tex=%5Ceta_j" alt="\eta_j" eeimg="1"> 从按大到小排序，在降维时抛弃掉较小的特征值。</p><h2><b>实际操作</b></h2><p data-pid="0DGyJvFM">给定数据 <img src="https://www.zhihu.com/equation?tex=X" alt="X" eeimg="1"> 为 <img src="https://www.zhihu.com/equation?tex=N" alt="N" eeimg="1"> 个 <img src="https://www.zhihu.com/equation?tex=p" alt="p" eeimg="1"> 维行向量 <img src="https://www.zhihu.com/equation?tex=x_i%5ET" alt="x_i^T" eeimg="1"> 组成的矩阵。首先进行均值规范化，然后计算协方差矩阵 <img src="https://www.zhihu.com/equation?tex=X%5ETX" alt="X^TX" eeimg="1"> 并将其对角化为 <img src="https://www.zhihu.com/equation?tex=V%5ETHV" alt="V^THV" eeimg="1">，其中 <img src="https://www.zhihu.com/equation?tex=V" alt="V" eeimg="1"> 的列向量由 <img src="https://www.zhihu.com/equation?tex=X%5ETX" alt="X^TX" eeimg="1"> 的特征向量组成，对角矩阵 <img src="https://www.zhihu.com/equation?tex=H" alt="H" eeimg="1"> 的元素为对应的特征值。从大到小选择前 <img src="https://www.zhihu.com/equation?tex=q" alt="q" eeimg="1"> 个特征值对应的特征向量组成矩阵 <img src="https://www.zhihu.com/equation?tex=V_q" alt="V_q" eeimg="1"> 。最终，降维后的数据为 <img src="https://www.zhihu.com/equation?tex=XV_q" alt="XV_q" eeimg="1"> ，从 <img src="https://www.zhihu.com/equation?tex=N%5Ctimes+p" alt="N\times p" eeimg="1"> 降维至 <img src="https://www.zhihu.com/equation?tex=N%5Ctimes+q" alt="N\times q" eeimg="1">。维度 <img src="https://www.zhihu.com/equation?tex=q" alt="q" eeimg="1"> 作为超参，可以通过分类器模型的交叉验证进行筛选。</p><p data-pid="yLG9SO_R">以上基于对角化的方法需要先计算协方差矩阵，而奇异值分解（SVD）方法则不需要。我们将 <img src="https://www.zhihu.com/equation?tex=X" alt="X" eeimg="1"> 分解为 <img src="https://www.zhihu.com/equation?tex=UD+V%5ET" alt="UD V^T" eeimg="1">，其中 <img src="https://www.zhihu.com/equation?tex=U" alt="U" eeimg="1"> 和 <img src="https://www.zhihu.com/equation?tex=V" alt="V" eeimg="1"> 都由单位正交列向量组成，分别为 <img src="https://www.zhihu.com/equation?tex=N%5Ctimes+p" alt="N\times p" eeimg="1"> 维和 <img src="https://www.zhihu.com/equation?tex=p%5Ctimes+p" alt="p\times p" eeimg="1"> 维。协方差矩阵变为： <img src="https://www.zhihu.com/equation?tex=X%5ETX%3DVD+U%5ETUD+V%5ET%3DVD%5E2V%5ET%5C%5C" alt="X^TX=VD U^TUD V^T=VD^2V^T\\" eeimg="1"></p><p data-pid="skMiXi9C">从形式上看，这里的 <img src="https://www.zhihu.com/equation?tex=V" alt="V" eeimg="1"> 其实就是前文的 <img src="https://www.zhihu.com/equation?tex=V" alt="V" eeimg="1"> ，而 <img src="https://www.zhihu.com/equation?tex=D%5E2" alt="D^2" eeimg="1"> 就是特征值组成的对角矩阵 <img src="https://www.zhihu.com/equation?tex=H" alt="H" eeimg="1"> 。我们按照特征值从大到小对 <img src="https://www.zhihu.com/equation?tex=V" alt="V" eeimg="1"> 的列向量进行排序，并取前 <img src="https://www.zhihu.com/equation?tex=q" alt="q" eeimg="1"> 列得到矩阵 <img src="https://www.zhihu.com/equation?tex=V_q" alt="V_q" eeimg="1"> 。此时 <img src="https://www.zhihu.com/equation?tex=U" alt="U" eeimg="1"> 取对应的 <img src="https://www.zhihu.com/equation?tex=q" alt="q" eeimg="1"> 列， <img src="https://www.zhihu.com/equation?tex=D" alt="D" eeimg="1"> 也只保留前 <img src="https://www.zhihu.com/equation?tex=q" alt="q" eeimg="1"> 大的奇异值（特征值开根号），那么降维后的特征矩阵为：</p><p data-pid="yqXwoWl0"><img src="https://www.zhihu.com/equation?tex=XV_q%3DU_qD_q+V_q%5ETV_q%3DU_qD_q%5C%5C" alt="XV_q=U_qD_q V_q^TV_q=U_qD_q\\" eeimg="1">所以，我们不需要计算协方差矩阵，SVD分解的结果直接就是PCA的结果，这是SVD分解的性质决定的。类似地，我们也可以筛选 <img src="https://www.zhihu.com/equation?tex=U" alt="U" eeimg="1"> 矩阵的行而不是 <img src="https://www.zhihu.com/equation?tex=V" alt="V" eeimg="1"> 矩阵的列，相当于对协方差矩阵 <img src="https://www.zhihu.com/equation?tex=XX%5ET" alt="XX^T" eeimg="1"> 进行对角化。这样做相当于数据筛选，可以去除线性相关的冗余数据。为什么SVD具有这样的性质，可以参考<a href="https://zhuanlan.zhihu.com/p/29846048" class="internal" target="_blank">漫漫成长：奇异值分解（SVD）</a>中的计算过程。</p><p data-pid="5S56DoYv">注意，本文参考的是<a href="http://link.zhihu.com/?target=https%3A//esl.hohoweiya.xyz/14-Unsupervised-Learning/14.5-Principal-Components-Curves-and-Surfaces/index.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">统计学习基础（ESL）</a>这本书中 <img src="https://www.zhihu.com/equation?tex=X" alt="X" eeimg="1"> 的定义，有些地方定义的 <img src="https://www.zhihu.com/equation?tex=X" alt="X" eeimg="1"> 可能是 <img src="https://www.zhihu.com/equation?tex=p%5Ctimes+N" alt="p\times N" eeimg="1"> 得矩阵。在本文中， <img src="https://www.zhihu.com/equation?tex=X%5ETX" alt="X^TX" eeimg="1"> 得到的是 <img src="https://www.zhihu.com/equation?tex=p%5Ctimes+p" alt="p\times p" eeimg="1"> 的矩阵，也就是特征之间的协方差矩阵；而 <img src="https://www.zhihu.com/equation?tex=XX%5ET" alt="XX^T" eeimg="1"> 是 <img src="https://www.zhihu.com/equation?tex=N%5Ctimes+N" alt="N\times N" eeimg="1"> 的矩阵，是样本之间的协方差矩阵。后者在样本数量较大的时候计算量巨大，我想这可能也是为啥没有把PCA搬过来做“主样本分析”的方法。</p>