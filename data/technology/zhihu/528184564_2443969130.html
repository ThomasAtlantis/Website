<p data-pid="_eZYA6KY"><b>一种主流的说法认为深度网络主要具有两点优势：表示效率高和泛化能力强，</b>当然前提是网络的宽度在一个合理的范围内。本文主要参考了Deep Learning这本书，由于我不是专门做神经网络理论研究的，这里只提供一些我比较相信的观点。</p><h2>1 表示效率</h2><p data-pid="Ah8R96sg">参考花书6.4.1小节对神经网络的万能近似性质和深度的讨论。万能近似定理告诉我们，一个足够大的MLP能够表示任意一个函数，但<b>没有告诉我们这个MLP需要多大才算足够大。</b>Montufar在文章<a href="https://link.zhihu.com/?target=https%3A//proceedings.neurips.cc/paper/2014/file/109d2dd3608f669ca17920c511c2a41e-Paper.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">On the Number of Linear Regions of Deep Neural Networks</a>的推论6中指出：</p><p data-pid="O9UNL6d8">对于一个以整流单元（比如ReLU）为激活函数的神经网络，<img src="https://www.zhihu.com/equation?tex=n_0" alt="n_0" eeimg="1"> 个输入单元，<img src="https://www.zhihu.com/equation?tex=L" alt="L" eeimg="1"> 个隐藏层，每层具有 <img src="https://www.zhihu.com/equation?tex=n" alt="n" eeimg="1"> 个神经元（保证 <img src="https://www.zhihu.com/equation?tex=n%5Cge+n_0" alt="n\ge n_0" eeimg="1"> ），则可以拟合具有 <img src="https://www.zhihu.com/equation?tex=M" alt="M" eeimg="1"> 个线性区域的函数，其中</p><p data-pid="wpDcMuxt"><img src="https://www.zhihu.com/equation?tex=M%3D%5COmega%5Cleft%28%5Cleft%28%5Cfrac%7Bn%7D%7Bn_0%7D%5Cright%29%5E%7Bn_0%28L-1%29%7D+n%5E%7Bn_0%7D%5Cright%29%5C%5C" alt="M=\Omega\left(\left(\frac{n}{n_0}\right)^{n_0(L-1)} n^{n_0}\right)\\" eeimg="1"> </p><p data-pid="kgHOFZF5">我们知道对于ReLU激活的网络或者是Spline回归，都是用分段线性函数去逼近任意一个待拟合的函数。我们可以在<a href="https://link.zhihu.com/?target=http%3A//playground.tensorflow.org/%23activation%3Drelu%26batchSize%3D10%26dataset%3Dcircle%26regDataset%3Dreg-plane%26learningRate%3D0.03%26regularizationRate%3D0%26noise%3D0%26networkShape%3D5%26seed%3D0.78263%26showTestData%3Dfalse%26discretize%3Dfalse%26percTrainData%3D50%26x%3Dtrue%26y%3Dtrue%26xTimesY%3Dfalse%26xSquared%3Dfalse%26ySquared%3Dfalse%26cosX%3Dfalse%26sinX%3Dfalse%26cosY%3Dfalse%26sinY%3Dfalse%26collectStats%3Dfalse%26problem%3Dclassification%26initZero%3Dfalse%26hideText%3Dfalse" class=" wrap external" target="_blank" rel="nofollow noreferrer">TensorFlow的Playground</a>里直观地体会，ReLU激活会导致分界面分段线性。随着神经元数量的提升，线性段的数量也会提升，当线性段趋于无穷时，我们理论上可以完美拟合任意函数，这实际上也是微元法的道理。</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/50/v2-3bedd72224d38ca01bc0d6581f8c7184_720w.jpg?source=c8b7c179" data-caption="" data-size="normal" data-rawwidth="2560" data-rawheight="1328" data-original-token="v2-2898ad84f0ab0ce84704794c8e7a0557" data-default-watermark-src="https://picx.zhimg.com/50/v2-58b65eb770227d202bcfa6b436749dd4_720w.jpg?source=c8b7c179" class="origin_image zh-lightbox-thumb" width="2560" data-original="https://pic1.zhimg.com/v2-3bedd72224d38ca01bc0d6581f8c7184_r.jpg?source=c8b7c179"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='2560'%20height='1328'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="2560" data-rawheight="1328" data-original-token="v2-2898ad84f0ab0ce84704794c8e7a0557" data-default-watermark-src="https://picx.zhimg.com/50/v2-58b65eb770227d202bcfa6b436749dd4_720w.jpg?source=c8b7c179" class="origin_image zh-lightbox-thumb lazy" width="2560" data-original="https://pic1.zhimg.com/v2-3bedd72224d38ca01bc0d6581f8c7184_r.jpg?source=c8b7c179" data-actualsrc="https://picx.zhimg.com/50/v2-3bedd72224d38ca01bc0d6581f8c7184_720w.jpg?source=c8b7c179"></figure><p data-pid="JROeE4ic">在以上公式中，将 <img src="https://www.zhihu.com/equation?tex=n_0" alt="n_0" eeimg="1"> 看做 <img src="https://www.zhihu.com/equation?tex=O%281%29" alt="O(1)" eeimg="1"> 的常数，那么很明显，对于神经元总数为 <img src="https://www.zhihu.com/equation?tex=nL" alt="nL" eeimg="1"> 的神经网络，层数的增多带来的能力下界的提升是指数级别的，而宽度的增加带来的只是多项式级别的。文中提到，<b>如果将深度整流网络的表示效率定义为单位参数平均能表示的函数的线性区域数量，那么深度模型的表示效率将会指数倍的优于浅层模型。</b>这个指数优势在论文中还有一个有意思的比喻：</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/50/v2-4f9c0d10cbaca6497090d88a2de81455_720w.jpg?source=c8b7c179" data-caption="" data-size="normal" data-rawwidth="1240" data-rawheight="640" data-original-token="v2-bc8415aa287b40aff2e3d752121b950c" data-default-watermark-src="https://pic1.zhimg.com/50/v2-9b857d4718e74900571963d725473708_720w.jpg?source=c8b7c179" class="origin_image zh-lightbox-thumb" width="1240" data-original="https://picx.zhimg.com/v2-4f9c0d10cbaca6497090d88a2de81455_r.jpg?source=c8b7c179"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1240'%20height='640'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1240" data-rawheight="640" data-original-token="v2-bc8415aa287b40aff2e3d752121b950c" data-default-watermark-src="https://pic1.zhimg.com/50/v2-9b857d4718e74900571963d725473708_720w.jpg?source=c8b7c179" class="origin_image zh-lightbox-thumb lazy" width="1240" data-original="https://picx.zhimg.com/v2-4f9c0d10cbaca6497090d88a2de81455_r.jpg?source=c8b7c179" data-actualsrc="https://picx.zhimg.com/50/v2-4f9c0d10cbaca6497090d88a2de81455_720w.jpg?source=c8b7c179"></figure><p data-pid="U9y0bKNN">当然需要注意的一点是理论推导过程中假定了每层宽度都不小于输入的维度，虽然神经网络的最优宽度并没有理论指导，但根据实践经验会有很多经验公式。其中比较流行的一种说法是：</p><p data-pid="O4dSywyH"><img src="https://www.zhihu.com/equation?tex=N_h%3D%5Cfrac%7BN_s%7D%7B%5Calpha+%28N_i%2BN_o%29%7D%5C%5C" alt="N_h=\frac{N_s}{\alpha (N_i+N_o)}\\" eeimg="1"> </p><p data-pid="vZeh2jgA">其中，<img src="https://www.zhihu.com/equation?tex=N_h" alt="N_h" eeimg="1"> 、<img src="https://www.zhihu.com/equation?tex=N_s" alt="N_s" eeimg="1"> 、<img src="https://www.zhihu.com/equation?tex=N_i" alt="N_i" eeimg="1"> 、<img src="https://www.zhihu.com/equation?tex=N_o" alt="N_o" eeimg="1"> 分别表示隐层节点数量、训练样本数量、输入节点数量、输出节点数量。其中 <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="\alpha" eeimg="1"> 是一个可调节的因子，一般取2~10。我没有尝试过按照这个公式的指导设计网络，但这类经验公式的出现至少说明了，追求网络深度的同时，宽度需要在合理的范围，否则性能会很差。</p><p data-pid="9Tk6A_6J">除了上文提到的文章，大家也可以参考近些年对神经网络的VC维的探讨，会得出相似的结论。</p><h2>2 泛化能力</h2><p data-pid="NJCTov7z">神经网络虽然是端到端的模型，但可以看做是特征提取器和分类器的组合。对于网络的前几层，神经元用于提取输入的低维特征，比如图像的边缘信息。之后这些低维特征将会逐步组合和变换成包含更加抽象语义的深层特征，比如图像的形状。在特征提取时，我们希望这些特征是相互独立的，在传统机器学习中我们会先使用PCA之类的方法去除冗余信息，但在神经网络中我们希望通过模型的设计和训练自动实现。</p><p data-pid="AuI7_F5M">当浅网络的宽度过宽时，我们相当于在低维特征提取时引入了大量重复的特征。比如上面提到的使用ReLU激活的网络，可能本来只需要一条直线即可分开的区域，现在使用了多个分段直线拼接在一起。而由于网络很浅，对于这些低维特征的组合变化能力不足，就导致模型更加关注细节，而不是背后抽象的逻辑，或者是对样本进行死记硬背。夸张的说，如果神经网络的宽度大于样本的数量，则可以对特定的样本只激活特定的神经元。而宽度较小时，很多相似的样本需要激活相似的神经元集合，这就引入了归纳和分类的能力。</p><p data-pid="KmJVuCvw">同样，深度模型泛化能力更强也是经过长期的实验论证得到的经验。比如</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/50/v2-d1a513bb962464cce6fa2048c7285f16_720w.jpg?source=c8b7c179" data-caption="" data-size="normal" data-rawwidth="1254" data-rawheight="932" data-original-token="v2-912bb9fe29954e2c5ec09549b5600343" data-default-watermark-src="https://picx.zhimg.com/50/v2-bb68c67e926ad47271ee42f5e31bae1a_720w.jpg?source=c8b7c179" class="origin_image zh-lightbox-thumb" width="1254" data-original="https://pic1.zhimg.com/v2-d1a513bb962464cce6fa2048c7285f16_r.jpg?source=c8b7c179"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1254'%20height='932'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1254" data-rawheight="932" data-original-token="v2-912bb9fe29954e2c5ec09549b5600343" data-default-watermark-src="https://picx.zhimg.com/50/v2-bb68c67e926ad47271ee42f5e31bae1a_720w.jpg?source=c8b7c179" class="origin_image zh-lightbox-thumb lazy" width="1254" data-original="https://pic1.zhimg.com/v2-d1a513bb962464cce6fa2048c7285f16_r.jpg?source=c8b7c179" data-actualsrc="https://picx.zhimg.com/50/v2-d1a513bb962464cce6fa2048c7285f16_720w.jpg?source=c8b7c179"></figure><hr><h2>花书勘误</h2><p data-pid="5FY7Lxz2">6.4.1小节引用的Montufar定理的公式写错了，话说Yoshua Bengio既是花书的作者，也是引用的这篇论文的作者，我还是想不明白为什么会出错。书上的公式是这么写的：</p><blockquote data-pid="6LwOFygZ">Montufar <i>et al. </i>(2014) 的主要定理指出，具有 <img src="https://www.zhihu.com/equation?tex=d" alt="d" eeimg="1"> 个输入、深度为 <img src="https://www.zhihu.com/equation?tex=l" alt="l" eeimg="1"> 、每个隐藏层具有 <img src="https://www.zhihu.com/equation?tex=n" alt="n" eeimg="1"> 个单元的深度整流网络可以描述的线性区域的数量是：</blockquote><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/50/v2-83731b61bb4cc8bf941dbe57e1ea18f7_720w.png?source=c8b7c179" data-caption="" data-size="normal" data-rawwidth="1500" data-rawheight="160" data-original-token="v2-83731b61bb4cc8bf941dbe57e1ea18f7" class="origin_image zh-lightbox-thumb" width="1500" data-original="https://picx.zhimg.com/v2-83731b61bb4cc8bf941dbe57e1ea18f7_r.jpg?source=c8b7c179"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1500'%20height='160'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1500" data-rawheight="160" data-original-token="v2-83731b61bb4cc8bf941dbe57e1ea18f7" class="origin_image zh-lightbox-thumb lazy" width="1500" data-original="https://picx.zhimg.com/v2-83731b61bb4cc8bf941dbe57e1ea18f7_r.jpg?source=c8b7c179" data-actualsrc="https://pic1.zhimg.com/50/v2-83731b61bb4cc8bf941dbe57e1ea18f7_720w.png?source=c8b7c179"></figure><p data-pid="2eF08tTK">注意这里指数的底是个组合数，然而在论文中这里是分数，所以<b>我严重怀疑印刷漏掉了一个分数线</b>。为了证明论文中的<code>/</code>不是一种我没见过的组合数表示方法，而就是分数，我们可以从主定理5推导一下推论6。</p><p data-pid="7XXtap-7">定理5指出具有 <img src="https://www.zhihu.com/equation?tex=n_0" alt="n_0" eeimg="1"> 个输入单元和 <img src="https://www.zhihu.com/equation?tex=L" alt="L" eeimg="1"> 个隐藏层的神经网络，第 <img src="https://www.zhihu.com/equation?tex=i" alt="i" eeimg="1"> 层具有 <img src="https://www.zhihu.com/equation?tex=n_i+%5Cge+n_0" alt="n_i \ge n_0" eeimg="1"> 个整流激活单元，能够表示的线性区域的最大值的下界为：</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/50/v2-72b78f584c9ace8f9a3d2969d6434b71_720w.png?source=c8b7c179" data-caption="" data-size="normal" data-rawwidth="1772" data-rawheight="174" data-original-token="v2-72b78f584c9ace8f9a3d2969d6434b71" class="origin_image zh-lightbox-thumb" width="1772" data-original="https://picx.zhimg.com/v2-72b78f584c9ace8f9a3d2969d6434b71_r.jpg?source=c8b7c179"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1772'%20height='174'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1772" data-rawheight="174" data-original-token="v2-72b78f584c9ace8f9a3d2969d6434b71" class="origin_image zh-lightbox-thumb lazy" width="1772" data-original="https://picx.zhimg.com/v2-72b78f584c9ace8f9a3d2969d6434b71_r.jpg?source=c8b7c179" data-actualsrc="https://pic1.zhimg.com/50/v2-72b78f584c9ace8f9a3d2969d6434b71_720w.png?source=c8b7c179"></figure><p data-pid="SoO4kAUh">在 <img src="https://www.zhihu.com/equation?tex=n_i%3Dn" alt="n_i=n" eeimg="1"> 即每层都具有相同的节点数时，假设输入 <img src="https://www.zhihu.com/equation?tex=n_0" alt="n_0" eeimg="1"> 可以看做 <img src="https://www.zhihu.com/equation?tex=O%281%29" alt="O(1)" eeimg="1"> ，那么上式可以变换为：</p><p data-pid="TXqA5r9c"><img src="https://www.zhihu.com/equation?tex=%5Clfloor+%5Cfrac%7Bn%7D%7Bn_0%7D%5Crfloor%5E%7Bn_0%28L-1%29%7D+%5Csum_%7Bj%3D0%7D%5E%7Bn_0%7D%5Ctext%7BC%7D_n%5Ej%5C%5C" alt="\lfloor \frac{n}{n_0}\rfloor^{n_0(L-1)} \sum_{j=0}^{n_0}\text{C}_n^j\\" eeimg="1"> </p><p data-pid="aVqw5O-k">根据这篇文章的上一篇工作<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1312.6098.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">On the number of response regions of deep feedforward networks with piecewise linear activations</a>中指出</p><p data-pid="qx-8E_CC"><img src="https://www.zhihu.com/equation?tex=+%5Csum_%7Bj%3D0%7D%5E%7Bn_0%7D%5Ctext%7BC%7D_n%5Ej%3D%5CTheta%28n%5E%7Bn_0%7D%29%5C%5C" alt=" \sum_{j=0}^{n_0}\text{C}_n^j=\Theta(n^{n_0})\\" eeimg="1"> </p><p data-pid="AXL1-2Yf">所以上述公式可以等价为</p><p data-pid="9LWY3_Dc"><img src="https://www.zhihu.com/equation?tex=%5COmega%5Cleft%28%5Cleft%28+%5Cfrac%7Bn%7D%7Bn_0%7D%5Cright%29%5E%7Bn_0%28L-1%29%7D+n%5E%7Bn_0%7D%5Cright%29%5C%5C" alt="\Omega\left(\left( \frac{n}{n_0}\right)^{n_0(L-1)} n^{n_0}\right)\\" eeimg="1"> </p><p data-pid="eDd8MGeW">这个公式与两篇论文的结论始终一致，说明书中印刷有误。</p>