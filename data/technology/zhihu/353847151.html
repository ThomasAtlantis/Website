<p data-pid="dJa-m2nY">高斯-马尔可夫定理「在线性回归模型中，如果误差满足零均值、同方差且互不相关，则回归系数的最佳线性无偏估计就是普通最小二乘法估计。」这个定义包含两层含义，一是最小二乘法的估计是无偏的，即其期望值就是最优参数；二是所有对于线性回归的系数的估计方法最优不会优于最小二乘法，或者说估计的方差不会小于最小二乘法。</p><h2>假设条件</h2><p data-pid="U5nCuVsq">假设数据集中的输入 <img src="https://www.zhihu.com/equation?tex=X" alt="X" eeimg="1"> 和输出 <img src="https://www.zhihu.com/equation?tex=Y" alt="Y" eeimg="1"> ，满足以下线性关系</p><p data-pid="VACQFvnb"><img src="https://www.zhihu.com/equation?tex=Y+%3D+X%5Cbeta+%2B+%5Cepsilon+%5C%5C" alt="Y = X\beta + \epsilon \\" eeimg="1"> </p><p data-pid="vqbEccTG">其中 <img src="https://www.zhihu.com/equation?tex=X+%5Cin+%5Cmathbb%7BR%7D%5E%7Bn%5Ctimes+d%7D" alt="X \in \mathbb{R}^{n\times d}" eeimg="1"> ， <img src="https://www.zhihu.com/equation?tex=Y%5Cin+%5Cmathbb%7BR%7D%5E%7Bn%5Ctimes+1%7D" alt="Y\in \mathbb{R}^{n\times 1}" eeimg="1"> ，即输入是 <img src="https://www.zhihu.com/equation?tex=d" alt="d" eeimg="1"> 维向量，输出是 <img src="https://www.zhihu.com/equation?tex=1" alt="1" eeimg="1"> 维标量，数据集中共有 <img src="https://www.zhihu.com/equation?tex=n" alt="n" eeimg="1"> 个样本点；方阵 <img src="https://www.zhihu.com/equation?tex=X%5ETX" alt="X^TX" eeimg="1"> 可逆，即样本点没有重合，分量间也互相独立；误差变量 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"> 代表随机噪声，均值为 <img src="https://www.zhihu.com/equation?tex=0" alt="0" eeimg="1"> ，方差记为 <img src="https://www.zhihu.com/equation?tex=%5Csigma%5E2" alt="\sigma^2" eeimg="1"> 。这里的误差是在线性模型视角下的数据集天然的噪声，是无法通过优化模型算法去除的，我们所能做的就是尽量不放大它。提醒一下 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"> 是 <img src="https://www.zhihu.com/equation?tex=n" alt="n" eeimg="1"> 维向量，而方差是 <img src="https://www.zhihu.com/equation?tex=n" alt="n" eeimg="1"> 维方阵。</p><h2>最小二乘法是无偏估计</h2><p data-pid="GvwTF8_T">最小二乘法得到的 <img src="https://www.zhihu.com/equation?tex=%5Chat%5Cbeta" alt="\hat\beta" eeimg="1"> 是无偏的，即其期望应当等于理想中的最优参数</p><p data-pid="E-T6j_EC"><img src="https://www.zhihu.com/equation?tex=E%28%5Chat%5Cbeta%29%3D%5Cbeta%5C%5C" alt="E(\hat\beta)=\beta\\" eeimg="1"> </p><p data-pid="0zKlkSBD">下面我们来证明一下。最小二乘法的均方误差最小时取得参数的估计值</p><p data-pid="6NwT0-sA"><img src="https://www.zhihu.com/equation?tex=%5Chat%5Cbeta%3D%5Carg+%5Cmin_%7B%5Cbeta%27%7D%5CVert+X%5Cbeta%27-Y%5CVert%5E2_2%5C%5C" alt="\hat\beta=\arg \min_{\beta'}\Vert X\beta'-Y\Vert^2_2\\" eeimg="1"> </p><p data-pid="VSPnaj01">当然我们可以直接对矩阵求导，让导数等于零来解出估计值的表达式。矩阵求导相关知识请参考<a href="http://link.zhihu.com/?target=http%3A//www.math.uwaterloo.ca/%7Ehwolkowi/matrixcookbook.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">Matrix Cookbook</a>第二章，我这里给出详细的推导过程。</p><p data-pid="Z561bh-l"><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%26%5Cpartial+%5CVert+X%5Chat%5Cbeta-Y+%5CVert%5E2_2+%5C%5C+%26%3D+%5Cpartial+%5B%28X%5Chat%5Cbeta-Y%29%5ET%28X%5Chat%5Cbeta-Y%29%5D+%5C%5C+%26%3D%5B%5Cpartial%28X%5Chat%5Cbeta-Y%29%5D%5ET%28X%5Chat%5Cbeta-Y%29%2B%28X%5Chat%5Cbeta-Y%29%5ET%5Cpartial%28X%5Chat%5Cbeta-Y%29%5C%5C+%26%3D2%5B%5Cpartial%28X%5Chat%5Cbeta-Y%29%5D%5ET%28X%5Chat%5Cbeta-Y%29%5C%5C+%26%3D2%28%5Cpartial+X%5Ccirc%5Chat%5Cbeta%2BX%5Ccirc%5Cpartial%5Chat%5Cbeta-%5Cpartial+Y%29%5ET%28X%5Chat%5Cbeta-Y%29%5C%5C+%26%5Cpartial+%5CVert+X%5Chat%5Cbeta-Y+%5CVert%5E2_2+%2F%5Cpartial+%5Chat%5Cbeta%5C%5C+%26%3D2%28%5Cpartial+X%5Ccirc%5Chat%5Cbeta%2BX%5Ccirc%5Cpartial%5Chat%5Cbeta-%5Cpartial+Y%29%5ET%28X%5Chat%5Cbeta-Y%29%2F%5Cpartial%5Chat%5Cbeta%5C%5C+%26%3D2X%5ET%28X%5Chat%5Cbeta-Y%29%3D0%5C%5C+%26%5CRightarrow+%5Chat%5Cbeta%3D%28X%5ETX%29%5E%7B-1%7DX%5ETY+%5Cend%7Balign%2A%7D%5C%5C+" alt="\begin{align*} &amp;\partial \Vert X\hat\beta-Y \Vert^2_2 \\ &amp;= \partial [(X\hat\beta-Y)^T(X\hat\beta-Y)] \\ &amp;=[\partial(X\hat\beta-Y)]^T(X\hat\beta-Y)+(X\hat\beta-Y)^T\partial(X\hat\beta-Y)\\ &amp;=2[\partial(X\hat\beta-Y)]^T(X\hat\beta-Y)\\ &amp;=2(\partial X\circ\hat\beta+X\circ\partial\hat\beta-\partial Y)^T(X\hat\beta-Y)\\ &amp;\partial \Vert X\hat\beta-Y \Vert^2_2 /\partial \hat\beta\\ &amp;=2(\partial X\circ\hat\beta+X\circ\partial\hat\beta-\partial Y)^T(X\hat\beta-Y)/\partial\hat\beta\\ &amp;=2X^T(X\hat\beta-Y)=0\\ &amp;\Rightarrow \hat\beta=(X^TX)^{-1}X^TY \end{align*}\\ " eeimg="1"> </p><p data-pid="OxaIqjPm">在交大的人工智能数学基础课上，<a href="http://link.zhihu.com/?target=https%3A//space.bilibili.com/95975441%3Ffrom%3Dsearch%26seid%3D16058231123379670049" class=" wrap external" target="_blank" rel="nofollow noreferrer">许志钦</a>老师介绍了一种更巧妙的思维角度，但有些细节不太好理解。在高度领会他的精神之后，我在这里补充一部分内容。</p><p data-pid="8Orm0JBe">将 <img src="https://www.zhihu.com/equation?tex=X" alt="X" eeimg="1"> 矩阵竖着看可以划分为 <img src="https://www.zhihu.com/equation?tex=d" alt="d" eeimg="1"> 个 <img src="https://www.zhihu.com/equation?tex=n" alt="n" eeimg="1"> 维向量 <img src="https://www.zhihu.com/equation?tex=x%5Ei" alt="x^i" eeimg="1"> ，那么 <img src="https://www.zhihu.com/equation?tex=X%5Cbeta%27" alt="X\beta'" eeimg="1"> 就可以理解为对这 <img src="https://www.zhihu.com/equation?tex=d" alt="d" eeimg="1"> 个向量的线性组合，且它们都处在一个 <img src="https://www.zhihu.com/equation?tex=n" alt="n" eeimg="1"> 维的超平面上。由于随机噪声变量独立于 <img src="https://www.zhihu.com/equation?tex=x%5Ei" alt="x^i" eeimg="1"> 的各个分量，所以可以将其看成是正交于这个超平面的一个超轴，因为 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"> 本身也是 <img src="https://www.zhihu.com/equation?tex=n" alt="n" eeimg="1"> 维的。那么 <img src="https://www.zhihu.com/equation?tex=Y%3DX%5Cbeta%27%2B%5Cepsilon" alt="Y=X\beta'+\epsilon" eeimg="1"> 就可以看成是在超平面外游动的一个 <img src="https://www.zhihu.com/equation?tex=2n" alt="2n" eeimg="1"> 维的向量。它有时在平面外，但从期望的角度看是在平面内的，即 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"> 全为 <img src="https://www.zhihu.com/equation?tex=0" alt="0" eeimg="1"> 时。我们可以绘制示意图如下。</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-bcb0f9eaf5a1c7d6cde171239af59377_720w.jpg?source=d16d100b" data-size="normal" data-rawwidth="1940" data-rawheight="934" class="origin_image zh-lightbox-thumb" width="1940" data-original="https://pic1.zhimg.com/v2-bcb0f9eaf5a1c7d6cde171239af59377_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1940'%20height='934'&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="1940" data-rawheight="934" class="origin_image zh-lightbox-thumb lazy" width="1940" data-original="https://pic1.zhimg.com/v2-bcb0f9eaf5a1c7d6cde171239af59377_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-bcb0f9eaf5a1c7d6cde171239af59377_720w.jpg?source=d16d100b"><figcaption>这里X\beta’-Y向量的方向画反了，懒得改了</figcaption></figure><p data-pid="bCViKQsl">其中 <img src="https://www.zhihu.com/equation?tex=AOB+" alt="AOB " eeimg="1"> 平面为 <img src="https://www.zhihu.com/equation?tex=x%5Ei" alt="x^i" eeimg="1"> 向量所在的 <img src="https://www.zhihu.com/equation?tex=n" alt="n" eeimg="1"> 维超平面。那么最小二乘法的均方误差公式可以看做向量 <img src="https://www.zhihu.com/equation?tex=X%5Cbeta%27+-Y" alt="X\beta' -Y" eeimg="1"> 的模长的平方。显然，这个向量在垂直于 <img src="https://www.zhihu.com/equation?tex=AOB+" alt="AOB " eeimg="1"> 平面时长度最短，或者说向量 <img src="https://www.zhihu.com/equation?tex=X%5Cbeta%27" alt="X\beta'" eeimg="1"> 为高维向量 <img src="https://www.zhihu.com/equation?tex=Y+" alt="Y " eeimg="1"> 在 <img src="https://www.zhihu.com/equation?tex=AOB+" alt="AOB " eeimg="1"> 平面上的投影时该向量最短。那么此时该向量与 <img src="https://www.zhihu.com/equation?tex=AOB" alt="AOB" eeimg="1"> 平面上的 <img src="https://www.zhihu.com/equation?tex=d" alt="d" eeimg="1"> 个 <img src="https://www.zhihu.com/equation?tex=x%5Ei" alt="x^i" eeimg="1"> 向量分别正交，得到的方程组合并为矩阵形式有</p><p data-pid="qIrG8VVe"><img src="https://www.zhihu.com/equation?tex=X%5ET%28X%5Chat%5Cbeta-Y%29%3D0%5C%5C" alt="X^T(X\hat\beta-Y)=0\\" eeimg="1"> </p><p data-pid="tADWWsbO">这个结果与导数方法的结果保持一致。如果该向量不与平面垂直，可以按照上图中的方法沿各个坐标轴分解，那么分解后得到的 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"> 轴方向的偏差为数据集的自然误差，其他在 <img src="https://www.zhihu.com/equation?tex=AOB" alt="AOB" eeimg="1"> 平面上的分量则为模型拟合误差，这时该模型就不是无偏估计了。有了参数 <img src="https://www.zhihu.com/equation?tex=%5Chat%5Cbeta" alt="\hat\beta" eeimg="1"> 的表达式，下面从代数的角度证明一下它是对理想参数 <img src="https://www.zhihu.com/equation?tex=%5Cbeta" alt="\beta" eeimg="1"> 的无偏估计。</p><p data-pid="f2rcu77w"><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+E%28%5Chat%5Cbeta%7CX%29%26%3DE%28%28X%5ETX%29%5E%7B-1%7DX%5ETY%7CX%29%5C%5C+%26%3DE%28%28X%5ETX%29%5E%7B-1%7DX%5ET%28X%5Cbeta%2B%5Cepsilon%29%7CX%29%5C%5C+%26%3DE%28%28X%5ETX%29%5E%7B-1%7D%28X%5ETX%29%5Cbeta%7CX%29%2B%28X%5ETX%29%5E%7B-1%7DX%5ETE%28%5Cepsilon%7CX%29%5C%5C+%26%3DE%28%5Cbeta%7CX%29%2B0%3D%5Cbeta+%5Cend%7Balign%2A%7D%5C%5C" alt="\begin{align*} E(\hat\beta|X)&amp;=E((X^TX)^{-1}X^TY|X)\\ &amp;=E((X^TX)^{-1}X^T(X\beta+\epsilon)|X)\\ &amp;=E((X^TX)^{-1}(X^TX)\beta|X)+(X^TX)^{-1}X^TE(\epsilon|X)\\ &amp;=E(\beta|X)+0=\beta \end{align*}\\" eeimg="1"> </p><p data-pid="cFsbkGpE">这里注意 <img src="https://www.zhihu.com/equation?tex=%5Cbeta" alt="\beta" eeimg="1"> 是我们假设的理想参数，在已知数据集 <img src="https://www.zhihu.com/equation?tex=X" alt="X" eeimg="1"> 的情况下是一个唯一固定的值。无偏估计就是说最小二乘法在模型的准确性上无可挑剔，已经是最好的模型了。</p><h2>无偏估计中，最小二乘法是最优估计</h2><p data-pid="FcziMb6b">对于线性回归模型的任意无偏估计的参数可以表示成形式 <img src="https://www.zhihu.com/equation?tex=%5Ctilde%7B%5Cbeta%7D%3D%5Chat+%5Cbeta+%2B+%5Cxi" alt="\tilde{\beta}=\hat \beta + \xi" eeimg="1"> ，则估计的方差满足：</p><p data-pid="-gYKhkul"><img src="https://www.zhihu.com/equation?tex=%5Ctext%7BVar%7D%28%5Chat%5Cbeta%7CX%29%5Cleq%5Ctext%7BVar%7D%28%5Ctilde%5Cbeta%7CX%29%5C%5C" alt="\text{Var}(\hat\beta|X)\leq\text{Var}(\tilde\beta|X)\\" eeimg="1"> </p><p data-pid="bHp_p97k">当 <img src="https://www.zhihu.com/equation?tex=%5Cxi" alt="\xi" eeimg="1"> 为零向量时，以上形式变为最小二乘法估计，并取得最小方差。下面我们展开证明一下。</p><p data-pid="DfABH3qy">由于 <img src="https://www.zhihu.com/equation?tex=%5Ctilde%5Cbeta" alt="\tilde\beta" eeimg="1"> 是无偏估计，需满足：</p><p data-pid="uaVFgEl1"><img src="https://www.zhihu.com/equation?tex=%5Ctext+E%5B%5Ctilde%5Cbeta%7CX%5D%3D%5Ctext+E%5B%5Chat%5Cbeta%2B%5Cxi%7CX%5D%3D%5Ctext+E%5B%5Chat+%5Cbeta%7CX%5D%2B%5Ctext+E%5B%5Cxi%7CX%5D%5CRightarrow+%5Ctext+E%5B%5Cxi%7CX%5D%3D%5Cmathbf%7B0%7D%5C%5C" alt="\text E[\tilde\beta|X]=\text E[\hat\beta+\xi|X]=\text E[\hat \beta|X]+\text E[\xi|X]\Rightarrow \text E[\xi|X]=\mathbf{0}\\" eeimg="1"> </p><p data-pid="JuzUlVZY">给定 <img src="https://www.zhihu.com/equation?tex=X" alt="X" eeimg="1"> 时 <img src="https://www.zhihu.com/equation?tex=%5Chat%5Cbeta" alt="\hat\beta" eeimg="1"> 与 <img src="https://www.zhihu.com/equation?tex=%5Cxi" alt="\xi" eeimg="1"> 的协方差矩阵为（其中 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"> 与 <img src="https://www.zhihu.com/equation?tex=%5Cxi" alt="\xi" eeimg="1"> 无关）：</p><p data-pid="IUEs4q3C"><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Ctext%7BCov%7D%28%5Chat%5Cbeta%2C%5Cxi%29%26%3DE%28%28%5Chat%5Cbeta-E%5Chat%5Cbeta%29%28%5Cxi-E%5Cxi%29%5ET%29%3DE%28%28%5Chat%5Cbeta-E%5Chat%5Cbeta%29%5Cxi%5ET%29%5C%5C+%26%3DE%28%5Chat%5Cbeta%5Cxi%5ET%29-E%5Chat%5Cbeta+%28E%5Cxi%29%5ET%3DE%28%5Chat%5Cbeta%5Cxi%5ET%29%5C%5C+%26%3DE%28%28X%5ETX%29%5E%7B-1%7DX%5ET%28X%5Cbeta%2B%5Cepsilon%29%5Cxi%5ET%29%5C%5C+%26%3DE%28%5Cbeta%5Cxi%5ET%2B%28X%5ETX%29%5E%7B-1%7DX%5ET%5Cepsilon%5Cxi%5ET%29%5C%5C+%26%3D%5Cbeta+E%28%5Cxi%29%5ET%2B%28X%5ETX%29%5E%7B-1%7DX%5ETE%28%5Cepsilon%29E%28%5Cxi%29%5ET%5C%5C+%26%3D%5Cmathbf%7B0%7D+%5Cend%7Balign%2A%7D%5C%5C" alt="\begin{align*} \text{Cov}(\hat\beta,\xi)&amp;=E((\hat\beta-E\hat\beta)(\xi-E\xi)^T)=E((\hat\beta-E\hat\beta)\xi^T)\\ &amp;=E(\hat\beta\xi^T)-E\hat\beta (E\xi)^T=E(\hat\beta\xi^T)\\ &amp;=E((X^TX)^{-1}X^T(X\beta+\epsilon)\xi^T)\\ &amp;=E(\beta\xi^T+(X^TX)^{-1}X^T\epsilon\xi^T)\\ &amp;=\beta E(\xi)^T+(X^TX)^{-1}X^TE(\epsilon)E(\xi)^T\\ &amp;=\mathbf{0} \end{align*}\\" eeimg="1"> </p><p data-pid="I8IjSKUa">则任意无偏估计的方差：</p><p data-pid="AKDfqvLB"><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D++%5Ctext%7BVar%7D%28%5Ctilde%5Cbeta%7CX%29%26%3D%5Ctext%7BVar%7D%28%5Chat%5Cbeta%2B%5Cxi%7CX%29%5C%5C++%26%3D%5Ctext%7BVar%7D%28%5Chat%5Cbeta%7CX%29%2B%5Ctext%7BVar%7D%28%5Cxi%7CX%29%2B2%5Ctext%7BCov%7D%28%5Chat%5Cbeta%2C%5Cxi%7CX%29%5C%5C+%26%3D%5Ctext%7BVar%7D%28%5Chat%5Cbeta%7CX%29%2B%5Ctext+E%5B%28%5Cxi-%5Ctext+E%5B%5Cxi%7CX%5D%29%5Ccdot%28%5Cxi-%5Ctext+E%5B%5Cxi%7CX%5D%29%5ET%7CX%5D%5C%5C+%26%3D%5Ctext%7BVar%7D%28%5Chat%5Cbeta%7CX%29%2B%5Ctext+E%5B%5Cxi%5Ccdot%5Cxi%5ET%7CX%5D-%5Ctext+E%5B%5Cxi%7CX%5D%5Ccdot%5Ctext+E%5B%5Cxi%7CX%5D%5ET%5C%5C+%26%3D%5Ctext%7BVar%7D%28%5Chat%5Cbeta%7CX%29%2B%5Ctext+E%5B%5Cxi%5Ccdot%5Cxi%5ET%7CX%5D%5Cend%7Balign%2A%7D%5C%5C" alt="\begin{align*}  \text{Var}(\tilde\beta|X)&amp;=\text{Var}(\hat\beta+\xi|X)\\  &amp;=\text{Var}(\hat\beta|X)+\text{Var}(\xi|X)+2\text{Cov}(\hat\beta,\xi|X)\\ &amp;=\text{Var}(\hat\beta|X)+\text E[(\xi-\text E[\xi|X])\cdot(\xi-\text E[\xi|X])^T|X]\\ &amp;=\text{Var}(\hat\beta|X)+\text E[\xi\cdot\xi^T|X]-\text E[\xi|X]\cdot\text E[\xi|X]^T\\ &amp;=\text{Var}(\hat\beta|X)+\text E[\xi\cdot\xi^T|X]\end{align*}\\" eeimg="1"> </p><p data-pid="FXwNllvS">显然矩阵 <img src="https://www.zhihu.com/equation?tex=%5Cxi%5Cxi%5ET" alt="\xi\xi^T" eeimg="1"> 的秩为1：其中一个特征值为 <img src="https://www.zhihu.com/equation?tex=%5Cxi%5ET%5Cxi%3D%5CVert%5Cxi%5CVert_2%5E2%5Cgeq+0" alt="\xi^T\xi=\Vert\xi\Vert_2^2\geq 0" eeimg="1"> ，其余特征值都为0。也就是说 <img src="https://www.zhihu.com/equation?tex=%5Cxi%5Cxi%5ET" alt="\xi\xi^T" eeimg="1"> 是一个半正定矩阵（可以用二次型证明）当且仅当 <img src="https://www.zhihu.com/equation?tex=%5Cxi" alt="\xi" eeimg="1"> 为0向量时，无偏估计的方差最小，等于最小二乘法估计的方差：</p><p data-pid="3nlG3JJF"><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Ctext%7BVar%7D%28%5Chat%5Cbeta%7CX%29%26%3D%5Ctext%7BVar%7D%28%28X%5ETX%29%5E%7B-1%7DX%5ETY%7CX%29%5C%5C+%26%3D%5Ctext%7BVar%7D%28%28X%5ETX%29%5E%7B-1%7DX%5ET%28X%5Cbeta%2B%5Cepsilon%29%7CX%29%5C%5C+%26%3D%5Ctext%7BVar%7D%28%5Cbeta%2B%28X%5ETX%29%5E%7B-1%7DX%5ET%5Cepsilon%7CX%29%5C%5C+%26%3D0%2B%28X%5ETX%29%5E%7B-1%7DX%5ET%5Ctext%7BVar%7D%28%5Cepsilon%7CX%29%5B%28X%5ETX%29%5E%7B-1%7DX%5ET%5D%5ET%5C%5C+%26%3D%28X%5ETX%29%5E%7B-1%7DX%5ET%5Csigma%5E2X%28X%5ETX%29%5E%7B-T%7D%5C%5C+%26%3D%5Csigma%5E2%28X%5ETX%29%5E%7B-T%7D%3D%5Csigma%5E2%28X%5ETX%29%5E%7B-1%7D+%5Cend%7Balign%2A%7D%5C%5C" alt="\begin{align*} \text{Var}(\hat\beta|X)&amp;=\text{Var}((X^TX)^{-1}X^TY|X)\\ &amp;=\text{Var}((X^TX)^{-1}X^T(X\beta+\epsilon)|X)\\ &amp;=\text{Var}(\beta+(X^TX)^{-1}X^T\epsilon|X)\\ &amp;=0+(X^TX)^{-1}X^T\text{Var}(\epsilon|X)[(X^TX)^{-1}X^T]^T\\ &amp;=(X^TX)^{-1}X^T\sigma^2X(X^TX)^{-T}\\ &amp;=\sigma^2(X^TX)^{-T}=\sigma^2(X^TX)^{-1} \end{align*}\\" eeimg="1"> </p><p data-pid="rpxRpNB4">以上证明中，我们其实没有假设加上的这个 <img src="https://www.zhihu.com/equation?tex=%5Cxi" alt="\xi" eeimg="1"> 关于 <img src="https://www.zhihu.com/equation?tex=Y" alt="Y" eeimg="1"> 是线性的，如果我们引入 <img src="https://www.zhihu.com/equation?tex=%5Cxi%3DCY" alt="\xi=CY" eeimg="1"> ，其中 <img src="https://www.zhihu.com/equation?tex=C" alt="C" eeimg="1"> 是任意矩阵，则可以得到：</p><p data-pid="b8-LtjHe"><img src="https://www.zhihu.com/equation?tex=E%5Cxi%3DE%28CY%29%3DE%28CX%5Cbeta%2BC%5Cepsilon%29%3DCX%5Cbeta%3D%5Cmathbf+0%5C%5C" alt="E\xi=E(CY)=E(CX\beta+C\epsilon)=CX\beta=\mathbf 0\\" eeimg="1"> </p><p data-pid="3DRhzNfZ">要求上式对于所有 <img src="https://www.zhihu.com/equation?tex=X" alt="X" eeimg="1"> 恒成立，则 <img src="https://www.zhihu.com/equation?tex=C" alt="C" eeimg="1"> 必须满足 <img src="https://www.zhihu.com/equation?tex=CX%3D%5Cmathbf+0" alt="CX=\mathbf 0" eeimg="1"> 。于是得到</p><p data-pid="vD79e_VX"><img src="https://www.zhihu.com/equation?tex=%5Cxi%3DCX%5Cbeta%2BC%5Cepsilon%3DC%5Cepsilon%5C%5C" alt="\xi=CX\beta+C\epsilon=C\epsilon\\" eeimg="1"> </p><p data-pid="n2lPU750"><img src="https://www.zhihu.com/equation?tex=%5Ctilde%7B%5Cbeta%7D" alt="\tilde{\beta}" eeimg="1"> 的方差变为</p><p data-pid="BjYOAM1o"><img src="https://www.zhihu.com/equation?tex=%5Ctext%7BVar%7D%28%5Ctilde%5Cbeta%7CX%29%3D%5Ctext%7BVar%7D%28%5Chat%5Cbeta%7CX%29%2BE%28C%5Cepsilon%5Cepsilon%5ETC%5ET%29%3D%5Ctext%7BVar%7D%28%5Chat%5Cbeta%7CX%29%2B%5Csigma%5E2CC%5ET%5C%5C" alt="\text{Var}(\tilde\beta|X)=\text{Var}(\hat\beta|X)+E(C\epsilon\epsilon^TC^T)=\text{Var}(\hat\beta|X)+\sigma^2CC^T\\" eeimg="1"> </p><p data-pid="e4RQkzs5">这与网上更常见的证明方法一致：<a href="https://zhuanlan.zhihu.com/p/43387521" class="internal" target="_blank">清雅白鹿记：最小二乘法与高斯-马尔可夫定理</a>、<a href="https://zhuanlan.zhihu.com/p/34119477" class="internal" target="_blank">李家偉：OLS 线性回归的性质：高斯马尔科夫定理</a>，即本文的证明是等价的。</p><h2>最小二乘法并非最好的方法</h2><p data-pid="_61jzCiJ">本文涉及到的最小二乘法全称应为普通最小二乘法（OLS），不要认为它在无偏估计中是方差最小的就认为它是最好的方法。统计学习中的一个重要的概念就是方差与偏差的trade-off，参考<a href="http://link.zhihu.com/?target=https%3A//esl.hohoweiya.xyz/02-Overview-of-Supervised-Learning/2.9-Model-Selection-and-the-Bias-Variance-Tradeoff/index.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">ESL2.9</a>。我们可以将最小二乘法的估计误差期望按照如下方式进行分解：</p><p data-pid="4k7Xn4IK"><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Ctext%7BMSE%7D_%7B%5Cbeta%7D%28x_i%29%26%3D%5Ctext+E_%7B%5Cbeta%7D%5B%5CVert+%5Chat+%7By_i%7D-y_i%5CVert%5E2_2%5D%5C%5C+%26%3D%5Ctext+E_%7B%5Cbeta%7D%5B%5Chat+%7By_i%7D%5E2-2%5Chat+%7By_i%7D+y_i%2By_i%5E2%5D%5C%5C+%26%3D%5Ctext+E_%7B%5Cbeta%7D%5B%5Chat%7By_i%7D%5E2%5D-2y_i%5Ctext+E_%7B%5Cbeta%7D%5B%5Chat%7By_i%7D%5D%2By_i%5E2%5C%5C+%26%3D%5Ctext+E_%7B%5Cbeta%7D%5B%5Chat%7By_i%7D%5E2%5D-%28%5Ctext+E_%7B%5Cbeta%7D%5B%5Chat%7By_i%7D%5D%29%5E2%2B%28E_%7B%5Cbeta%7D%5B%5Chat+%7By_i%7D%5D%29%5E2-2y_i%5Ctext+E_%7B%5Cbeta%7D%5B%5Chat%7By_i%7D%5D%2By_i%5E2%5C%5C+%26%3D%5Ctext+E_%7B%5Cbeta%7D%28%5Chat%7By_i%7D-%5Ctext+E_%7B%5Cbeta%7D%5B%5Chat%7By_i%7D%5D%29%5E2%2B%28E_%7B%5Cbeta%7D%5B%5Chat+%7By_i%7D%5D-y_i%29%5E2%5C%5C+%26%3D%5Ctext%7BVar%7D_%7B%5Cbeta%7D%28%5Chat+%7By_i%7D%29%2B%5Ctext%7BBias%7D_%7B%5Cbeta%7D%5E2%28%5Chat+%7By_i%7D%29+%5Cend%7Balign%2A%7D%5C%5C" alt="\begin{align*} \text{MSE}_{\beta}(x_i)&amp;=\text E_{\beta}[\Vert \hat {y_i}-y_i\Vert^2_2]\\ &amp;=\text E_{\beta}[\hat {y_i}^2-2\hat {y_i} y_i+y_i^2]\\ &amp;=\text E_{\beta}[\hat{y_i}^2]-2y_i\text E_{\beta}[\hat{y_i}]+y_i^2\\ &amp;=\text E_{\beta}[\hat{y_i}^2]-(\text E_{\beta}[\hat{y_i}])^2+(E_{\beta}[\hat {y_i}])^2-2y_i\text E_{\beta}[\hat{y_i}]+y_i^2\\ &amp;=\text E_{\beta}(\hat{y_i}-\text E_{\beta}[\hat{y_i}])^2+(E_{\beta}[\hat {y_i}]-y_i)^2\\ &amp;=\text{Var}_{\beta}(\hat {y_i})+\text{Bias}_{\beta}^2(\hat {y_i}) \end{align*}\\" eeimg="1"> </p><p data-pid="PnzErZeZ">其他方式建模的损失函数通常都可以构造成方差项和偏差项相加的形式，在实际应用中我们要找到二者之间一个合理的平衡点，使得模型尽量准确，但同时不能过拟合，具有优秀的泛化能力。</p><p data-pid="89g-UDnJ">观察最小二乘法的方差公式，在输入变量线性相关性较强时， <img src="https://www.zhihu.com/equation?tex=%28X%5ETX%29%5E%7B-1%7D" alt="(X^TX)^{-1}" eeimg="1"> 较大，模型具有较大的方差，也就是容易过拟合。在实际应用中，我们常常会使用一些有偏估计方法，比如岭回归和LASSO回归，牺牲掉一部分无偏性，来大幅度减小方差。在岭回归中，参数表达式为</p><p data-pid="-Dnp67yJ"><img src="https://www.zhihu.com/equation?tex=%5Chat%5Cbeta_%7Bridge%7D%3D%28X%5ETX%2B%5Clambda+I%29%5E%7B-1%7DX%5ETY%5C%5C" alt="\hat\beta_{ridge}=(X^TX+\lambda I)^{-1}X^TY\\" eeimg="1"> </p><p data-pid="HwFGbcOB">其中 <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"> 是L2正则惩罚项的系数。可以看出逆矩阵部分多了一项 <img src="https://www.zhihu.com/equation?tex=%5Clambda+I" alt="\lambda I" eeimg="1"> ，直观上讲可以使得最后的方差表达式更小，具体推导我不会:(。除此之外最小二乘法这一类的方法还存在矩阵求逆困难等问题。当然也要首先保证这是一个线性模型，否则要换个基底或者用别的方法了。</p><p data-pid="YwY4bkdb">总之要记住一句话，最小二乘法太蓝（BLUE，Best Linear Unbiased Estimator）了！</p><hr><h2>错误的历史版本（2022年7月2日最新更新）</h2><h3>1. 2021年12月16日更新：方差推导过程勘误</h3><p data-pid="EtjQSTae">之前版本的文章在证明「最小二乘法是无偏估计中最优方法」时的问题背景如下</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-aaca326b8019f88f0ea69719eeec97da_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="763" data-rawheight="471" class="origin_image zh-lightbox-thumb" width="763" data-original="https://picx.zhimg.com/v2-aaca326b8019f88f0ea69719eeec97da_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='763'%20height='471'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="763" data-rawheight="471" class="origin_image zh-lightbox-thumb lazy" width="763" data-original="https://picx.zhimg.com/v2-aaca326b8019f88f0ea69719eeec97da_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-aaca326b8019f88f0ea69719eeec97da_720w.jpg?source=d16d100b"></figure><p data-pid="v1QjMn6T">计算方差时有如下推导过程：</p><figure data-size="normal"><noscript><img src="https://pica.zhimg.com/v2-052a954be835f3b61a15fd205dd38617_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="797" data-rawheight="284" class="origin_image zh-lightbox-thumb" width="797" data-original="https://pic1.zhimg.com/v2-052a954be835f3b61a15fd205dd38617_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='797'%20height='284'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="797" data-rawheight="284" class="origin_image zh-lightbox-thumb lazy" width="797" data-original="https://pic1.zhimg.com/v2-052a954be835f3b61a15fd205dd38617_720w.jpg?source=d16d100b" data-actualsrc="https://pica.zhimg.com/v2-052a954be835f3b61a15fd205dd38617_720w.jpg?source=d16d100b"></figure><p data-pid="SogoDKAt">感谢 <a class="member_mention" href="http://www.zhihu.com/people/03f817b1fd9ec29937df2f9749d357ef" data-hash="03f817b1fd9ec29937df2f9749d357ef" data-hovercard="p$b$03f817b1fd9ec29937df2f9749d357ef">@zhb</a> 指出我的疏漏：蓝框内 <img src="https://www.zhihu.com/equation?tex=D%5ETD" alt="D^TD" eeimg="1"> 只能保证是实对称矩阵，既不能保证每个元素非负，也不保证正定。这一步错误其实是上面箭头处的推导错误所导致的，在我没有用到之前的无偏条件时就应该意识到，偏差约束松弛后我应该根本证不出来才对。由于 <img src="https://www.zhihu.com/equation?tex=D" alt="D" eeimg="1"> 并不是常量（有可能是 <img src="https://www.zhihu.com/equation?tex=%5Cbeta" alt="\beta" eeimg="1"> 的函数）所以不能直接移到Var括号外：</p><p data-pid="LptU-eo1"><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Ctext%7BVar%7D%28%5Ctilde%7B%5Cbeta%7D%7CX%29%26%3D%5Ctext%7BVar%7D%28%5Chat%7B%5Cbeta%7D%2BDY%7CX%29%5C%5C+%26%3D%5Ctext%7BVar%7D%28%5Chat%7B%5Cbeta%7D%7CX%29%2B%5Ctext%7BVar%7D%28DY%7CX%29%5C%5C+%26%3D%5Ctext%7BVar%7D%28%5Chat%7B%5Cbeta%7D%7CX%29%2B%5Ctext%7BE%7D%5B%28DY-%5Ctext+E%28DY%7CX%29%29%5ET%28DY-%5Ctext+E%28DY%7CX%29%29%7CX%5D%5C%5C+%26%3D%5Ctext%7BVar%7D%28%5Chat%7B%5Cbeta%7D%7CX%29%2B%5Ctext%7BE%7D%5B%28DY%29%5ETDY%7CX%5D-%5Ctext%7BE%7D%5BDY%7CX%5D%5ET%5Ctext%7BE%7D%5BDY%7CX%5D%5C%5C+%26%3D%5Ctext%7BVar%7D%28%5Chat%7B%5Cbeta%7D%7CX%29%2B%5Ctext%7BE%7D%5B%28DY%29%5ETDY%7CX%5D-0%5C%5C+%26%3D%5Ctext%7BVar%7D%28%5Chat%7B%5Cbeta%7D%7CX%29%2B%5Ctext+E%5B%5CVert%5Cxi%5CVert%5E2_%7B2%7D%7CX%5D%5C%5C+%26%5Cgeq%5Ctext%7BVar%7D%28%5Chat%7B%5Cbeta%7D%7CX%29+%5Cend%7Balign%2A%7D%5C%5C" alt="\begin{align*} \text{Var}(\tilde{\beta}|X)&amp;=\text{Var}(\hat{\beta}+DY|X)\\ &amp;=\text{Var}(\hat{\beta}|X)+\text{Var}(DY|X)\\ &amp;=\text{Var}(\hat{\beta}|X)+\text{E}[(DY-\text E(DY|X))^T(DY-\text E(DY|X))|X]\\ &amp;=\text{Var}(\hat{\beta}|X)+\text{E}[(DY)^TDY|X]-\text{E}[DY|X]^T\text{E}[DY|X]\\ &amp;=\text{Var}(\hat{\beta}|X)+\text{E}[(DY)^TDY|X]-0\\ &amp;=\text{Var}(\hat{\beta}|X)+\text E[\Vert\xi\Vert^2_{2}|X]\\ &amp;\geq\text{Var}(\hat{\beta}|X) \end{align*}\\" eeimg="1"> </p><p data-pid="uRYo3dqk">其中 <img src="https://www.zhihu.com/equation?tex=%5Cxi%3DDY" alt="\xi=DY" eeimg="1"> 是一个d维向量，而这里我们就用到了之前的无偏的条件 <img src="https://www.zhihu.com/equation?tex=E%5BDY%7CX%5D%3D0" alt="E[DY|X]=0" eeimg="1"> 。其实这个证明过程可以更简单，是我给搞复杂了。假设的不太好，原文已更新。另外，高斯马尔可夫定理的介绍也可以参考ESL的3.2.2小节，ESL中文版电子书地址：<a href="http://link.zhihu.com/?target=https%3A//esl.hohoweiya.xyz/index.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">ESL CN</a>。其中作业题Ex. 3.3就是这个定理的证明，在上述中文电子书的仓库下，作者用issue维护了作业题的答案，简直用爱发电，参考：<a href="http://link.zhihu.com/?target=https%3A//github.com/szcf-weiya/ESL-CN/issues/70" class=" wrap external" target="_blank" rel="nofollow noreferrer">Ex. 3.3 定理证明</a>。</p><h3>2. 2022年7月2日更新：方差推导过程勘误</h3><p data-pid="hR0ah4pu">感谢 <a class="member_mention" href="http://www.zhihu.com/people/2ebb18de832a6e3dd2db1f0fb82ed405" data-hash="2ebb18de832a6e3dd2db1f0fb82ed405" data-hovercard="p$b$2ebb18de832a6e3dd2db1f0fb82ed405">@随机森林里的剑龙</a> 指正，在上次勘误之后，方差的推导过程仍然有问题。这里是2021年12月16日到2022年7月2日的版本：</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-18edf4b1abe66e075014c9f0cbf5e829_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="753" data-rawheight="553" class="origin_image zh-lightbox-thumb" width="753" data-original="https://picx.zhimg.com/v2-18edf4b1abe66e075014c9f0cbf5e829_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='753'%20height='553'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="753" data-rawheight="553" class="origin_image zh-lightbox-thumb lazy" width="753" data-original="https://picx.zhimg.com/v2-18edf4b1abe66e075014c9f0cbf5e829_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-18edf4b1abe66e075014c9f0cbf5e829_720w.jpg?source=d16d100b"></figure><p data-pid="NlKww8ma">我们引入了一个任意向量 <img src="https://www.zhihu.com/equation?tex=%5Cxi" alt="\xi" eeimg="1"> ，原意是用向量的二范数 <img src="https://www.zhihu.com/equation?tex=%5CVert+%5Cxi+%5CVert_2%5E2" alt="\Vert \xi \Vert_2^2" eeimg="1"> 来替代矩阵乘积 <img src="https://www.zhihu.com/equation?tex=DD%5ET" alt="DD^T" eeimg="1"> 更方便放缩，然而我们不能假定这个向量是独立于 <img src="https://www.zhihu.com/equation?tex=%5Chat+%5Cbeta" alt="\hat \beta" eeimg="1"> 的，因此在方差推导的第一行到第二行处丢失了一项协方差：</p><p data-pid="KN_9euGP"><img src="https://www.zhihu.com/equation?tex=%5Ctext%7BVar%7D%28%5Chat+%5Cbeta%2B%5Cxi%7CX%29%3D%5Ctext%7BVar%7D%28%5Chat+%5Cbeta%7CX%29%2B%5Ctext%7BVar%7D%28%5Cxi%7CX%29%2B2%5Ctext%7BCov%7D%28%5Chat+%5Cbeta%2C%5Cxi%7CX%29%5C%5C" alt="\text{Var}(\hat \beta+\xi|X)=\text{Var}(\hat \beta|X)+\text{Var}(\xi|X)+2\text{Cov}(\hat \beta,\xi|X)\\" eeimg="1"> </p><p data-pid="MxJgHInJ">同时下面的推导也不严谨。 <img src="https://www.zhihu.com/equation?tex=%5Ctext%7BVar%7D%28%5Cxi%7CX%29" alt="\text{Var}(\xi|X)" eeimg="1"> 应该是一个p维的方阵，所以后面并不能得到 <img src="https://www.zhihu.com/equation?tex=%5Cxi" alt="\xi" eeimg="1"> 的二范数，而应该是：</p><p data-pid="H8RCdGFJ"><img src="https://www.zhihu.com/equation?tex=%5Ctext%7BVar%7D%28%5Cxi%7CX%29%3DE%28%5Cxi%5Ccdot%5Cxi%5ET%7CX%29%5C%5C" alt="\text{Var}(\xi|X)=E(\xi\cdot\xi^T|X)\\" eeimg="1"> </p><p data-pid="n6aRnLgs">更正后的证明过程已在最新版的正文中给出，如果大家发现还有什么错误或者不理解的地方欢迎讨论。</p>