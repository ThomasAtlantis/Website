<p data-pid="Kyn12Hz3">各位达瓦里希，本文是LLM推理开销优化系列的第一篇，将来会慢慢更新。<b>您在收藏的同时点个赞可以加快清川的更新速度。</b>本系列拒绝机翻，主张编译而不是翻译，加入了很多个人的理解和证明，欢迎探讨！</p><p data-pid="gpJT0_gS"><a href="http://link.zhihu.com/?target=https%3A//scholar.google.com/citations%3Fuser%3Dhjdlwz8AAAAJ%26hl%3Den%26oi%3Dsra" class=" wrap external" target="_blank" rel="nofollow noreferrer">E Frantar</a>、<a href="http://link.zhihu.com/?target=https%3A//scholar.google.com/citations%3Fuser%3D75q-6ZQAAAAJ%26hl%3Den%26oi%3Dsra" class=" wrap external" target="_blank" rel="nofollow noreferrer">D Alistarh</a>：<a href="http://link.zhihu.com/?target=https%3A//proceedings.mlr.press/v202/frantar23a/frantar23a.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">Massive Language Models Can be Accurately Pruned in One-Shot</a>，ICML'23</p><h2><b>研究背景</b></h2><p data-pid="nezSTwCA">目前针对大模型的压缩方法主要集中在量化。在大模型兴起前，量化就是最快速有效的压缩算法，在工业界有广泛的应用。相比之下，剪枝的效果就不那么稳定了。一般来说，剪枝后模型性能损失较大，需要重训练来弥补，而无需重训练的剪枝算法本身复杂度较高，对于大规模预训练、参数量巨大的大模型开销无法接受。</p><h2><b>研究摘要</b></h2><p data-pid="mUz2KboI">文章提出的 SparseGPT 剪枝算法，<u>无需重训练（one-shot）</u>，即可将 GPT 系列模型剪枝到 50% 以上稀疏度，同时将准确率损失降至最低。对 OPT-175B 和 BLOOM-176B 进行剪枝，可以在不到 4.5 小时达到 60% 的非结构化稀疏度，而 perplexity 的增加可以忽略不计。更直观地换算，推理时忽略了 1000 多亿个权重参数。SparseGPT 可以推广到半结构化（2:4 和 4:8）模式，并且与量化方法兼容。参见 <a href="http://link.zhihu.com/?target=https%3A//github.com/IST-DASLab/sparsegpt" class=" wrap external" target="_blank" rel="nofollow noreferrer">Github 仓库</a>。本工作可以说是<b>首个在千亿级模型上可用的剪枝算法</b>。</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-3b3c6db0306c382f35ec2b0479fa8b9a_720w.jpg?source=d16d100b" data-rawwidth="2000" data-rawheight="637" data-size="normal" data-caption="" class="origin_image zh-lightbox-thumb" width="2000" data-original="https://picx.zhimg.com/v2-3b3c6db0306c382f35ec2b0479fa8b9a_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='2000'%20height='637'&gt;&lt;/svg&gt;" data-rawwidth="2000" data-rawheight="637" data-size="normal" data-caption="" class="origin_image zh-lightbox-thumb lazy" width="2000" data-original="https://picx.zhimg.com/v2-3b3c6db0306c382f35ec2b0479fa8b9a_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-3b3c6db0306c382f35ec2b0479fa8b9a_720w.jpg?source=d16d100b"></figure><p data-pid="f0QwJ7ZA">左图：在 OPT-175B 模型上使用 SparseGPT 和按照数值大小剪枝的稀疏度 vs. perplexity</p><p data-pid="0z3qedcT">右图：剪枝前模型参数量 vs. perplexity（颜色代表不同的剪枝类型，曲线上的点代表OPT系列的不同模型）</p><h2><b>补充知识</b></h2><p data-pid="Kj_oPg94">本文讨论的是训练后剪枝（Post-Training Pruning），即给定一个优化完毕的模型 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E%2A" alt="\theta^*" eeimg="1"> 和一些校准数据，获得压缩后的模型。这种模式最早流行于量化方法，后来也成功扩展到模型剪枝。</p><p data-pid="4k3h0afJ">训练后剪枝通常通过逐层剪枝（Layer-Wise Pruning）的方法实现。对于每个网络层 <img src="https://www.zhihu.com/equation?tex=l" alt="l" eeimg="1">，给定原始权重 <img src="https://www.zhihu.com/equation?tex=%5Ctextbf%7BW%7D_l" alt="\textbf{W}_l" eeimg="1"> 和该层的输入 <img src="https://www.zhihu.com/equation?tex=%5Ctextbf%7Bx%7D_l" alt="\textbf{x}_l" eeimg="1"> ，计算一个满足目标稀疏度的稀疏掩码矩阵 <img src="https://www.zhihu.com/equation?tex=%5Ctextbf%7BM%7D_l" alt="\textbf{M}_l" eeimg="1"> 和校准后的权重 <img src="https://www.zhihu.com/equation?tex=%5Cwidehat%7B%5Ctextbf%7BW%7D%7D_l" alt="\widehat{\textbf{W}}_l" eeimg="1"> 以满足：</p><p data-pid="0kV_08Vv"><img src="https://www.zhihu.com/equation?tex=%5Carg+%5Cmin_%7B%5Ctextbf%7BM%7D_l%2C%5Cwidehat%7B%5Ctextbf%7BW%7D%7D_l%7D%5CVert+%5Ctextbf%7BW%7D_l+%5Ctextbf%7Bx%7D_l+-+%28%5Ctextbf%7BM%7D_l+%5Codot+%5Cwidehat%7B%5Ctextbf%7BW%7D%7D_l%29+%5Ctextbf%7Bx%7D_l%5CVert%5E2_2+%5Ctag%7B1%7D" alt="\arg \min_{\textbf{M}_l,\widehat{\textbf{W}}_l}\Vert \textbf{W}_l \textbf{x}_l - (\textbf{M}_l \odot \widehat{\textbf{W}}_l) \textbf{x}_l\Vert^2_2 \tag{1}" eeimg="1">然后再将压缩后的各层缝合在一起。</p><p data-pid="zpT2Ur_o">以上建模方式将掩码矩阵 <img src="https://www.zhihu.com/equation?tex=%5Ctextbf%7BM%7D_l" alt="\textbf{M}_l" eeimg="1"> 和校准权重 <img src="https://www.zhihu.com/equation?tex=%5Cwidehat%7B%5Ctextbf%7BW%7D%7D_l" alt="\widehat{\textbf{W}}_l" eeimg="1"> 联合优化，掩码优化等价于最优子集选择问题，本身就是NP难问题，二者联合优化难上加难。现有的方法将问题拆分为<u>掩码选择</u>和<u>权重重建</u>两个步骤：先使用某种显著性指标（例如权重的数值大小）选择一个掩码，然后将其固定，剩下的权重优化就变成线性回归问题，变得好解。</p><p data-pid="N_hefm-o"><a href="http://link.zhihu.com/?target=https%3A//proceedings.neurips.cc/paper_files/paper/2021/file/b0490b85e92b64dbb5db76bf8fca6a82-Paper.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">AdaPrune</a> 在基于数值大小剪枝的基础上加入 SGD 优化，后续其迭代版本 <a href="http://link.zhihu.com/?target=https%3A//proceedings.mlr.press/v162/frantar22a/frantar22a.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">SPDY</a> 采用类似轮换优化的方法，在掩码选择的过程中穿插权重重建，<a href="http://link.zhihu.com/?target=https%3A//proceedings.neurips.cc/paper_files/paper/2022/file/1caf09c9f4e6b0150b06a07e77f2710c-Paper-Conference.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">OBC</a> 更进一步，每次只删除一个权重，并对所有权重进行重建，得到了公式的闭式解。然而这些方法在面对大模型时显得十分吃力，175B的模型使用 AdaPrune 剪枝需要好几周才能跑完，如果用后面更精确的版本，可能还要慢上好几倍。</p><h2>算法设计</h2><h3>闭式解的复杂度</h3><p data-pid="cxi1doxJ">在选取掩码后，以上最优化问题可以得到闭式解如下，参考<a href="https://zhuanlan.zhihu.com/p/353847151" class="internal" target="_blank">最小二乘法估计公式</a>：<img src="https://www.zhihu.com/equation?tex=%5Ctext%7Bw%7D_%7B%5Ctext%7BM%7D_i%7D%5Ei%3D%28%5Ctextbf%7Bx%7D_%7B%5Ctext%7BM%7D_i%7D%5Ctextbf%7Bx%7D_%7B%5Ctext%7BM%7D_i%7D%5E%5Ctop%29%5E%7B-1%7D%5Ctextbf%7Bx%7D_%7B%5Ctext%7BM%7D_i%7D%28%5Ctext%7Bw%7D_%7B%5Ctext%7BM%7D_i%7D%5Ctextbf%7Bx%7D_%7B%5Ctext%7BM%7D_i%7D%29%5E%5Ctop+%5Ctag%7B2%7D" alt="\text{w}_{\text{M}_i}^i=(\textbf{x}_{\text{M}_i}\textbf{x}_{\text{M}_i}^\top)^{-1}\textbf{x}_{\text{M}_i}(\text{w}_{\text{M}_i}\textbf{x}_{\text{M}_i})^\top \tag{2}" eeimg="1">其中，<img src="https://www.zhihu.com/equation?tex=%5Ctext+M_i" alt="\text M_i" eeimg="1"> 为掩码 <img src="https://www.zhihu.com/equation?tex=%5Ctext+M" alt="\text M" eeimg="1"> 的第 <img src="https://www.zhihu.com/equation?tex=i" alt="i" eeimg="1"> 个行向量， <img src="https://www.zhihu.com/equation?tex=%5Ctext%7Bw%7D_%7B%5Ctext%7BM%7D_i%7D%5Ei" alt="\text{w}_{\text{M}_i}^i" eeimg="1"> 表示第 <img src="https://www.zhihu.com/equation?tex=i" alt="i" eeimg="1"> 行没有被裁剪掉的权重，<img src="https://www.zhihu.com/equation?tex=%5Ctextbf%7Bx%7D_%7B%5Ctext%7BM%7D_i%7D" alt="\textbf{x}_{\text{M}_i}" eeimg="1"> 表示对应的输入特征子集。这个求解过程需要求 <img src="https://www.zhihu.com/equation?tex=%5Ctextbf%7Bx%7D_%7B%5Ctext%7BM%7D_i%7D%5Ctextbf%7Bx%7D_%7B%5Ctext%7BM%7D_i%7D%5E%5Ctop" alt="\textbf{x}_{\text{M}_i}\textbf{x}_{\text{M}_i}^\top" eeimg="1"> 的逆矩阵，而它实际上是公式1中的目标函数的 Hessian 阵，推导过程如下。</p><p data-pid="QnQQd4F_"><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%26%5Cnabla_%7B%5Cwidehat%7B%5Cmathbf+%7BW%7D%7D%7D%5CVert+%5Ctextbf%7BW%7D+%5Ctextbf%7Bx%7D+-+%28%5Ctextbf%7BM%7D+%5Codot+%5Cwidehat%7B%5Ctextbf%7BW%7D%7D%29+%5Ctextbf%7Bx%7D%5CVert%5E2_2+%5C%5C+%26%3D+-%5Cmathbf+M+%5Codot+%5Cleft%28+2%28%5Ctextbf+W+%5Ctextbf+x-%28%5Ctextbf+M+%5Codot+%5Cwidehat%7B%5Ctextbf+W%7D%29%5Ctextbf+x%29%5Ctextbf+x+%5E%5Ctop+%5Cright%29+%5C%5C+%26%3D+-2+%5Cmathbf+M+%5Codot+%28%5Ctextbf+W+%5Ctextbf+x+%5Ctextbf+x%5E%5Ctop%29+%2B2+%5Cmathbf+M+%5Codot+%5Cleft%28+%28%5Ctextbf+M+%5Codot+%5Cwidehat%7B%5Ctextbf+W%7D%29%5Ctextbf+x%5Ctextbf+x+%5E%5Ctop%5Cright%29+%5Cend%7Balign%2A%7D+%5C%5C" alt="\begin{align*} &amp;\nabla_{\widehat{\mathbf {W}}}\Vert \textbf{W} \textbf{x} - (\textbf{M} \odot \widehat{\textbf{W}}) \textbf{x}\Vert^2_2 \\ &amp;= -\mathbf M \odot \left( 2(\textbf W \textbf x-(\textbf M \odot \widehat{\textbf W})\textbf x)\textbf x ^\top \right) \\ &amp;= -2 \mathbf M \odot (\textbf W \textbf x \textbf x^\top) +2 \mathbf M \odot \left( (\textbf M \odot \widehat{\textbf W})\textbf x\textbf x ^\top\right) \end{align*} \\" eeimg="1">可以看到标量对权重矩阵的一阶梯度结果与该矩阵同维度，那么 Hessian 阵将会是更高阶的矩阵，不太好直接表示。于是，我们取掩码矩阵的每一行（即对于线性层的每一个输出通道）单独讨论。</p><p data-pid="CmCl07Nq"><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cnabla_%7B%5Cwidehat%7B%5Ctext+%7Bw%7D%7D%5Ei%7D%26%3D-2+%5Ctext+M_i+%5Codot+%28%5Ctext+w%5Ei+%5Ctextbf+x+%5Ctextbf+x+%5E%5Ctop%29+%2B2+%5Ctext+M_i+%5Codot+%5Cleft%28+%28%5Ctext+M_i+%5Codot+%5Cwidehat%7B%5Ctext+w%7D%5Ei%29%5Ctextbf+x%5Ctextbf+x+%5E%5Ctop%5Cright%29+%5Cend%7Balign%2A%7D%5C%5C" alt="\begin{align*} \nabla_{\widehat{\text {w}}^i}&amp;=-2 \text M_i \odot (\text w^i \textbf x \textbf x ^\top) +2 \text M_i \odot \left( (\text M_i \odot \widehat{\text w}^i)\textbf x\textbf x ^\top\right) \end{align*}\\" eeimg="1">于是局部的 Hessian 矩阵转换为为向量对向量的偏导计算。该矩阵的第 <img src="https://www.zhihu.com/equation?tex=k" alt="k" eeimg="1"> 行：</p><p data-pid="rznqlwpP"><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%28%5Cnabla_%7B%5Cwidehat%7B%5Ctext+%7Bw%7D%7D%5Ei%7D%5E2%29_k%5E%5Ctop+%26%3D+%5Cpartial%28%5Cfrac%7B%5Cpartial+%7D%7B%5Cpartial+%5Cwidehat+%7B%5Ctext+w%7D%5Ei%7D%29_k+%2F+%5Cpartial+%5Cwidehat+%7B%5Ctext+w%7D%5Ei%5C%5C++%26%3D%5Cpartial+%5Cleft%282+%5Ctext+M_%7Bi%2Ck%7D++%5Ctextbf+x_k+%28+%28%5Ctext+M_i+%5Codot+%5Cwidehat%7B%5Ctext+w%7D%5Ei%29%5Ctextbf+x+%29%5Cright%29%2F+%5Cpartial+%5Cwidehat+%7B%5Ctext+w%7D%5Ei+%5C%5C+%26%3D+2+%5Ctext+M_%7Bi%2Ck%7D++%5Ctextbf+x_k+%28%5Ctext+M_i+%5Codot+%5Cmathbf+x%5E%5Ctop%29+%5Cend%7Balign%2A%7D%5C%5C" alt="\begin{align*} (\nabla_{\widehat{\text {w}}^i}^2)_k^\top &amp;= \partial(\frac{\partial }{\partial \widehat {\text w}^i})_k / \partial \widehat {\text w}^i\\  &amp;=\partial \left(2 \text M_{i,k}  \textbf x_k ( (\text M_i \odot \widehat{\text w}^i)\textbf x )\right)/ \partial \widehat {\text w}^i \\ &amp;= 2 \text M_{i,k}  \textbf x_k (\text M_i \odot \mathbf x^\top) \end{align*}\\" eeimg="1"></p><p data-pid="Fp1mDoX7">如果不考虑裁剪掉的权重和输入，即只考虑 <img src="https://www.zhihu.com/equation?tex=%5Ctext+M_%7Bi%2Ck%7D%3D1" alt="\text M_{i,k}=1" eeimg="1">，则 <img src="https://www.zhihu.com/equation?tex=%28%5Cnabla_%7B%5Cwidehat%7B%5Ctext+%7Bw%7D%7D%5Ei%7D%5E2%29_%7Bk%7D%5E%5Ctop%3D2+++%5Ctextbf+x_k+%5Cmathbf++x_%7B%5Ctext+M_i%7D%5E%5Ctop" alt="(\nabla_{\widehat{\text {w}}^i}^2)_{k}^\top=2   \textbf x_k \mathbf  x_{\text M_i}^\top" eeimg="1"> 。于是掩码矩阵的第 <img src="https://www.zhihu.com/equation?tex=i" alt="i" eeimg="1"> 行对应的 Hessian 矩阵 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf+H_%7B%5Ctext+M_i%7D+%3D+%5Cnabla_%7B%5Cwidehat%7B%5Ctext+%7Bw%7D%7D%5Ei%7D%5E2%3D2+%5Cmathbf+x_%7B%5Ctext+M_i%7D%5Cmathbf+x_%7B%5Ctext+M_i%7D%5E%5Ctop" alt="\mathbf H_{\text M_i} = \nabla_{\widehat{\text {w}}^i}^2=2 \mathbf x_{\text M_i}\mathbf x_{\text M_i}^\top" eeimg="1"> （忽略系数的差异，后面泰勒展开的时候 Hessian 阵这一项还有个 1/2 的系数刚好消掉2）。</p><p data-pid="ULtZeEZo">回到公式2，我们发现 <img src="https://www.zhihu.com/equation?tex=%28%5Ctextbf%7BH%7D_%7B%5Ctext+%7BM%7D_i%7D%29%5E%7B-1%7D" alt="(\textbf{H}_{\text {M}_i})^{-1}" eeimg="1"> 在 <img src="https://www.zhihu.com/equation?tex=d_%7Brow%7D" alt="d_{row}" eeimg="1"> 行上总的计算复杂度为 <img src="https://www.zhihu.com/equation?tex=O%28d_%7Brow%7D%5Ccdot+d_%7Bcol%7D%5E3%29" alt="O(d_{row}\cdot d_{col}^3)" eeimg="1"> 。对于 Transformer 模型，这相当于隐层维度的四次方复杂度，开销巨大。</p><p data-pid="8t9eI4MN">作者分析，开销瓶颈在于计算每一行权重时需要独立计算 <img src="https://www.zhihu.com/equation?tex=O%28d_%7Bcol%7D%5Ctimes+d_%7Bcol%7D%29" alt="O(d_{col}\times d_{col})" eeimg="1"> 的矩阵求逆，这是因为掩码的每一行一般都不相同，并且 <img src="https://www.zhihu.com/equation?tex=%28%5Ctextbf%7BH%7D_%7B%5Ctext+%7BM%7D_i%7D%29%5E%7B-1%7D%5Cneq+%28%5Ctextbf%7BH%7D%5E%7B-1%7D%29_%7B%5Ctext+%7BM%7D_i%7D" alt="(\textbf{H}_{\text {M}_i})^{-1}\neq (\textbf{H}^{-1})_{\text {M}_i}" eeimg="1"> 。如果在选择掩码时就加入使每一行都相同的约束，则只需计算一个共享的逆矩阵 <img src="https://www.zhihu.com/equation?tex=%5Ctextbf+H%5E%7B-1%7D%3D+%28%5Ctextbf%7BX%7D_%7B%5Ctext+M%7D%5Ctextbf%7BX%7D_%7B%5Ctext+M%7D%5E%5Ctop%29%5E%7B-1%7D" alt="\textbf H^{-1}= (\textbf{X}_{\text M}\textbf{X}_{\text M}^\top)^{-1}" eeimg="1"> ，但这种策略对剪枝后的模型性能有较大影响。</p><h3>近似迭代算法</h3><p data-pid="U2DQOWh3">由于每个网络层输出通道互相独立，我们依旧可以考虑逐行对权重矩阵进行剪枝。对于第 <img src="https://www.zhihu.com/equation?tex=i" alt="i" eeimg="1"> 行，公式1的目标函数是个二次型，参考<a href="http://link.zhihu.com/?target=https%3A//ieeexplore.ieee.org/abstract/document/298572/" class=" wrap external" target="_blank" rel="nofollow noreferrer">OBS</a>算法可知，假设当前的权重 <img src="https://www.zhihu.com/equation?tex=%5Ctext%7Bw%7D" alt="\text{w}" eeimg="1"> 是最优的，移除掉索引为 <img src="https://www.zhihu.com/equation?tex=m" alt="m" eeimg="1"> 的权重，对剩余权重可以进行最优调整： <img src="https://www.zhihu.com/equation?tex=%5Cdelta+%5Cmathbf%7Bw%7D_m%5E%2A+%3D-+%5Cfrac%7Bw_m%7D%7B%5B%5Ctextbf%7BH%7D%5E%7B-1%7D%5D_%7Bmm%7D%7D%5B%5Ctextbf%7BH%7D%5E%7B-1%7D%5D_%7B%3A%2Cm%7D%5C%5C" alt="\delta \mathbf{w}_m^* =- \frac{w_m}{[\textbf{H}^{-1}]_{mm}}[\textbf{H}^{-1}]_{:,m}\\" eeimg="1">证明过程见文末附录。 我们将 <img src="https://www.zhihu.com/equation?tex=%5Ctext+M_i" alt="\text M_i" eeimg="1"> 的初始值表示为列下标的全集，每次去掉一个下标 <img src="https://www.zhihu.com/equation?tex=m" alt="m" eeimg="1"> ，更新剩余权重只需要计算 <img src="https://www.zhihu.com/equation?tex=%28%5Cmathbf+H_%7B%5Ctext+M_i%7D+%29%5E%7B-1%7D" alt="(\mathbf H_{\text M_i} )^{-1}" eeimg="1"> ，这样就得到一个与公式2完全等价的迭代算法。</p><p data-pid="QGz9ge52">然而，我们每层去掉的下标顺序不尽相同，Hessian 矩阵的逆也不能复用。针对这个核心问题，<b>作者提出在移除某个权重后，仅更新剩余权重的一个子集，并要求这个子集在各行的迭代计算中统一</b>。</p><p data-pid="8axoP3_D">具体地，将网络层输入特征按照某种顺序排列： <img src="https://www.zhihu.com/equation?tex=j%3D1%2C%5Cdots%2Cd_%7Bcol%7D" alt="j=1,\dots,d_{col}" eeimg="1"> ，初始集合 <img src="https://www.zhihu.com/equation?tex=U_1" alt="U_1" eeimg="1"> 表示下标全集，每次从特征子集 <img src="https://www.zhihu.com/equation?tex=U_j" alt="U_j" eeimg="1"> 中去掉最小元素 <img src="https://www.zhihu.com/equation?tex=j" alt="j" eeimg="1"> （以表示去掉特征下标 <img src="https://www.zhihu.com/equation?tex=j" alt="j" eeimg="1"> 对应的权重）。于是，每次迭代我们只需要计算 Hessian 矩阵的子矩阵 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7BH%7D_%7BU_j%7D%3D%28%5Cmathbf%7Bxx%7D%5ET%29_%7BU_j%7D" alt="\mathbf{H}_{U_j}=(\mathbf{xx}^T)_{U_j}" eeimg="1"> 的逆。可以发现，这个子矩阵在各行计算中是相同的。</p><p data-pid="4fwZH-qj">并且，参考 <a href="http://link.zhihu.com/?target=https%3A//proceedings.neurips.cc/paper_files/paper/2022/file/1caf09c9f4e6b0150b06a07e77f2710c-Paper-Conference.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning</a>，可以通过递推公式，在 <img src="https://www.zhihu.com/equation?tex=O%28d_%7Bcol%7D%5E2%29" alt="O(d_{col}^2)" eeimg="1"> 的复杂度内从 <img src="https://www.zhihu.com/equation?tex=%28%5Cmathbf%7BH%7D_%7BU_j%7D%29%5E%7B-1%7D" alt="(\mathbf{H}_{U_j})^{-1}" eeimg="1"> 得到 <img src="https://www.zhihu.com/equation?tex=%28%5Cmathbf%7BH%7D_%7BU_%7Bj%2B1%7D%7D%29%5E%7B-1%7D" alt="(\mathbf{H}_{U_{j+1}})^{-1}" eeimg="1"> <img src="https://www.zhihu.com/equation?tex=%28%5Cmathbf%7BH%7D_%7BU_%7Bj%2B1%7D%7D%29%5E%7B-1%7D+%3D+%28%5Cmathbf%7BB%7D-%5Cfrac%7B1%7D%7B%5B%5Cmathbf%7BB%7D%5D_%7B11%7D%7D%5Ccdot+%5Cmathbf%7BB%7D_%7B%3A%2C+1%7D%5Cmathbf%7BB%7D_%7B1%2C%3A%7D%29_%7B2%3A%2C+2%3A%7D%5C%5C" alt="(\mathbf{H}_{U_{j+1}})^{-1} = (\mathbf{B}-\frac{1}{[\mathbf{B}]_{11}}\cdot \mathbf{B}_{:, 1}\mathbf{B}_{1,:})_{2:, 2:}\\" eeimg="1">其中 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7BB%7D%3D%28%5Cmathbf%7BH%7D_%7BU_%7Bj%7D%7D%29%5E%7B-1%7D" alt="\mathbf{B}=(\mathbf{H}_{U_{j}})^{-1}" eeimg="1"> 。于是对于 <img src="https://www.zhihu.com/equation?tex=d_%7Bcol%7D" alt="d_{col}" eeimg="1"> 个特征，总的计算复杂度为 <img src="https://www.zhihu.com/equation?tex=O%28d_%7Bcol%7D%5E3%29" alt="O(d_{col}^3)" eeimg="1"> 。作者对算法过程给出了一个非常直观的示意图：</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-0f84d221a62fe9931cdc18d0cfe027d4_720w.jpg?source=d16d100b" data-rawwidth="2027" data-rawheight="567" data-size="normal" data-caption="" class="origin_image zh-lightbox-thumb" width="2027" data-original="https://picx.zhimg.com/v2-0f84d221a62fe9931cdc18d0cfe027d4_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='2027'%20height='567'&gt;&lt;/svg&gt;" data-rawwidth="2027" data-rawheight="567" data-size="normal" data-caption="" class="origin_image zh-lightbox-thumb lazy" width="2027" data-original="https://picx.zhimg.com/v2-0f84d221a62fe9931cdc18d0cfe027d4_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-0f84d221a62fe9931cdc18d0cfe027d4_720w.jpg?source=d16d100b"></figure><p data-pid="SGIrwdnp">我们对于每一列，先选择一个掩码进行剪枝，然后只更新未遍历到的列的权重。这意味着对于某一列，如果我们没有选择剪除当前权重，它也将在未来的迭代中冻结（黑色方框左侧的浅蓝色方块）。这样一来，每次迭代更新权重时，只考虑未遍历到的输入特征，Hessian 子矩阵的逆在各行的同一迭代步骤中变得可以共享。</p><h3>权重重要性指标</h3><p data-pid="-99Vjo9R">以上论述了给定掩码如何做权重重建，至于如何得到掩码，即如何确定剪除哪些权重，文章使用非常简单的基于数值大小的剪枝。因为根据 OBS 算法，剪枝后的损失函数变化量为</p><p data-pid="yhuMaUew"><img src="https://www.zhihu.com/equation?tex=%5Cepsilon+_m%3D%5Cfrac%7Bw_m%5E2%7D%7B%5B%5Ctextbf%7BH%7D%5E%7B-1%7D%5D_%7Bmm%7D%7D+%5C%5C" alt="\epsilon _m=\frac{w_m^2}{[\textbf{H}^{-1}]_{mm}} \\" eeimg="1">参考附录证明过程。对于每一列，分母是固定的，所以直接选择 <img src="https://www.zhihu.com/equation?tex=p" alt="p" eeimg="1">% 最小的权重进行裁剪，从而达到整体上 <img src="https://www.zhihu.com/equation?tex=p" alt="p" eeimg="1">% 的稀疏度。但是有工作提到这样效果不好，因为相当于加入了每列都是均匀稀疏的很强的假设。作者于是对列下标进行分块，以128维作为最小粒度，每次在行数乘以128的块中进行剪枝，重要性就按照上式进行计算，对于不同的列既考虑权重大小，又考虑 Hessian 矩阵的对角线元素大小。</p><h3>对半结构化剪枝的支持</h3><p data-pid="l5yzOLU8">我们知道非结构化剪枝，即逐神经元剪枝会带来极不规则的稀疏矩阵，硬件对于这种稀疏矩阵乘法支持得并不好。而结构化剪枝，即每次剪除整个通道或卷积核又会导致粒度太粗，影响性能。半结构化剪枝就介于两者之间，它在 <a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2102.04010" class=" wrap external" target="_blank" rel="nofollow noreferrer">Learning N:M fine-grained structured sparse neural networks from scratch</a>、<a href="http://link.zhihu.com/?target=https%3A//proceedings.neurips.cc/paper_files/paper/2021/file/b0490b85e92b64dbb5db76bf8fca6a82-Paper.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">Accelerated sparse neural training: A provable and efficient method to find N:M transposable masks</a> 中有所讨论，规定了在每m个连续的权重中有固定的n个0，并在NVIDIA的GPU上有高效实现。</p><p data-pid="JfMJ7mX1">SparseGPT 可以对半结构化剪枝提供方便的支持。只需要将上述分块剪枝的粒度设定为m，并在每行中挑选n个重要性最低的权重进行剪枝即可。</p><p data-pid="AxarejYQ">文章后面还提到了和量化方法 GPTQ 的结合，请参考原文。</p><h2>实验结果</h2><p data-pid="_EEg6kOD">本文进行了多项实验。第一组实验研究了稀疏化难度随模型大小的变化情况。为此，文章考虑了 OPT 系列模型的不同大小版本，并统一剪枝所有线性层（不包括embedding和自注意力头），分别达到 50% 的非结构化稀疏度、4:8 或 2:4 的半结构化稀疏度。下表给出了在 raw-WikiText2 数据集上的实验结果：</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-b4b36b398a37f0c5420db677c6c401ad_720w.jpg?source=d16d100b" data-rawwidth="1080" data-rawheight="297" data-size="normal" data-caption="" class="origin_image zh-lightbox-thumb" width="1080" data-original="https://picx.zhimg.com/v2-b4b36b398a37f0c5420db677c6c401ad_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='297'&gt;&lt;/svg&gt;" data-rawwidth="1080" data-rawheight="297" data-size="normal" data-caption="" class="origin_image zh-lightbox-thumb lazy" width="1080" data-original="https://picx.zhimg.com/v2-b4b36b398a37f0c5420db677c6c401ad_720w.jpg?source=d16d100b" data-actualsrc="https://pic1.zhimg.com/v2-b4b36b398a37f0c5420db677c6c401ad_720w.jpg?source=d16d100b"></figure><p data-pid="H5ALmEe-">接下来是对目前公开可用最大规模模型 OPT-175B 和 BLOOM-176B 的研究结果。左图展示了 BLOOM-176B 的性能随 SparseGPT 、数值大小剪枝的稀疏度变化的结果。右图展示了 OPT 系列模型上剪枝+GPTQ量化的效果。</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-a862568aed00f32e4c533458902592b7_720w.jpg?source=d16d100b" data-rawwidth="1080" data-rawheight="443" data-size="normal" data-caption="" class="origin_image zh-lightbox-thumb" width="1080" data-original="https://pic1.zhimg.com/v2-a862568aed00f32e4c533458902592b7_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1080'%20height='443'&gt;&lt;/svg&gt;" data-rawwidth="1080" data-rawheight="443" data-size="normal" data-caption="" class="origin_image zh-lightbox-thumb lazy" width="1080" data-original="https://pic1.zhimg.com/v2-a862568aed00f32e4c533458902592b7_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-a862568aed00f32e4c533458902592b7_720w.jpg?source=d16d100b"></figure><p data-pid="Wz134U7y">下面提供了 OPT-175B 模型在几个不同的数据集下 zero-shot 剪枝的 perplexity 结果对比。</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-e15372f4e344f2d6e9806e5d43ad2daf_720w.jpg?source=d16d100b" data-rawwidth="1027" data-rawheight="286" data-size="normal" data-caption="" class="origin_image zh-lightbox-thumb" width="1027" data-original="https://picx.zhimg.com/v2-e15372f4e344f2d6e9806e5d43ad2daf_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1027'%20height='286'&gt;&lt;/svg&gt;" data-rawwidth="1027" data-rawheight="286" data-size="normal" data-caption="" class="origin_image zh-lightbox-thumb lazy" width="1027" data-original="https://picx.zhimg.com/v2-e15372f4e344f2d6e9806e5d43ad2daf_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-e15372f4e344f2d6e9806e5d43ad2daf_720w.jpg?source=d16d100b"></figure><h2>附录</h2><p data-pid="UrSqAtWC"><b>公式2证明过程</b></p><p data-pid="sGcEjX9_">参考 <a href="http://link.zhihu.com/?target=https%3A//proceedings.neurips.cc/paper_files/paper/2020/file/d1ff1ec86b62cd5f3903ff19c3a326b2-Supplemental.pdf%23page%3D14.09" class=" wrap external" target="_blank" rel="nofollow noreferrer">WoodFisher: Efficient Second-Order Approximation for Neural Network Compression</a> 文章附录 S1。我们将权重重建的目标定义为，在原始最优权重的基础上加上一个扰动量 <img src="https://www.zhihu.com/equation?tex=%5Cdelta+%5Cmathbf%7Bw%7D" alt="\delta \mathbf{w}" eeimg="1"> ，使得损失函数的数值变化最小。为了表示权重剪枝这一步，我们约束第 <img src="https://www.zhihu.com/equation?tex=m" alt="m" eeimg="1"> 个权重扰动量等于 <img src="https://www.zhihu.com/equation?tex=-w_m" alt="-w_m" eeimg="1"> ，这样加上扰动量后，第 <img src="https://www.zhihu.com/equation?tex=m" alt="m" eeimg="1"> 个权重就清零了。</p><p data-pid="Ca7SzkVR">我们对损失函数进行泰勒展开 ：<img src="https://www.zhihu.com/equation?tex=%5Cepsilon%28%5Cmathbf%7Bw%7D%2B%5Cdelta%5Cmathbf%7Bw%7D%29%3D%5Cepsilon%28%5Cmathbf%7Bw%7D%29%2B%5Cnabla_%7B%5Cmathbf%7Bw%7D%7D+%5Cepsilon%5E%5Ctop%5Cdelta+%5Cmathbf%7Bw%7D+%2B+%5Cfrac%7B1%7D%7B2%7D+%5Cdelta+%5Cmathbf%7Bw%7D%5E%5Ctop+%5Cnabla%5E2_%7B%5Cmathbf%7Bw%7D%7D+%5Cepsilon%5Cdelta+%5Cmathbf%7Bw%7D%2BO%28%5CVert+%5Cmathbf%7Bw%7D+%5CVert%5E3%29+%5C%5C" alt="\epsilon(\mathbf{w}+\delta\mathbf{w})=\epsilon(\mathbf{w})+\nabla_{\mathbf{w}} \epsilon^\top\delta \mathbf{w} + \frac{1}{2} \delta \mathbf{w}^\top \nabla^2_{\mathbf{w}} \epsilon\delta \mathbf{w}+O(\Vert \mathbf{w} \Vert^3) \\" eeimg="1"></p><p data-pid="C53UFn66">假设对网络进行剪枝时，权重处在临近局部最优点，因此损失函数的一阶梯度为0。我们抛弃3阶以上的余项，最小化以上损失函数等价于</p><p data-pid="TJGA9SKE"><img src="https://www.zhihu.com/equation?tex=%5Cmin_%7B%5Cdelta+%5Cmathbf%7Bw%7D%5Cin+%5Cmathbb+R%5Ed%7D+%5Cquad+%5Cdelta%5Cmathbf%7Bw%7D%5E%5Ctop+%5Cmathbf%7BH%7D%5Cdelta%5Cmathbf%7Bw%7D%2C+%5Cquad+%5Ctext%7Bs.t.%7D+%5Cquad+%5Cmathbf+e_m+%5E%5Ctop+%5Cdelta+%5Cmathbf%7Bw%7D+%2B+w_m%3D0%5C%5C" alt="\min_{\delta \mathbf{w}\in \mathbb R^d} \quad \delta\mathbf{w}^\top \mathbf{H}\delta\mathbf{w}, \quad \text{s.t.} \quad \mathbf e_m ^\top \delta \mathbf{w} + w_m=0\\" eeimg="1"></p><p data-pid="rezf2XRr">其中， <img src="https://www.zhihu.com/equation?tex=%5Cmathbf+%7Be%7D_q" alt="\mathbf {e}_q" eeimg="1"> 表示第 <img src="https://www.zhihu.com/equation?tex=q%5E%7Bth%7D" alt="q^{th}" eeimg="1"> 分量为1的 one-hot 向量。使用拉格朗日乘子法，转为无约束优化 <img src="https://www.zhihu.com/equation?tex=%5Cmin_%7B%5Cdelta+%5Cmathbf%7Bw%7D%5Cin+%5Cmathbb+R%5Ed%7D+%5Cquad+%5Cdelta+%5Cmathbf%7Bw%7D%5E%5Ctop%5Cmathbf%7BH%7D%5Cdelta+%5Cmathbf%7Bw%7D+%2B+%5Clambda+%28%5Cmathbf+e_m%5E%5Ctop+%5Cdelta+%5Ctextbf%7Bw%7D+%2Bw_m%29%5C%5C" alt="\min_{\delta \mathbf{w}\in \mathbb R^d} \quad \delta \mathbf{w}^\top\mathbf{H}\delta \mathbf{w} + \lambda (\mathbf e_m^\top \delta \textbf{w} +w_m)\\" eeimg="1"></p><p data-pid="T49OXYMZ">分别令上式对与 <img src="https://www.zhihu.com/equation?tex=%5Cdelta+%5Cmathbf%7Bw%7D" alt="\delta \mathbf{w}" eeimg="1"> 和 <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"> 的梯度为0，得到 <img src="https://www.zhihu.com/equation?tex=%5Cdelta+%5Cmathbf%7Bw%7D_m%5E%2A+%3D-+%5Cfrac%7Bw_m%7D%7B%5Cmathbf%7Be%7D_m%5E%5Ctop%5Ctextbf%7BH%7D%5E%7B-1%7D%5Cmathbf%7Be%7D_m%7D%5Ctextbf%7BH%7D%5E%7B-1%7D%5Cmathbf%7Be%7D_m%3D+-%5Cfrac%7Bw_m%7D%7B%5B%5Cmathbf%7BH%7D%5E%7B-1%7D%5D_%7Bmm%7D%7D%5B%5Ctextbf%7BH%7D%5E%7B-1%7D%5D_%7B%3A%2Cm%7D%5C%5C" alt="\delta \mathbf{w}_m^* =- \frac{w_m}{\mathbf{e}_m^\top\textbf{H}^{-1}\mathbf{e}_m}\textbf{H}^{-1}\mathbf{e}_m= -\frac{w_m}{[\mathbf{H}^{-1}]_{mm}}[\textbf{H}^{-1}]_{:,m}\\" eeimg="1"></p><p data-pid="PC6dGlRu">代入可以得到损失函数的变化量为</p><p data-pid="_OGj6gkE"><img src="https://www.zhihu.com/equation?tex=%5Cepsilon+_m%3D%5Cfrac%7Bw_m%5E2%7D%7B%5B%5Ctextbf%7BH%7D%5E%7B-1%7D%5D_%7Bmm%7D%7D%5C%5C" alt="\epsilon _m=\frac{w_m^2}{[\textbf{H}^{-1}]_{mm}}\\" eeimg="1">当我们想在此基础上再剪除一个权重时，只需要把对应的输入特征和权重从公式1中去掉，再重新使用以上方法。</p>