<p data-pid="S6lJ-nUo"><b>2023年7月30日更新：load模型时的注意事项</b></p><p data-pid="RHJxLQ7i">不能在实例化模型之后直接load_state_dict，需要让网络forward一次强制实例化Lazy模块。可以使用batch_size为1的随机向量。这一点比较麻烦，可能是应用Lazy的一个代价。</p><h3>现有方案</h3><p data-pid="Y0G7lbHv">这个问题我好奇很久了，我觉得这是Torch里最不方便的特性之一，在1.8版本之后Torch的官方逐渐引入延迟构造的方式：<a href="https://link.zhihu.com/?target=https%3A//pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">LazyLinear - PyTorch 1.12 documentation</a>。但在我使用的1.10版本中仍然有以下警告：带有Lazy特性的模块仍处在频繁的开发阶段。</p><div class="highlight"><pre><code class="language-text"><span></span>UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.
</code></pre></div><p data-pid="URo9RjkD">考虑到代码的兼容性和稳定性，我们可以自己封装这个Lazy模块。但正如题主所说，我们直接在forward中做延迟构造并不优雅，为什么不优雅呢？我们看来自官方论坛的这段代码：</p><div class="highlight"><pre><code class="language-python"><span></span><span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="kc">None</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="n">num_classes</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span> 
</code></pre></div><p data-pid="VWFbE-HN"><a href="https://link.zhihu.com/?target=https%3A//discuss.pytorch.org/u/ptrblck" class=" wrap external" target="_blank" rel="nofollow noreferrer">ptrblck</a>在<a href="https://link.zhihu.com/?target=https%3A//discuss.pytorch.org/t/how-to-automatically-get-in-features-from-nn-conv2d-to-nn-linear/27385/6" class=" wrap external" target="_blank" rel="nofollow noreferrer">他的回复</a>中提到：我们创建优化器的时候需要将待优化的参数集合注册进来，<b>那时很有可能忘记Lazy模块，因为这些模块在未推断时是没有参数的。</b></p><p data-pid="kljIar7s">除此之外，这样做还<b>很有可能因为运行时device的不统一而报错</b>。比如我们已经将Model创建出来，并拷贝到显存上，这时Linear的构造仍然默认在内存上，在数据流到达Linear处自然会出现RuntimeError。</p><p data-pid="OyZA6J6d"><a href="https://link.zhihu.com/?target=https%3A//discuss.pytorch.org/u/Kalle" class=" wrap external" target="_blank" rel="nofollow noreferrer">Kalle</a>提到可以将Linear和之前的结构拆解开，创建好前面的结构之后显式地计算一下前面层次的输出维度，然后再创建Linear层，参考<a href="https://link.zhihu.com/?target=https%3A//github.com/yjlolo/vae-audio/blob/3a43e3122da55daf89fffaf5bbf11fdb805a597f/model/model.py%23L205" class=" wrap external" target="_blank" rel="nofollow noreferrer">代码</a>。这样做没有功能性的问题，但是我们<b>把原本的推断路径在创建时割裂开了</b>，比起一个直接的Sequential层<b>这样显得不太清晰</b>。在更复杂的网络，比如非直线型推断的ResNet中，代码会显得很乱。</p><p data-pid="ulwWzrsH"><a href="https://link.zhihu.com/?target=https%3A//discuss.pytorch.org/u/ptrblck" class=" wrap external" target="_blank" rel="nofollow noreferrer">ptrblck</a>主张编写代码时print一下维度，然后再修改代码，将维度填写进去。但这就回到了我们的问题，这样做比keras麻烦的多，而且<b>如果测试期频繁修改代码结构，将增加很多工作量</b>。虽然但是，这是我平时最常用的方法。</p><h3>解决方案</h3><p data-pid="Y7HRdmep">我们可以利用好Python的函数式编程特性，使用其内置的科里化模块functools.partial来实现延迟构造；同时将forward中构造模块的代码执行前移，避免在设备间移动或者正式推断时没有参数。</p><p data-pid="MyyX6EqQ">后面这一点其实是关键，你完全可以将上文的代码中的forward函数封装成一个工具函数，然后在构造函数和forward中分别调用它。但是题主既然提到了优雅，这里就介绍一下如何用partial来提取统一的逻辑。</p><p data-pid="F6zrU7VX">partial实际上是一个class，它的作用就是将函数（或类的构造）指定一部分参数，变成一个还未准备好真正执行的新的函数。举个例子，对于具有a、b两个形式参数的函数f，我们打算传入实参1和2。我们常见的做法是调用f(a=1, b=2)。但有的时候a和b在当前作用域下不是全部可知的，那么我们可以先得到g=partial(f, a=1)，然后再去调用g(b=2)，效果完全一样。</p><p data-pid="vK9uFpp8">我们先定义一个Lazy模块，为了让它能像正常的模块一样使用，我们继承nn.Module。</p><div class="highlight"><pre><code class="language-python"><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span>

<span class="k">class</span> <span class="nc">Lazy</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">later</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Any</span><span class="p">],</span> <span class="n">Dict</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Lazy</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">later</span> <span class="o">=</span> <span class="n">later</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">,</span> <span class="n">partial</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">later</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div><p data-pid="seYu43y7">构造函数的参数中的layer是我们指定的某个模块类，比如nn.Linear。later是一个函数，这个函数的输入相当于forward中的x，输出是一个字典。later定义了我们需要动态指定的参数字典，以及根据输入x指定它的方法。选用字典这个结构作为返回值，是因为位置参数和关键字参数都可以按照关键字参数的方式传参。</p><p data-pid="0crkcAYf">构造函数中先用partial构造一个传入了部分参数的函数layer，然后在第一次执行forward时根据later的推断，将新得到的参数通过**解包后传入layer中进行动态构造。以下给出一个使用示例：</p><div class="highlight"><pre><code class="language-python"><span></span><span class="kn">from</span> <span class="nn">torchinfo</span> <span class="kn">import</span> <span class="n">summary</span>


<span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Model</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">input_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
            <span class="n">Lazy</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">later</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">{</span>
                <span class="s1">'in_features'</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="p">}),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">static_check</span><span class="p">(</span><span class="n">input_size</span><span class="p">)</span>  <span class="c1"># !important</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_weights</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">static_check</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">input_size</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">_initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">'''User specified weights initialization'''</span>
        <span class="o">...</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">'__main__'</span><span class="p">:</span>
    <span class="n">input_size</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">]</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">input_size</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">summary</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="o">*</span><span class="n">input_size</span><span class="p">])</span>
</code></pre></div><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/50/v2-342cd842e787ae1d62fb3ae2b26d6e0a_720w.jpg?source=c8b7c179" data-caption="" data-size="normal" data-rawwidth="730" data-rawheight="390" data-original-token="v2-a1d9ae31284c4370f5b2a2f10a8a44b9" data-default-watermark-src="https://pic1.zhimg.com/50/v2-df635c16357608d3b3af57e669df5127_720w.jpg?source=c8b7c179" class="origin_image zh-lightbox-thumb" width="730" data-original="https://pica.zhimg.com/v2-342cd842e787ae1d62fb3ae2b26d6e0a_r.jpg?source=c8b7c179"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='730'%20height='390'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="730" data-rawheight="390" data-original-token="v2-a1d9ae31284c4370f5b2a2f10a8a44b9" data-default-watermark-src="https://pic1.zhimg.com/50/v2-df635c16357608d3b3af57e669df5127_720w.jpg?source=c8b7c179" class="origin_image zh-lightbox-thumb lazy" width="730" data-original="https://pica.zhimg.com/v2-342cd842e787ae1d62fb3ae2b26d6e0a_r.jpg?source=c8b7c179" data-actualsrc="https://picx.zhimg.com/50/v2-342cd842e787ae1d62fb3ae2b26d6e0a_720w.jpg?source=c8b7c179"></figure><p data-pid="PU-l8FuI">重点是下面这句话，可以看到，我们将Lazy模块很好的融入到原本的结构定义逻辑中，而且可以很清晰地看出我们缺省的参数和动态指定的方法。</p><div class="highlight"><pre><code class="language-python"><span></span><span class="n">Lazy</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">later</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">{</span>
    <span class="s1">'in_features'</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="p">}),</span>
</code></pre></div><p data-pid="DEVIR6g1">注意，构造函数中，一定要在网络各层定义完毕之后，参数初始化等操作之前调用一次静态检查。另外，除了上面这种万能的写法，在它基础上可以将常用的LazyLinear、LazyBatchNorm等模块再封装一下。</p>