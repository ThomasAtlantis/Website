<p data-pid="5dXKeg9d"><a href="http://link.zhihu.com/?target=https%3A//aaai.org/ojs/index.php/AAAI/article/view/6409/6265" class=" wrap external" target="_blank" rel="nofollow noreferrer">Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT</a>，AAAI-20</p><p data-pid="luiZ9sO5">作者是来自 UC Berkeley 的 Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W. Mahoney, Kurt Keutzer</p><h2>文章摘要</h2><p data-pid="6mbOYcfV">基于 BERT 的模型<u>内存开销大</u>、<u>计算延迟高</u>，难以在资源受限环境中部署。本文广泛地分析了微调后的 BERT 模型的二阶 Hessian 信息，并据此提出一种超低精度的量化方法：一种分组量化（group-wise quantization）算法，并使用基于 Hessian 的混合精度算法进一步压缩模型。</p><p data-pid="6V-R6NgB">本文在 SST-2、MNLI、CoNLL-03 和 SQuAD 等 BERT 下游任务上对提出的方法进行了广泛评估。即使用低至 2 位的超低精度量化，也能保证不高于 2.3% 的性能损失。这相当于模型参数压缩高达 13 倍，嵌入表和激活压缩高达 4 倍。在所有任务中，我们观察到在 SQuAD 上微调的 BERT 的性能损失最高。通过分析 Hessian 信息和可视化，我们发现这与 BERT 的当前训练/微调策略在 SQuAD 上不收敛有关。</p><h2>算法设计</h2><p data-pid="oFzmNSQ3">微调后的 BERT-base 模型由三部分组成：嵌入层（embedding）、Transformer 编码器层和输出层。设 <img src="https://www.zhihu.com/equation?tex=+x%5Cin+X+" alt=" x\in X " eeimg="1"> 为输入语句， <img src="https://www.zhihu.com/equation?tex=+y+%5Cin+Y+" alt=" y \in Y " eeimg="1"> 为对应的标签，损失函数定义为：</p><p data-pid="z1gHHqKq"><img src="https://www.zhihu.com/equation?tex=++L%28%5Ctheta%29%3D%5Csum_%7B%28x_i%2Cy_i%29%7D%5Cmathrm%7BCE%7D%28%5Cmathrm%7Bsoftmax%7D%28W_c%28W_n%28...W_1%28W_e%28x_i%29%29%29%29%29%2Cy_i%29%5C%5C++" alt="  L(\theta)=\sum_{(x_i,y_i)}\mathrm{CE}(\mathrm{softmax}(W_c(W_n(...W_1(W_e(x_i))))),y_i)\\  " eeimg="1"> </p><p data-pid="iKM7Y3Jl">其中 <img src="https://www.zhihu.com/equation?tex=+CE+" alt=" CE " eeimg="1"> 是交叉熵函数（或者其他合适的损失函数）， <img src="https://www.zhihu.com/equation?tex=+%5Ctheta+" alt=" \theta " eeimg="1"> 是嵌入表 <img src="https://www.zhihu.com/equation?tex=+W_e+" alt=" W_e " eeimg="1"> 、编码器层 <img src="https://www.zhihu.com/equation?tex=+W_1%2CW_2%2C...%2CW_n+" alt=" W_1,W_2,...,W_n " eeimg="1"> 和输出分类层 <img src="https://www.zhihu.com/equation?tex=+W_c+" alt=" W_c " eeimg="1"> 的一个组合。</p><p data-pid="ZIxVkd1u">BERT-base 模型的参数大小为：嵌入层 91MB、编码器层 325MB、输出层 0.01MB。由于输出层的大小可以忽略不计，因此我们不对其进行量化，而是专注于嵌入层和编码器层的量化。</p><h3>量化过程</h3><p data-pid="BuKJROmx">一般神经网络的推理都是在浮点精度的权重和激活上进行的，而量化将网络的权重值限制在一个有限的集合 <img src="https://www.zhihu.com/equation?tex=+Q%28z%29%3Dq_j+" alt=" Q(z)=q_j " eeimg="1"> ，其中 <img src="https://www.zhihu.com/equation?tex=+t_%7Bj%7D+%5Cleq+z+%5Cleq+t_%7Bj%2B1%7D+" alt=" t_{j} \leq z \leq t_{j+1} " eeimg="1"> 。这里 <img src="https://www.zhihu.com/equation?tex=+Q+" alt=" Q " eeimg="1"> 是量化算子， <img src="https://www.zhihu.com/equation?tex=+z+" alt=" z " eeimg="1"> 是一个实值输入（权重或激活）。此外，令 <img src="https://www.zhihu.com/equation?tex=+k+" alt=" k " eeimg="1"> 表示量化的精度。对于量化算子 <img src="https://www.zhihu.com/equation?tex=+Q+" alt=" Q " eeimg="1"> 有诸多选择，我们使用均匀量化函数，即一个张量内的浮点数值范围被等分，并表示为无符号整数 <img src="https://www.zhihu.com/equation?tex=+%5Cbegin%7BBmatrix%7D0%2C%5Cldots%2C2%5Ek-1%5Cend%7BBmatrix%7D+" alt=" \begin{Bmatrix}0,\ldots,2^k-1\end{Bmatrix} " eeimg="1"> 。注意，非均匀的量化算子可能可以提高准确率，但本文只关注均匀量化，因为它更高效，也更容易用硬件实现。</p><p data-pid="VqPDPzpf">正向传播中，权重或激活的张量 <img src="https://www.zhihu.com/equation?tex=+X+" alt=" X " eeimg="1"> 将被如下量化：</p><p data-pid="mqu9HCg0"><img src="https://www.zhihu.com/equation?tex=++X%5E%7B%5Cprime%7D%3D%5Cmathrm%7BClamp%7D%28X%2Cq_0%2Cq_%7B2%5Ek-1%7D%29%2C%5C%5CX%5EI%3D%5Clfloor%5Cfrac%7BX%5E%7B%5Cprime%7D-q_0%7D%5CDelta%5Crceil%2C%5Ctext%7Bwhere+%7D%5CDelta%3D%5Cfrac%7Bq_%7B2%5Ek-1%7D-q_0%7D%7B2%5Ek-1%7D%2C%5C%5CQ%28X%29%3D%5CDelta+X%5EI%2Bq_0++%5C%5C" alt="  X^{\prime}=\mathrm{Clamp}(X,q_0,q_{2^k-1}),\\X^I=\lfloor\frac{X^{\prime}-q_0}\Delta\rceil,\text{where }\Delta=\frac{q_{2^k-1}-q_0}{2^k-1},\\Q(X)=\Delta X^I+q_0  \\" eeimg="1">其中 <img src="https://www.zhihu.com/equation?tex=+%5Cleft%5Clfloor%5Ccdot%5Cright%5Crceil+" alt=" \left\lfloor\cdot\right\rceil " eeimg="1"> 表示取整算子， <img src="https://www.zhihu.com/equation?tex=+%5CDelta+" alt=" \Delta " eeimg="1"> 表示量化的相邻数值的间距， <img src="https://www.zhihu.com/equation?tex=+X%5EI+" alt=" X^I " eeimg="1"> 是整数索引的集合。 <img src="https://www.zhihu.com/equation?tex=+%5Bq_0%2Cq_%7B2%5Ek-1%7D%5D+" alt=" [q_0,q_{2^k-1}] " eeimg="1"> 表示浮点数的量化数值范围，Clamp 函数将所有小于  <img src="https://www.zhihu.com/equation?tex=+q_0+" alt=" q_0 " eeimg="1"> 的元素设置为 <img src="https://www.zhihu.com/equation?tex=+q_0+" alt=" q_0 " eeimg="1"> ，大于 <img src="https://www.zhihu.com/equation?tex=+q_%7B2%5Ek-1%7D+" alt=" q_{2^k-1} " eeimg="1"> 的元素设置为 <img src="https://www.zhihu.com/equation?tex=+q_%7B2%5Ek-1%7D+" alt=" q_{2^k-1} " eeimg="1"> 。注意这个范围可能是最小值到最大值范围的一个子区间，被截断掉的那部分就被称作 outliers（异常值/离群值）。量化之后，只需在 <img src="https://www.zhihu.com/equation?tex=+X%5EI+" alt=" X^I " eeimg="1"> 的基础上计算整数的矩阵乘法，然后将结果使用 gather 的方式反量化。为了通过 <img src="https://www.zhihu.com/equation?tex=+Q+" alt=" Q " eeimg="1"> 反向传播梯度，我们使用 Bengio 的 STE（Straight-through Estimator）算法<sup data-text="Estimating or propagating gradients through stochastic neurons for conditional computation" data-url="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=62c76ca0b2790c34e85ba1cce09d47be317c7235" data-draft-node="inline" data-draft-type="reference" data-numero="1">[1]</sup>。</p><h3>混合精度量化</h3><p data-pid="twQTLqye">不同的编码器层对量化的敏感程度不同，使用统一的精度量化会导致次优，这在极低位宽量化时更加严重，所以我们选用混合精度量化来保持性能。</p><p data-pid="yx7_CB2B">HAWQ（Hessian AWare Quantization）的主要思想是：Hessian 矩阵的谱（即特征值集合）更大意味着损失函数的曲面更加不平滑，即该网络层对于量化更加敏感，也就需要更高的位宽。虽然每个编码层有 7M 大小的参数，Hessian 矩阵将是  <img src="https://www.zhihu.com/equation?tex=+7M%5Ctimes7M+" alt=" 7M\times7M " eeimg="1">  大小，计算开销极大，但是，Hessian 矩阵的谱可以使用 matrix-free power iteration 方法计算，它不需要对算子进行显式的建模。</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-f5460706d880c5be6f5ef1469bf4b9ba_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="1818" data-rawheight="491" class="origin_image zh-lightbox-thumb" width="1818" data-original="https://picx.zhimg.com/v2-f5460706d880c5be6f5ef1469bf4b9ba_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1818'%20height='491'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1818" data-rawheight="491" class="origin_image zh-lightbox-thumb lazy" width="1818" data-original="https://picx.zhimg.com/v2-f5460706d880c5be6f5ef1469bf4b9ba_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-f5460706d880c5be6f5ef1469bf4b9ba_720w.jpg?source=d16d100b"></figure><p data-pid="PgA-fYft">我们以第一层编码器层为例进行说明。设该层的梯度为 <img src="https://www.zhihu.com/equation?tex=+g_1+" alt=" g_1 " eeimg="1"> ，对于一个与 <img src="https://www.zhihu.com/equation?tex=+g_1+" alt=" g_1 " eeimg="1"> 无关的随机向量 <img src="https://www.zhihu.com/equation?tex=+v+" alt=" v " eeimg="1"> ，我们有：</p><p data-pid="ItZrcUby"><img src="https://www.zhihu.com/equation?tex=++%5Cbegin%7Baligned%7D%5Cfrac%7B%5Cpartial+g_1%5ETv%7D%7B%5Cpartial+W_1%7D%3D%5Cfrac%7B%5Cpartial+g_1%5ET%7D%7B%5Cpartial+W_1%7Dv%2Bg_1%5ET%5Cfrac%7B%5Cpartial+v%7D%7B%5Cpartial+W_1%7D%3D%5Cfrac%7B%5Cpartial+g_1%5ET%7D%7B%5Cpartial+W_1%7Dv%3DH_1v%5Cend%7Baligned%7D%5C%5C" alt="  \begin{aligned}\frac{\partial g_1^Tv}{\partial W_1}=\frac{\partial g_1^T}{\partial W_1}v+g_1^T\frac{\partial v}{\partial W_1}=\frac{\partial g_1^T}{\partial W_1}v=H_1v\end{aligned}\\" eeimg="1">其中 <img src="https://www.zhihu.com/equation?tex=+H_1+" alt=" H_1 " eeimg="1"> 是该层的 Hessian 阵，其最大的特征值可以通过 power iteration 计算。令 <img src="https://www.zhihu.com/equation?tex=+%5Clambda_i+" alt=" \lambda_i " eeimg="1"> 表示第 <img src="https://www.zhihu.com/equation?tex=+i+" alt=" i " eeimg="1"> 个编码器层的最大的特征值。下图展示了 BERT-base 模型不同的编码器层的 Hessian 矩阵的最大特征值分布情况。</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-dc854d0a8c7e2d67529ef2c032f58162_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="1681" data-rawheight="489" class="origin_image zh-lightbox-thumb" width="1681" data-original="https://pic1.zhimg.com/v2-dc854d0a8c7e2d67529ef2c032f58162_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1681'%20height='489'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1681" data-rawheight="489" class="origin_image zh-lightbox-thumb lazy" width="1681" data-original="https://pic1.zhimg.com/v2-dc854d0a8c7e2d67529ef2c032f58162_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-dc854d0a8c7e2d67529ef2c032f58162_720w.jpg?source=d16d100b"></figure><p data-pid="griAxTU6">尽管各层有相同的大小和结构，但特征值的大小展现出不同的分布情况。HAWQ 主张在不同的训练数据上对最大特征值做平均，然后作为量化敏感性的指标。然而我们发现，有些层的最大特征值的数值分布方差很大。例如在 SQuAD 数据集上的第 7 层的方差大于 61.6，而均值却在 1.0 附近，尽管每个数据点使用的是整个数据集的 10%（9K 样本）。为了应对这个问题，我们将指标修改为如下：</p><p data-pid="L0uniRQQ"><img src="https://www.zhihu.com/equation?tex=++%5COmega_i%5Ctriangleq%7C%5Cmathrm%7Bmean%7D%28%5Clambda_i%29%7C%2B%5Cmathrm%7Bstd%7D%28%5Clambda_i%29%5C%5C" alt="  \Omega_i\triangleq|\mathrm{mean}(\lambda_i)|+\mathrm{std}(\lambda_i)\\" eeimg="1">计算完 <img src="https://www.zhihu.com/equation?tex=+%5COmega_i+" alt=" \Omega_i " eeimg="1"> ，我们将其按照降序排序，然后据此确定各层的量化精度。接下来在这些精度设定下进行量化感知的微调（quantization-aware fine-tuning）。</p><p data-pid="XYx_tYdI">我们的方法假设量化之前，训练好的模型收敛在一个局部最优点，即在该点处的梯度为 0，并且具有正曲率（正的 Hessian 特征值）。在我们的分析中，我们观察到 MNLI、CoNLL-03 和 SST-2 的最大特征值确实是正的，然而在 SQuAD 数据集上微调的模型并没有收敛到局部最优点，其 Hessian 特征值中有很大的负值，如图 1-(d) 所示。</p><h3>分组量化</h3><p data-pid="VoQ3UsWm">假设输入序列有 <img src="https://www.zhihu.com/equation?tex=+n+" alt=" n " eeimg="1"> 个单词，每个单词有一个 <img src="https://www.zhihu.com/equation?tex=+d+" alt=" d " eeimg="1"> 维嵌入向量（对于 BERT-base， <img src="https://www.zhihu.com/equation?tex=+d%3D768+" alt=" d=768 " eeimg="1"> ），即 <img src="https://www.zhihu.com/equation?tex=+x+%3D+%28x%281%29%2C%5Cldots%2Cx%28n%29%29%5ET+%5Cin+%5Cmathbb%7BR%7D%5E%7Bn%5Ctimes+d%7D+" alt=" x = (x(1),\ldots,x(n))^T \in \mathbb{R}^{n\times d} " eeimg="1"> 。在 Transformer 编码器中，每个自注意力头有 4 个稠密矩阵，即 <img src="https://www.zhihu.com/equation?tex=+W_k%2CW_q%2CW_v%2CW_o+%5Cin+%5Cmathbb%7BR%7D%5E%7B%5Cfrac%7Bd%7D%7BN_h%7D%5Ctimes+d%7D+" alt=" W_k,W_q,W_v,W_o \in \mathbb{R}^{\frac{d}{N_h}\times d} " eeimg="1"> ，分别代表 key、query、value 和输出权重矩阵，其中 <img src="https://www.zhihu.com/equation?tex=+N_h+" alt=" N_h " eeimg="1"> 是注意力头的数量。每个自注意力头计算一个加权和</p><p data-pid="2Nxg-p8v"><img src="https://www.zhihu.com/equation?tex=++%5Cmathrm%7BAtt%7D%28x%2Cx%28j%29%29%3DW_o%5Csum_%7Bi%3D1%7D%5En%5Cmathrm%7Bsoftmax%7D%5Cleft%28%5Cfrac%7Bx%28j%29%5ETW_q%5ETW_kx%28i%29%7D%7B%5Csqrt%7Bd%7D%7D%5Cright%29W_vx%28i%29%5C%5C" alt="  \mathrm{Att}(x,x(j))=W_o\sum_{i=1}^n\mathrm{softmax}\left(\frac{x(j)^TW_q^TW_kx(i)}{\sqrt{d}}\right)W_vx(i)\\" eeimg="1">多头自注意力（MHSA）将对这些结果进行加和</p><p data-pid="xeGy3vlZ"><img src="https://www.zhihu.com/equation?tex=++%5Csum_%7Bi%3D1%7D%5E%7BN_h%7D%5Ctext%7BAtt%7D_i%28x%2Cx%28j%29%29%5C%5C" alt="  \sum_{i=1}^{N_h}\text{Att}_i(x,x(j))\\" eeimg="1">对这 4 个矩阵使用统一的量化数值范围将会极大地损害准确率。CNN 量化中常用逐通道的量化来缓解这个问题，其将每个卷积核看做是单独的输出通道并使用独立的量化数值范围。然而这没法直接使用，因为每个稠密矩阵本身只是一个核。因此，我们针对基于注意力的模型提出分组量化算法。</p><p data-pid="4AMszcVA">我们将对应每一个注意力头的矩阵 <img src="https://www.zhihu.com/equation?tex=+W+" alt=" W " eeimg="1"> 归为一大组，在每个大组内对输出神经元分桶归作一些小组。例如 BERT-base 模型有 12 个注意力头，分作 12 大组，隐层维度为 768，则每个头的输出维度为 768/12=64，每 6 个神经元为一个小组，那么一共就有 12*64/6=128 小组。对于每一个小组，使用不同的量化数值范围。</p><h2>实验结果</h2><p data-pid="IOUFROo0">总体结果如下（每层 128 组，除了 baseline，激活都使用的是 8 bit）。其中 M/N MP 表示 M bit 和 N bit 混合精度量化。w-bits 和 e-bits 分别表示权重和 embedding 的量化位宽。Size 和 Size-w/o-e 分别表示模型大小和除去 embedding 层的模型大小，单位都是 MB。DirectQ 表示不使用混合精度量化也不使用分组量化。</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-ff0122871cf0655cd9beed6f45ce7e56_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="960" data-rawheight="738" class="origin_image zh-lightbox-thumb" width="960" data-original="https://picx.zhimg.com/v2-ff0122871cf0655cd9beed6f45ce7e56_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='960'%20height='738'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="960" data-rawheight="738" class="origin_image zh-lightbox-thumb lazy" width="960" data-original="https://picx.zhimg.com/v2-ff0122871cf0655cd9beed6f45ce7e56_720w.jpg?source=d16d100b" data-actualsrc="https://pic1.zhimg.com/v2-ff0122871cf0655cd9beed6f45ce7e56_720w.jpg?source=d16d100b"></figure><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-e46e80b47b44034d9337b655180b5cd5_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="997" data-rawheight="771" class="origin_image zh-lightbox-thumb" width="997" data-original="https://pic1.zhimg.com/v2-e46e80b47b44034d9337b655180b5cd5_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='997'%20height='771'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="997" data-rawheight="771" class="origin_image zh-lightbox-thumb lazy" width="997" data-original="https://pic1.zhimg.com/v2-e46e80b47b44034d9337b655180b5cd5_720w.jpg?source=d16d100b" data-actualsrc="https://pic1.zhimg.com/v2-e46e80b47b44034d9337b655180b5cd5_720w.jpg?source=d16d100b"></figure><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-64a62dd5f04ce35e387f787f86911fc1_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="943" data-rawheight="729" class="origin_image zh-lightbox-thumb" width="943" data-original="https://picx.zhimg.com/v2-64a62dd5f04ce35e387f787f86911fc1_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='943'%20height='729'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="943" data-rawheight="729" class="origin_image zh-lightbox-thumb lazy" width="943" data-original="https://picx.zhimg.com/v2-64a62dd5f04ce35e387f787f86911fc1_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-64a62dd5f04ce35e387f787f86911fc1_720w.jpg?source=d16d100b"></figure><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-f52a226c8afb4a86abc22c4bf7b87e4f_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="999" data-rawheight="717" class="origin_image zh-lightbox-thumb" width="999" data-original="https://picx.zhimg.com/v2-f52a226c8afb4a86abc22c4bf7b87e4f_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='999'%20height='717'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="999" data-rawheight="717" class="origin_image zh-lightbox-thumb lazy" width="999" data-original="https://picx.zhimg.com/v2-f52a226c8afb4a86abc22c4bf7b87e4f_720w.jpg?source=d16d100b" data-actualsrc="https://pic1.zhimg.com/v2-f52a226c8afb4a86abc22c4bf7b87e4f_720w.jpg?source=d16d100b"></figure><p data-pid="kbzmMwhU">整体上，嵌入层比编码器层的权重对量化更敏感。另外，我们发现位置编码对于量化非常敏感。ew-bits 和 ep-bits 分别表示 word embedding 和 position embedding 的量化位宽。</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-1e5f2889b1264f5d11df575ea325c0fb_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="1004" data-rawheight="384" class="origin_image zh-lightbox-thumb" width="1004" data-original="https://pica.zhimg.com/v2-1e5f2889b1264f5d11df575ea325c0fb_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1004'%20height='384'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1004" data-rawheight="384" class="origin_image zh-lightbox-thumb lazy" width="1004" data-original="https://pica.zhimg.com/v2-1e5f2889b1264f5d11df575ea325c0fb_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-1e5f2889b1264f5d11df575ea325c0fb_720w.jpg?source=d16d100b"></figure><p data-pid="fly8ms0r">FFN 层比自注意力层对量化更敏感。s-bits 和 f-bits 分别表示自注意力层和 FFN 层的量化位宽。</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-48208c37469a24aa7e6489ebd7e72eba_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="1014" data-rawheight="391" class="origin_image zh-lightbox-thumb" width="1014" data-original="https://picx.zhimg.com/v2-48208c37469a24aa7e6489ebd7e72eba_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1014'%20height='391'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1014" data-rawheight="391" class="origin_image zh-lightbox-thumb lazy" width="1014" data-original="https://picx.zhimg.com/v2-48208c37469a24aa7e6489ebd7e72eba_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-48208c37469a24aa7e6489ebd7e72eba_720w.jpg?source=d16d100b"></figure><p></p>