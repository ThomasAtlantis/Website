<p data-pid="388E3Dys">先说结论。分割学习的切分点选择主要考虑两方面因素，<b>一是分割后的端云计算量比例，二是分割点的隐层特征维度</b>。</p><p data-pid="WBCgbZXz">前者会影响端侧的训练和推断时延以及隐层特征的隐私保护能力，优化时主要考虑端侧设备的处理速度、内存大小和能耗预算，我们可以通过一些离线和在线的测量进行建模。后者主要影响的是通信开销，优化时主要考虑端云网络链路的带宽，可以通过每轮通信的时间动态估计。</p><p data-pid="_R4CF7H2">下面展开。其实模型分割这件事最早考虑的还不是训练阶段，而是推断时的计算卸载。而计算卸载再往前追溯就是个很古老的话题了。智能手机刚开始普及的时候，计算能力“有，但并不多”，我们就考虑将一部分代码转移到算力更强的设备上运行。</p><p data-pid="yJ-u3E4C">从远程过程调用 (RPC) 协议到现在爆火的Serverless<sup data-text="Serverless (01) — RPC to Serverless. How did we get here?" data-url="https://medium.com/@manvendrapsingh/rpc-to-serverless-how-did-we-get-here-8c722441fe10" data-draft-node="inline" data-draft-type="reference" data-numero="1">[1]</sup>都可以看作一种静态的计算卸载，它依赖程序员提前设计好被卸载的函数边界，那么<b>是否可以根据资源情况动态决定哪些代码在哪个设备上运行呢？</b>研究者以此为目标搭建了很多对程序员更加友好的傻瓜式代码卸载框架，比如10年的MAUI<sup data-text="MAUI: Making Smartphones Last Longer with Code Offload" data-url="https://www.cse.psu.edu/~gxc27/teach/597-17/maui.pdf" data-draft-node="inline" data-draft-type="reference" data-numero="2">[2]</sup>和11年的CloneCloud<sup data-text="CloneCloud: Elastic Execution between Mobile Device and Cloud" data-url="http://www.cs.columbia.edu/~lierranli/coms6998-11Fall2012/papers/clonecloud_eurosys2011.pdf" data-draft-node="inline" data-draft-type="reference" data-numero="3">[3]</sup>。</p><p data-pid="Fw1I6J_b">以后者为例，它在云侧服务器运行程序级虚拟机，在端侧需要计算卸载的时候，对代码进行线程粒度的克隆，并将执行权转交给云端。这个时期的计算卸载很底层，不仅需要精密的profiler来确定卸载时机，还要依赖程序语言的静态分析来确定切分位置以及需要传递的线程上下文。</p><p data-pid="FG0RgS3Q">随着深度学习浪潮的兴起，终于在17年，Neurosurgeon<sup data-text="Neurosurgeon: Collaborative intelligence between the cloud and mobile edge" data-url="https://dl.acm.org/doi/pdf/10.1145/3093337.3037698" data-draft-node="inline" data-draft-type="reference" data-numero="4">[4]</sup>首次提出对神经网络的计算卸载。我们知道，学术界对移动端神经网络推断加速的研究大致在2015年兴起，到2020年左右饱和，所以这时的计算卸载也主要考虑的是推断时期优化，即<b>如何根据资源情况动态分割模型，将一部分网络层移交给云侧推断？</b>这时的计算卸载已经高于程序层，我们不再考虑程序级上下文的同步，而是简单地沿着网络深度割一刀，即layer-wise partition。</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/50/v2-89285dca6159f3002bb3511bbc905374_720w.jpg?source=c8b7c179" data-size="normal" data-rawwidth="734" data-rawheight="254" data-original-token="v2-f35825d9f374fc93b850970055535778" data-default-watermark-src="https://picx.zhimg.com/50/v2-eb9de663b690430a18a1d54c281ab2dc_720w.jpg?source=c8b7c179" class="origin_image zh-lightbox-thumb" width="734" data-original="https://picx.zhimg.com/v2-89285dca6159f3002bb3511bbc905374_r.jpg?source=c8b7c179"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='734'%20height='254'&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="734" data-rawheight="254" data-original-token="v2-f35825d9f374fc93b850970055535778" data-default-watermark-src="https://picx.zhimg.com/50/v2-eb9de663b690430a18a1d54c281ab2dc_720w.jpg?source=c8b7c179" class="origin_image zh-lightbox-thumb lazy" width="734" data-original="https://picx.zhimg.com/v2-89285dca6159f3002bb3511bbc905374_r.jpg?source=c8b7c179" data-actualsrc="https://picx.zhimg.com/50/v2-89285dca6159f3002bb3511bbc905374_720w.jpg?source=c8b7c179"><figcaption>AlexNet各层的累积推理时延与切分点通信开销</figcaption></figure><p data-pid="6cHIB590">文章发现直线型 (chain-topology) 推断的CNN随着网络的深入，累积计算量逐渐增大，但切分位置的隐层向量维度却逐渐减小，所以必然有一个最优的切分点可以使端侧推理时延和通信时延达到最好的trade-off。这是一个合理的观察，因为卷积神经网络一般先在浅层使用较小的卷积核和较大的通道数量对低维特征进行“扩张”，然后随着网络的深入逐渐使用池化层进行分辨率的下采样。大家可以自己实现一个VGG19然后观察一下各层输出大小大致如此。</p><p data-pid="D7Xppkwr">之后的工作就越玩儿越花，开始分割各种网络，带残差连接的、多分支的，逐渐将神经网络看成是DAG图来进行分割和优化<sup data-text="Dynamic Adaptive DNN Surgery for Inference Acceleration on the Edge" data-url="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8737614" data-draft-node="inline" data-draft-type="reference" data-numero="5">[5]</sup>，这类文章可谓是锦上添花，但都局限在端云协同推断领域。同时期，由于端侧设备算力逐渐增强，以联邦学习 (FL)、分割学习 (SL) 为代表的端云协同训练开启了新赛道，而这两者最初的提法都涉及到了隐私保护。</p><p data-pid="EFEBcIFI">学术界似乎有一个潜在的共识，认为分割学习与联邦学习相比有两点优势：一是端云都没有形成对完整模型的完全控制所以更安全，二是可以实现端云模型异构从而更利于低资源设备训练。第一点其实比较扯淡，目前想要保证端云协同中的隐私安全都需要额外加点儿什么，比如差分隐私和同态加密。前者在网络层引入随机噪声来对抗差分攻击，但是损害模型精度；后者依靠密码学，让数据在加密状态下依然可以处理，但需要极大的计算开销。</p><p data-pid="RBY4dUPs">不加buff，分割学习易受模型反转攻击<sup data-text="Model Inversion Attacks Against Collaborative Inference" data-url="https://par.nsf.gov/servlets/purl/10208164" data-draft-node="inline" data-draft-type="reference" data-numero="6">[6]</sup>，即容易通过隐层特征恢复出原始数据。之所以在这里提，是因为<b>分割学习承受攻击的鲁棒性受分割点影响</b>。学术界喜欢称切分点为cut layer，输出的特征向量为smashed data，把分割点前的端侧模型叫head，把云侧部分叫tail。在黑盒攻击模型中，<b>当head比较深、由全连接层构成、smashed data的维度小于输入数据维度时，鲁棒性更好。</b></p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/50/v2-3936ab66fc139f0abbfc6fdbf3736312_720w.jpg?source=c8b7c179" data-size="normal" data-rawwidth="584" data-rawheight="321" data-original-token="v2-8e903506e51512ac9f7d27cf7ab58111" data-default-watermark-src="https://picx.zhimg.com/50/v2-2f07f0294613eb2ab00ee8b156a48b56_720w.jpg?source=c8b7c179" class="origin_image zh-lightbox-thumb" width="584" data-original="https://pica.zhimg.com/v2-3936ab66fc139f0abbfc6fdbf3736312_r.jpg?source=c8b7c179"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='584'%20height='321'&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="584" data-rawheight="321" data-original-token="v2-8e903506e51512ac9f7d27cf7ab58111" data-default-watermark-src="https://picx.zhimg.com/50/v2-2f07f0294613eb2ab00ee8b156a48b56_720w.jpg?source=c8b7c179" class="origin_image zh-lightbox-thumb lazy" width="584" data-original="https://pica.zhimg.com/v2-3936ab66fc139f0abbfc6fdbf3736312_r.jpg?source=c8b7c179" data-actualsrc="https://picx.zhimg.com/50/v2-3936ab66fc139f0abbfc6fdbf3736312_720w.jpg?source=c8b7c179"><figcaption>Ref是原始数据，其他是从中间层输出恢复出的数据</figcaption></figure><p data-pid="JybyumZd">后面这两点也比较扯，浅层用全连接不太常见，想象一下高维数据的全连接层得有多少参数；浅层不扩张特征维度而是削减维度也比较奇怪，容易丢失图像信息。但也不绝对，一方面有用自编码器预训练特征提取器的，一方面还有分割领域的BottleNet<sup data-text="BottleNet: A Deep Learning Architecture for Intelligent Mobile Cloud Computing Services" data-url="https://ieeexplore.ieee.org/iel7/8819888/8824790/08824955.pdf?casa_token=zhqGepvRFSYAAAAA:DsdOaZwRc2uofnnh2jjMV8JfRP8zxo1S1Qs2Wv36AurckOEp-CwkMpciHXlXdky-sWIS0h7KFWOu" data-draft-node="inline" data-draft-type="reference" data-numero="7">[7]</sup>这类工作。最简单的，<b>我们还是可以控制候选分割位置不要太浅，来象征性的保护隐私。</b>说实话，在FL和SL中讨论隐私问题已经比较落伍了，因为公认它们保护不了隐私。</p><p data-pid="Yrm0H_dt">在其他的端云分割文章中也对smashed data的维度有讨论，比如一些无线网络优化文章<sup data-text="Efficient Parallel Split Learning over Resource-constrained Wireless Edge Networks" data-url="https://arxiv.org/pdf/2303.15991" data-draft-node="inline" data-draft-type="reference" data-numero="8">[8]</sup>考虑频谱资源调度会把这个作为一个决策变量。我最崇拜的Nicholas Lane在研究端云分割的模型早退时<sup data-text="SPINN: Synergistic Progressive Inference of Neural Networks over Device and Cloud" data-url="https://dl.acm.org/doi/pdf/10.1145/3372224.3419194" data-draft-node="inline" data-draft-type="reference" data-numero="9">[9]</sup>，也提到<b>在ReLU激活函数后面做分割有优势</b>，因为大多数神经元被置零，在做后续的数据压缩时能实现更大的压缩率。同理，我们<b>在池化层后面分割也有优势</b>，毕竟输出尺寸缩减了嘛。另外，对于MobileNet以及ResNet这种天然由block构成的网络，我们可以<b>简单地选择模块的连接处进行分割</b>，以避免切到残差连接带来的较大的数据尺寸。</p><p data-pid="C86GSuvz">对于这个smashed data的尺寸再提一句，SL未必不能实现比FL更好的通信效率。当客户端很多，端侧数据集又不大时，每次上传的数据量可以控制在较小范围内，比如我们用FashionMNIST和VGG做模拟，那么可以将数据量控制在5MB以内，而此时完整模型的大小可以达到100MB。有些人会诟病SL是串行训练，通信效率不高。确实，传统的SSL没有FL快，所以会有SplitFed<sup data-text="SplitFed: When Federated Learning Meets Split Learning" data-url="https://ojs.aaai.org/index.php/AAAI/article/download/20825/20584" data-draft-node="inline" data-draft-type="reference" data-numero="10">[10]</sup>这种文章想把它们结合到一起，但也要看到有PSL这类并行分割学习的算法<sup data-text="Server-Side Local Gradient Averaging and Learning Rate Acceleration for Scalable Split Learning" data-url="https://arxiv.org/pdf/2112.05929" data-draft-node="inline" data-draft-type="reference" data-numero="11">[11]</sup>。</p><p data-pid="DGrXiZ13">但是如果想和集中式训练做对比，SL的通信效率很难赶超。集中式训练只需要上传一次原始数据，而SL每轮都要上传数据。尽管我们的分割位置可以选择得让smashed data尺寸比原始数据小得多，但乘上通信轮数，不太可能比集中式更小。</p>