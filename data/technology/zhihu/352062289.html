<h2>写在前面</h2><p data-pid="H7JdhakA">最近在论文中看到的TensorFlow代码眼花缭乱，TF2.0语法更新很多，现在又出了JAX等改进框架，所以觉得有必要系统的从头学习一下。下面这本书是很早以前从旧书摊淘来的。2017年出版，在日新月异的技术面前就是一件古董，但为了全面地了解各个版本的TF，我决定翻一翻~</p><a href="http://link.zhihu.com/?target=https%3A//book.douban.com/subject/26976457/" data-draft-node="block" data-draft-type="link-card" data-image="https://picx.zhimg.com/v2-e8f9f939bc610be93eff58a973af0c6b_200x0.jpg?source=d16d100b" data-image-width="270" data-image-height="356" class=" wrap external" target="_blank" rel="nofollow noreferrer">Tensorflow：实战Google深度学习框架 (豆瓣)</a><p data-pid="nnfDOZ_9">这本书写的只能说差强人意，知识点介绍的很浅显，内容也不乏错误和误人子弟的地方，今天就<b>记录一下书中第62页使用TF实现神经网络解决二分类问题的错误，作者在76页又重复强调了这个错误。</b>很多博主直接copy了书中的内容而没有亲自实验，比如<a href="http://link.zhihu.com/?target=https%3A//blog.csdn.net/qq_38702419/article/details/88066433" class=" wrap external" target="_blank" rel="nofollow noreferrer">Tensorflow实现训练神经网络解决二分类问题</a>，但也有教程指出了这个问题<a href="http://link.zhihu.com/?target=http%3A//www.manongjc.com/article/50785.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">使用TensorFlow实现二分类的方法示例</a>。下文使用的环境为TensorFlow1.15.0和Python3.7.9。这段程序就是辅助理解，没有什么实际意义。</p><h2>样例程序</h2><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-6c5ea5035a0c3df3f5087935f5cdd70b_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="1920" data-rawheight="942" class="origin_image zh-lightbox-thumb" width="1920" data-original="https://pic1.zhimg.com/v2-6c5ea5035a0c3df3f5087935f5cdd70b_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1920'%20height='942'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1920" data-rawheight="942" class="origin_image zh-lightbox-thumb lazy" width="1920" data-original="https://pic1.zhimg.com/v2-6c5ea5035a0c3df3f5087935f5cdd70b_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-6c5ea5035a0c3df3f5087935f5cdd70b_720w.jpg?source=d16d100b"></figure><p data-pid="dBOo-gZ0">我们可以在<a href="http://link.zhihu.com/?target=http%3A//playground.tensorflow.org/" class=" wrap external" target="_blank" rel="nofollow noreferrer">Tensorflow Neural Network Playground</a>网站上可视化用于二分类的全连接网络，现在我们使用TF编写一个类似的网络，解决类似的问题。</p><p data-pid="psqCETO7"><b>需要引入的库如下。</b>其中RandomState函数用来生成一个固定种子的随机数生成器，相同种子的随机数序列是固定的，这样重复实验可以得到稳定的结果。</p><div class="highlight"><pre><code class="language-python"><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">RandomState</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</code></pre></div><p data-pid="nhczvk4x"><b>下面模拟数据集。</b>随机生成一些点，这些点如果在指定的圆内，则类别标签为1，就像上图中的蓝色点，否则为0。rand函数生成了一个维度为DATASET_SIZEx2的numpy数组，数值为0到1之间的均匀分布。注意标签的维度为DATASET_SIZEx1，类型为list。我们的分类任务就是给定坐标，预测该点的颜色。这里没有抽取测试集，而是直接在训练过程中展示效果。</p><div class="highlight"><pre><code class="language-python"><span></span><span class="n">dataset_X</span> <span class="o">=</span> <span class="n">RandomState</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">DATASET_SIZE</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">dataset_y</span> <span class="o">=</span> <span class="p">[[</span><span class="nb">int</span><span class="p">((</span><span class="n">x1</span><span class="o">-</span><span class="mf">0.5</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="p">(</span><span class="n">x2</span><span class="o">-</span><span class="mf">0.5</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">&lt;</span> <span class="mf">0.15</span><span class="p">)]</span> <span class="k">for</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="ow">in</span> <span class="n">dataset_X</span><span class="p">]</span>
</code></pre></div><p data-pid="texh3owE">我们知道全连接网络的前向传播其实就是矩阵乘法。本文规定输入层有2个节点，分别代表给定点的横纵坐标，中间层有4个节点，输出层为1个节点，与Playground中保持一致。对于分类问题，我们一般使用onehot向量作为标签，也就是输出层的节点数等于类别数。但二分类相当于非此即彼，就可以直接使用一个输出节点。比如说，规定节点输出值代表给定点是蓝色的概率 <img src="https://www.zhihu.com/equation?tex=p" alt="p" eeimg="1"> ，那么是橙色的概率就是 <img src="https://www.zhihu.com/equation?tex=1-p" alt="1-p" eeimg="1"> ，假设以0.5作为阈值，那么 <img src="https://www.zhihu.com/equation?tex=p%3E0.5" alt="p&gt;0.5" eeimg="1"> 为蓝色， <img src="https://www.zhihu.com/equation?tex=1-p%3E0.5" alt="1-p&gt;0.5" eeimg="1"> 为橙色。</p><p data-pid="Wk9kutmZ"><b>下面定义权重矩阵和偏置变量。</b>使用tf.Variable新建变量，其参数为初始化方法。这里使用标准差为1，均值为0（默认值）的正态分布初始化权重。固定种子为1以重复实验。</p><div class="highlight"><pre><code class="language-python"><span></span><span class="n">w1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">stddev</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">b1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">stddev</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">stddev</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">b2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="mi">1</span><span class="p">],</span> <span class="n">stddev</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div><p data-pid="RqRO9b_n">定义输入坐标和ground truth标签的占位变量，和数据集的维度一致。这里None表示大小不确定，可以自动适配feed进来的值的维度。</p><div class="highlight"><pre><code class="language-python"><span></span><span class="n">_X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">"x_input"</span><span class="p">)</span>
<span class="n">_y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">"y_input"</span><span class="p">)</span>
</code></pre></div><p data-pid="4K2Ho5Ln"><b>前向传播过程如下。</b>使用sigmoid作为激活函数来归一化结果，因为结果的概率在0~1之间。</p><div class="highlight"><pre><code class="language-python"><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">_X</span><span class="p">,</span> <span class="n">w1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span> <span class="n">a</span><span class="p">,</span> <span class="n">w2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span><span class="p">)</span>
</code></pre></div><p data-pid="ILBmyCXA"><b>反向传播过程如下。</b>使用交叉熵损失函数，使用TF提供的Adam优化器进行优化。</p><div class="highlight"><pre><code class="language-python"><span></span><span class="n">cross_entropy</span> <span class="o">=</span> <span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span>
    <span class="n">_y</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">))</span> <span class="o">+</span>
    <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">_y</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">))</span>
<span class="p">)</span>
<span class="n">train_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="mf">0.03</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">)</span>
</code></pre></div><p data-pid="HWT12AiM"><b>书中对于交叉熵的定义有误。</b>交叉熵损失函数的定义可以参考这篇文章<a href="http://link.zhihu.com/?target=https%3A//blog.csdn.net/chao_shine/article/details/89925762" class=" wrap external" target="_blank" rel="nofollow noreferrer">交叉熵损失函数原理</a>，公式大致如下：</p><p data-pid="L7bB-0y_"><img src="https://www.zhihu.com/equation?tex=H%28p%2Cq%29%3D-%5Csum_i+p%28x_i%29%5Clog%28q%28x_i%29%29%5C%5C" alt="H(p,q)=-\sum_i p(x_i)\log(q(x_i))\\" eeimg="1"></p><p data-pid="iKXFiTJe">其中 <img src="https://www.zhihu.com/equation?tex=x_i" alt="x_i" eeimg="1"> 代表输入的第 <img src="https://www.zhihu.com/equation?tex=i" alt="i" eeimg="1"> 个样本的特征向量， <img src="https://www.zhihu.com/equation?tex=p%28x_i%29" alt="p(x_i)" eeimg="1"> 和 <img src="https://www.zhihu.com/equation?tex=q%28x_i%29" alt="q(x_i)" eeimg="1"> 分别代表该样本对应的在各类别上真实的概率分布和预测的概率分布。求和号表示总的交叉熵等于所有样本的交叉熵之和。这里有一个常见的误区，很多人认为二分类数据集的标签代表的就是这里的概率，0表示蓝色的概率为0，即为橙色，1表示蓝色的概率为1，即为蓝色。然后就想当然的把上式变成：</p><p data-pid="oY6ivH9q"><img src="https://www.zhihu.com/equation?tex=H%3D-%5Csum_i+%5Csum_ky_k%5Clog%28%5Chat%7By_k%7D%29%3D-%5Csum_i+%281+%5Ccdot+%5Clog%28%5Chat%7By%7D%29%2B0%5Ccdot%5Clog%28%5Chat%7By%7D%29%29%5C%5C" alt="H=-\sum_i \sum_ky_k\log(\hat{y_k})=-\sum_i (1 \cdot \log(\hat{y})+0\cdot\log(\hat{y}))\\" eeimg="1"> </p><p data-pid="wNUZnDxi">书中的例程就是这样的逻辑，极具误导性。其结果显而易见，优化器将使预测值 <img src="https://www.zhihu.com/equation?tex=%5Chat%7By%7D" alt="\hat{y}" eeimg="1"> 不断接近1，这样交叉熵就可以取到最小值0。直接运行书中的例程确实可以得到损失函数不断下降并收敛的假象，但只要稍微看一下预测结果就会发现问题。</p><p data-pid="ASzGcGAf"><b>这里犯的错误就是混淆了概率和概率分布。</b>上文提到，二分类中简化了输出层，用一个概率值来确定一个概率分布。但这并不代表公式里就可以直接代入。正确的做法是从概率值恢复出概率分布，再代入到公式中。</p><p data-pid="1WQJH19j"><img src="https://www.zhihu.com/equation?tex=H%3D-%5Csum_i+%28y+%5Clog%28%5Chat%7By%7D%29%2B%281-y%29%5Clog%281-%5Chat%7By%7D%29%5C%5C" alt="H=-\sum_i (y \log(\hat{y})+(1-y)\log(1-\hat{y})\\" eeimg="1"> </p><p data-pid="ny6e01QL">如果输出层使用两个节点，标签使用 <img src="https://www.zhihu.com/equation?tex=%281%2C0%29" alt="(1,0)" eeimg="1"> 表示类别0，使用 <img src="https://www.zhihu.com/equation?tex=%280%2C1%29" alt="(0,1)" eeimg="1"> 表示类别1。那么上面错误的公式可以修改为</p><p data-pid="4qeCkt60"><img src="https://www.zhihu.com/equation?tex=H%3D-%5Csum_i+%5Csum_ky_k+%5Clog%28%5Chat%7By_k%7D%29%3D-%5Csum_i+%28y_0+%5Clog%28%5Chat%7By_0%7D%29%2By_1%5Clog%28%5Chat%7By_1%7D%29%29%5C%5C" alt="H=-\sum_i \sum_ky_k \log(\hat{y_k})=-\sum_i (y_0 \log(\hat{y_0})+y_1\log(\hat{y_1}))\\" eeimg="1"> </p><p data-pid="V52L4N-b">这里由于 <img src="https://www.zhihu.com/equation?tex=y_0" alt="y_0" eeimg="1"> 和 <img src="https://www.zhihu.com/equation?tex=y_1" alt="y_1" eeimg="1"> 都既可以取0也可以取1，所以就不会发生上述的问题啦。在实际操作中，选用tf.reduce_mean计算平均值来代替求和同样能表征损失函数。另外由于我们的输出值可能取到0，这会使得-log的值无穷大。一般来说需要使用softmax来处理这个问题，但由于这里只有两个类别，可以简单地设置一个极小值1e-10来代替0。tf.clip_by_value(y,1e-10,1.0)将小于这个极小值的数值都设置成它。</p><p data-pid="tRWYe3fi"><b>接下来进行训练。</b>每隔2000步输出一下总的交叉熵。训练完毕后获取所有样本上的预测值。</p><div class="highlight"><pre><code class="language-python"><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">STEPS</span><span class="p">):</span>
        <span class="n">beg</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">BATCH_SIZE</span><span class="p">)</span> <span class="o">%</span> <span class="n">DATASET_SIZE</span>
        <span class="n">end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">beg</span> <span class="o">+</span> <span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">DATASET_SIZE</span><span class="p">)</span>
        <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">train_step</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">_X</span><span class="p">:</span> <span class="n">dataset_X</span><span class="p">[</span><span class="n">beg</span><span class="p">:</span> <span class="n">end</span><span class="p">],</span> <span class="n">_y</span><span class="p">:</span> <span class="n">dataset_y</span><span class="p">[</span><span class="n">beg</span><span class="p">:</span> <span class="n">end</span><span class="p">]})</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">total_cross_entropy</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
                <span class="n">cross_entropy</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">_X</span><span class="p">:</span> <span class="n">dataset_X</span><span class="p">,</span> <span class="n">_y</span><span class="p">:</span> <span class="n">dataset_y</span><span class="p">}</span>
            <span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"After </span><span class="si">{:&gt;5d}</span><span class="s2"> training_step(s), loss is </span><span class="si">{:.4f}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">total_cross_entropy</span><span class="p">))</span>
    <span class="n">predict</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">_X</span><span class="p">:</span> <span class="n">dataset_X</span><span class="p">})</span>
</code></pre></div><p data-pid="1a2TtwKp">我们将预测类别和真实类别可视化一下看看效果</p><div class="highlight"><pre><code class="language-python"><span></span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataset_X</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s2">"orange"</span><span class="p">,</span> <span class="s2">"blue"</span><span class="p">][</span><span class="nb">int</span><span class="p">(</span><span class="n">predict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">)])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataset_X</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s2">"orange"</span><span class="p">,</span> <span class="s2">"blue"</span><span class="p">][</span><span class="n">dataset_y</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p data-pid="vUz0nodu"><b>实验结果如下。</b>左侧的是预测值，右侧是真实值，可以看出两者非常接近了</p><div class="highlight"><pre><code class="language-text"><span></span>After     0 training_step(s), loss is 0.7133
After  2000 training_step(s), loss is 0.1970
After  4000 training_step(s), loss is 0.0976
After  6000 training_step(s), loss is 0.0678
After  8000 training_step(s), loss is 0.0486
</code></pre></div><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-6e4f0a204c08f311cbc15a0b208313d5_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="2296" data-rawheight="676" class="origin_image zh-lightbox-thumb" width="2296" data-original="https://pic1.zhimg.com/v2-6e4f0a204c08f311cbc15a0b208313d5_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='2296'%20height='676'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="2296" data-rawheight="676" class="origin_image zh-lightbox-thumb lazy" width="2296" data-original="https://pic1.zhimg.com/v2-6e4f0a204c08f311cbc15a0b208313d5_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-6e4f0a204c08f311cbc15a0b208313d5_720w.jpg?source=d16d100b"></figure><p data-pid="R9VAB4HN">书中的例程实际是不包含偏置值和非线性激活函数的。去掉b1和b2，将学习率修改为0.06，再次实验的效果如下</p><div class="highlight"><pre><code class="language-text"><span></span>After     0 training_step(s), loss is 0.7246
After  2000 training_step(s), loss is 0.4504
After  4000 training_step(s), loss is 0.3730
After  6000 training_step(s), loss is 0.3188
After  8000 training_step(s), loss is 0.2945
</code></pre></div><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-66fc827296ca669d8403b6282876cab7_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="2880" data-rawheight="810" class="origin_image zh-lightbox-thumb" width="2880" data-original="https://picx.zhimg.com/v2-66fc827296ca669d8403b6282876cab7_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='2880'%20height='810'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="2880" data-rawheight="810" class="origin_image zh-lightbox-thumb lazy" width="2880" data-original="https://picx.zhimg.com/v2-66fc827296ca669d8403b6282876cab7_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-66fc827296ca669d8403b6282876cab7_720w.jpg?source=d16d100b"></figure><p data-pid="A_0aEg1i">可以看出失去了偏置值，神经网络的表征能力明显下降。这相当于在后一层网络对前层的输出线性变换时无法进行平移，而只能简单的加权。在Playground中可视化一下各个神经元的特征图，可以看出具有偏置的神经网络隐藏层的四个节点像是割圆法的四条切线，重叠在一起时大致将圆的轮廓勾勒出来。输入的横纵坐标可以看做是横线和竖线，而隐藏层加权得到斜线，通过偏置值，将斜线平移到合适的位置，最终输出层将四条斜线综合起来。</p><p data-pid="rLfTUuBg">至于完全不使用激活函数，暂且不提非线性表征能力，光是输出值无法归一化到概率的取值范围，即0到1之间，就是个巨大的问题。这里不给出结果了，因为效果实在太差。但是一个有趣的事实是，如果只添加一层激活函数，在隐藏层和输出层之间归一化的效果要比在输出层之后效果要好，当然其原因也显而易见。</p><hr><p data-pid="0xhAgPsT"><b>2021-02-23更新：</b>激活函数归一化的说法不太严谨，sigmoid的归一化只是一个副作用。类似ReLU的激活函数并不能对值域进行压缩，真正将输出变为概率分布的其实是最后一步的softmax。这里由于使用的是阈值截断的方式，自然去除激活函数之后效果很差。可以尝试使用softmax，去除sigmoid再次实验。</p><p data-pid="uJNjuzZT"><b>最后给出完整代码</b></p><div class="highlight"><pre><code class="language-python"><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">RandomState</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">w1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">stddev</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">b1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">stddev</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">stddev</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">b2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_normal</span><span class="p">([</span><span class="mi">1</span><span class="p">],</span> <span class="n">stddev</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="n">_X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">"x_input"</span><span class="p">)</span>
<span class="n">_y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">"y_input"</span><span class="p">)</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">_X</span><span class="p">,</span> <span class="n">w1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span> <span class="n">a</span><span class="p">,</span> <span class="n">w2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span><span class="p">)</span>

<span class="n">cross_entropy</span> <span class="o">=</span> <span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span>
    <span class="n">_y</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">))</span> <span class="o">+</span>
    <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">_y</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">))</span>
<span class="p">)</span>
<span class="n">train_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="mf">0.03</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">)</span>

<span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">DATASET_SIZE</span><span class="p">,</span> <span class="n">STEPS</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">10000</span>
<span class="n">dataset_X</span> <span class="o">=</span> <span class="n">RandomState</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">DATASET_SIZE</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">dataset_y</span> <span class="o">=</span> <span class="p">[[</span><span class="nb">int</span><span class="p">((</span><span class="n">x1</span><span class="o">-</span><span class="mf">0.5</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="p">(</span><span class="n">x2</span><span class="o">-</span><span class="mf">0.5</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">&lt;</span> <span class="mf">0.15</span><span class="p">)]</span> <span class="k">for</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="ow">in</span> <span class="n">dataset_X</span><span class="p">]</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">STEPS</span><span class="p">):</span>
        <span class="n">beg</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">BATCH_SIZE</span><span class="p">)</span> <span class="o">%</span> <span class="n">DATASET_SIZE</span>
        <span class="n">end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">beg</span> <span class="o">+</span> <span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">DATASET_SIZE</span><span class="p">)</span>
        <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">train_step</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">_X</span><span class="p">:</span> <span class="n">dataset_X</span><span class="p">[</span><span class="n">beg</span><span class="p">:</span> <span class="n">end</span><span class="p">],</span> <span class="n">_y</span><span class="p">:</span> <span class="n">dataset_y</span><span class="p">[</span><span class="n">beg</span><span class="p">:</span> <span class="n">end</span><span class="p">]})</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">total_cross_entropy</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
                <span class="n">cross_entropy</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">_X</span><span class="p">:</span> <span class="n">dataset_X</span><span class="p">,</span> <span class="n">_y</span><span class="p">:</span> <span class="n">dataset_y</span><span class="p">}</span>
            <span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"After </span><span class="si">{:&gt;5d}</span><span class="s2"> training_step(s), loss is </span><span class="si">{:.4f}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">total_cross_entropy</span><span class="p">))</span>
    <span class="n">predict</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">_X</span><span class="p">:</span> <span class="n">dataset_X</span><span class="p">})</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataset_X</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s2">"orange"</span><span class="p">,</span> <span class="s2">"blue"</span><span class="p">][</span><span class="nb">int</span><span class="p">(</span><span class="n">predict</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">)])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataset_X</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s2">"orange"</span><span class="p">,</span> <span class="s2">"blue"</span><span class="p">][</span><span class="n">dataset_y</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p></p>