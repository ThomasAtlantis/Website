<h2>1 写在前面</h2><p data-pid="efajxZmo">深入浅出，在计算机教材界被用滥的词，总是继承着领域小白的初心和梦想。顾名思义，它既意味着理解得透彻，又要求复述得通俗。如果说复述是大名鼎鼎的费曼学习法的精髓，那么复刻便是其在程序世界最恰当的对应概念。君不见深度学习水涨船高，计算框架层出不穷。想要深入浅出这些框架，何不亲自动手复刻个轮子？</p><p data-pid="VagufnKT">自动求导机制是神经网络计算框架必备的组件，被赋予了多种称呼：Autograd、Autodiff、自动求导、自动梯度、自动微分。万变不离其宗，在神经网络的训练过程中，它被用来计算权重的最速下降方向，以指导优化器下一步迭代时对权重的更新。有了自动求导（比如JAX框架的grad），再辅之以矩阵运算（比如Numpy），就可以实现神经网络的基本功能了。</p><p data-pid="0Pxjk-iN">最近和 <a class="member_mention" href="http://www.zhihu.com/people/e5dc89fa048c571edf526743d2982896" data-hash="e5dc89fa048c571edf526743d2982896" data-hovercard="p$b$e5dc89fa048c571edf526743d2982896">@王桂波</a> 博主交流，受益良多。本文的框架使用Python3编写，主要参考了他的教程 <a href="http://link.zhihu.com/?target=https%3A//borgwang.github.io/dl/2019/09/15/autograd.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">Automatic Differentiation Tutorial</a> ，并补充了一些网上未曾讨论但很重要的细节。本系列会持续更新，分析一些常见的算子设计，之后还会给出使用本文的框架做线性回归和神经网络训练的例子，敬请关注！</p><p data-pid="6PxZz7H9">--------------------------</p><p data-pid="qQ8ndr_j"><b>2021年11月21日更新</b>：介绍broadcast算子设计和用户自定义算子，代码还有待整理，线性回归和神经网络的例子可能会另出一片文章。</p><h2>2 反向传播</h2><p data-pid="PH-OKuVP">这里以最简单的前馈神经网络为例（没有反馈链路的多层感知机）。正向传播是指输入数据逐层推断，最后得到预测值的过程，是机器对已学知识的演练；反向传播则是比较预测值和真实值，根据定义的损失函数反向逐层归谬的过程，是自我批评、寻找不足之处的改进。</p><p data-pid="7fjyOTMH">具体地，反向传播会求出损失值对于各权重的负梯度，来寻找改进的最佳方向。各层在推断时只是接收上层的信息、做决策、传递结果给下一层，所以在归谬时只需要各层自我反思即可。那么距离输出最远的输入层则归谬最为复杂。直觉上讲，它的决策经过了多层修改，想要判断好不好已经很模糊了。</p><h2>3 链式法则</h2><p data-pid="0ZLVhPOG">复合函数的链式法则是反向逐层计算梯度的理论支持。以最简单的一元（标量）复合函数为例，链式法则如下：</p><p data-pid="suu_8rgT"><img src="https://www.zhihu.com/equation?tex=y%3Df_1%28f_2%28...f_n%28x%29%29%29%5CRightarrow%5C+%5Cfrac%7Bd+y%7D%7Bd+x%7D+%3D+%5Cfrac%7Bd+y%7D%7Bd+f_1%7D%5Ccdot%5Cfrac%7Bd+f_1%7D%7Bd+f_2%7D%5Ccdots%5Cfrac%7Bd+f_%7Bn-1%7D%7D%7Bd+f_n%7D%5Ccdot%5Cfrac%7Bd+f_n%7D%7Bd+x%7D%5C%5C" alt="y=f_1(f_2(...f_n(x)))\Rightarrow\ \frac{d y}{d x} = \frac{d y}{d f_1}\cdot\frac{d f_1}{d f_2}\cdots\frac{d f_{n-1}}{d f_n}\cdot\frac{d f_n}{d x}\\" eeimg="1">类比神经网络，如果上式右侧连乘的每一项代表各层的局部梯度，则网络的输出对于各层的全局梯度就等于，从这一层开始到输出层的各层局部梯度的连乘。正因为输出层求梯度连乘只有一项，而输入层需要连乘所有项，求梯度的过程是反向逐层进行的。为什么可以这么类比呢？</p><p data-pid="zkK3A4Kh">一个两层的多层感知机可以定义如下，其中 <img src="https://www.zhihu.com/equation?tex=x" alt="x" eeimg="1"> 是输入， <img src="https://www.zhihu.com/equation?tex=o" alt="o" eeimg="1"> 是各层输出， <img src="https://www.zhihu.com/equation?tex=W" alt="W" eeimg="1"> 是权重矩阵， <img src="https://www.zhihu.com/equation?tex=b" alt="b" eeimg="1"> 是各层偏置， <img src="https://www.zhihu.com/equation?tex=%5Csigma" alt="\sigma" eeimg="1"> 是非线性激活函数：</p><p data-pid="NqhpJmeS"><img src="https://www.zhihu.com/equation?tex=o_1%3D%5Csigma_1%28xW_1%2Bb_1%29%2C+%5C+o_2%3D%5Csigma_2%28o_1W_2%2Bb_2%29%5C%5C" alt="o_1=\sigma_1(xW_1+b_1), \ o_2=\sigma_2(o_1W_2+b_2)\\" eeimg="1"></p><p data-pid="jWubunAz">根据上式， <img src="https://www.zhihu.com/equation?tex=o_2" alt="o_2" eeimg="1"> 显然可以看做 <img src="https://www.zhihu.com/equation?tex=x" alt="x" eeimg="1"> 的一个复合函数：</p><p data-pid="hSAN4asV"><img src="https://www.zhihu.com/equation?tex=o_2%3D+%5Csigma_2%28%5Csigma_1%28xW_1%2Bb_1%29W_2%2Bb_2%29%5C%5C" alt="o_2= \sigma_2(\sigma_1(xW_1+b_1)W_2+b_2)\\" eeimg="1">只不过要注意，神经网络的复合函数是矩阵函数，而<b>矩阵函数的链式法则有自己的规律，并不是简单的点积</b>！！！尽管没有统一的规律，要实现也并不困难，因为神经网络中能够用到的运算很有限，我们只需要按照矩阵粒度，将所有用到的运算的链式法则穷举定义即可。</p><p data-pid="1V6ltpGr">以矩阵作为梯度的最小单位而不是神经网络的一层，是为了更灵活的表达能力：任何运算过程都可以按照计算顺序看做复合函数：</p><p data-pid="MCtea7dD"><img src="https://www.zhihu.com/equation?tex=a%5Cdiv+b%2Bc%5Ctimes+d%5CRightarrow+%5Cmathrm%7Badd%7D%28%5Cmathrm%7Bdiv%7D%28a%2Cb%29%2C%5Cmathrm%7Bmul%7D%28c%2Cd%29%29%5C%5C" alt="a\div b+c\times d\Rightarrow \mathrm{add}(\mathrm{div}(a,b),\mathrm{mul}(c,d))\\" eeimg="1">实现了基础原子运算的链式法则，就可以适配各种类型的网络定义了。</p><h2>4 总体框架</h2><h3>4.1 封装张量类型</h3><p data-pid="jwLed9k4">我们将具备自动求导功能的矩阵封装成一个叫做张量的类：</p><div class="highlight"><pre><code class="language-python"><span></span><span class="c1"># import numpy as np</span>
<span class="k">class</span> <span class="nc">Tensor</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dependency</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">shape</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">requires_grad</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="n">requires_grad</span>

        <span class="k">if</span> <span class="n">dependency</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="n">dependency</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dependency</span> <span class="o">=</span> <span class="n">dependency</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">values</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_values</span>

    <span class="nd">@values</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_values</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">new_values</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div><p data-pid="1_RKbLoh">类的成员变量的作用如下：</p><p data-pid="OOlFh7Wp">values：通过初始化函数传入初值。被property装饰器定义为可读写的属性，主要为了在类外部对其进行赋值修改时控制其值始终为Numpy的ndarray类型。关于这部分Python语法可以参考<a href="http://link.zhihu.com/?target=https%3A//www.cnblogs.com/yangzhen-ahujhc/p/12300189.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">python中的property装饰器</a>，是工程中常用的Getter/Setter<a href="http://link.zhihu.com/?target=https%3A//www.runoob.com/design-pattern/design-pattern-intro.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">设计模式</a>。</p><p data-pid="Jrb4uBC9">grad：存储该矩阵最终的全局梯度值。</p><p data-pid="f9AVqqFL">requires_grad：表明该矩阵是否参与梯度计算。如果参与则给grad分配空间并初始化为与值形状相同的全零矩阵；如果不参与梯度计算，则不分配空间以优化内存效率。这里梯度清零操作定义如下：</p><div class="highlight"><pre><code class="language-python"><span></span><span class="k">class</span> <span class="nc">Tensor</span><span class="p">:</span>
    <span class="c1"># ...</span>
    <span class="k">def</span> <span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div><p data-pid="YS4IEZAp">dependency：当前矩阵可能储存的是某个运算的结果，我们需要记录其梯度如何向操作数矩阵传播。由于操作数可能不唯一，这个属性是列表类型。其中每一项将会是一个字典，字典的tensor字段指代操作数的张量，grad_fn字段指代传播到该张量需要执行的函数。</p><h3>4.2 实现反向传播</h3><p data-pid="VqX3_Lpq">我们先来看一下反向传播的定义，稍后再讨论梯度清零功能的必要性：</p><div class="highlight"><pre><code class="language-python"><span></span><span class="k">class</span> <span class="nc">Tensor</span><span class="p">:</span>
    <span class="c1"># ...</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="s2">"Call backward() on a non-requires-grad tensor."</span>
        <span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">),</span> <span class="s2">"grad can be implicitly created only for scalar outputs"</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="k">if</span> <span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">grad</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="n">grad</span>

        <span class="k">for</span> <span class="n">dep</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">dependency</span><span class="p">:</span>
            <span class="n">grad_for_dep</span> <span class="o">=</span> <span class="n">dep</span><span class="p">[</span><span class="s2">"grad_fn"</span><span class="p">](</span><span class="n">grad</span><span class="p">)</span>
            <span class="n">dep</span><span class="p">[</span><span class="s2">"tensor"</span><span class="p">]</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">grad_for_dep</span><span class="p">)</span>
</code></pre></div><p data-pid="qd1rJHUj">在我们的程序逻辑里，当前张量最终的梯度是在上级函数处计算完毕后传进来的。仍以上面的算数运算复合函数为例：</p><p data-pid="JlvZLp2h"><img src="https://www.zhihu.com/equation?tex=T_1%3D%5Cmathrm%7Bdiv%7D%28a%2Cb%29%2C%5C+T_2%3D%5Cmathrm%7Bmul%7D%28c%2Cd%29%2C%5C+T_3%3D%5Cmathrm%7Badd%7D%28T_1%2CT_2%29%5C%5C" alt="T_1=\mathrm{div}(a,b),\ T_2=\mathrm{mul}(c,d),\ T_3=\mathrm{add}(T_1,T_2)\\" eeimg="1">张量 <img src="https://www.zhihu.com/equation?tex=T_1" alt="T_1" eeimg="1"> 的偏导数 <img src="https://www.zhihu.com/equation?tex=%5Cpartial+T_3%2F%5Cpartial+T_1" alt="\partial T_3/\partial T_1" eeimg="1"> 就是由张量 <img src="https://www.zhihu.com/equation?tex=T_3" alt="T_3" eeimg="1"> 的反向传播函数计算好后传给 <img src="https://www.zhihu.com/equation?tex=T_1" alt="T_1" eeimg="1"> 的。这只是为了编程方便，没什么道理。如果当前张量就是计算图的最后一个节点（即反向传播开始的节点，比如 <img src="https://www.zhihu.com/equation?tex=T_3" alt="T_3" eeimg="1"> ），那么就不需要传入grad，下面在判断到这种情况时会将grad初始化为1.0。</p><p data-pid="0X0-vF0X">函数中首先两个断言，第一个判断要求当前张量参与梯度计算，第二个判断要求当前张量为输出节点时必须是标量才能反向传播。在Pytorch中就不支持非标量对向量反向传播求梯度，否则会报以下错误：</p><div class="highlight"><pre><code class="language-text"><span></span>RuntimeError: grad can be implicitly created only for scalar outputs
</code></pre></div><p data-pid="I87fE8Pf">这个很好理解：假如输出节点是向量，向量对向量求梯度会得到一个矩阵，而对矩阵求梯度就会得到更高维的矩阵，如此传播下去程序将会不可控；而标量对向量求梯度得到向量，对矩阵求梯度得到矩阵，程序可以链式传播。因此，我们必须在设计损失函数的时候就想办法压缩维度，让最后得到一个标量。</p><p data-pid="DkX33yNd">接下来<b>将现有的梯度上加上传入的梯度，这是一个有意思的Trick。</b>在这些矩阵第一次参与反向传播，或被我们手动调用zero_grad()清零后，就相当于将张量的梯度直接赋值为传入的梯度。然而如果我们不进行梯度清零，这里的梯度就会累加。下面插播一下实现这种梯度累加的好处，同样也是梯度清零的意义。</p><figure data-size="normal"><noscript><img src="https://pica.zhimg.com/v2-14e8ae15f498d1320c6afa67206dbf7a_720w.png?source=d16d100b" data-size="normal" data-rawwidth="2178" data-rawheight="134" class="origin_image zh-lightbox-thumb" width="2178" data-original="https://picx.zhimg.com/v2-14e8ae15f498d1320c6afa67206dbf7a_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='2178'%20height='134'&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="2178" data-rawheight="134" class="origin_image zh-lightbox-thumb lazy" width="2178" data-original="https://picx.zhimg.com/v2-14e8ae15f498d1320c6afa67206dbf7a_720w.jpg?source=d16d100b" data-actualsrc="https://pica.zhimg.com/v2-14e8ae15f498d1320c6afa67206dbf7a_720w.png?source=d16d100b"><figcaption>BEGIN TIP</figcaption></figure><p data-pid="WgkVaHkN">在神经网络训练中，我们使用的小批量梯度下降一般都是将一批样本的损失函数求和，然后进行反向传播。我们在定义网络的时候往往需要特别指定输入的形状，并且可以多留出一个维度用于设置弹性的batchsize。现代计算框架一般都会提供使用GPU并行处理梯度计算图的能力，我猜想框架会隐式地为一批样本建立同样数量的计算图，同时加载到显存中。在计算好每个计算图节点的梯度后再进行累加合并。</p><p data-pid="Pq6vpQd-">这种猜想不无根据，之前在Jetson TX2上跑实验时发现batchsize超过8就会炸显存。TX2是CPU/GPU共用8G内存的，而当时模型大小大概260MB，使用的是Adam优化器。我不知道有没有专门做模型训练内存/显存占用预测的论文，但这篇博客<a href="http://link.zhihu.com/?target=https%3A//blog.csdn.net/lien0906/article/details/78863118" class=" wrap external" target="_blank" rel="nofollow noreferrer">深度学习中GPU和显存分析</a>对显存占用讲得很透彻。据分析，Adam优化器需要额外的3倍模型大小来存储梯度和动量等信息。那么假设模型的参数和梯度分别需要一份整体的备份在内存里，Adam占用3倍模型大小，每次训练同时加载8份Adam，则需要的总大小为：</p><p data-pid="8GemMAN9"><img src="https://www.zhihu.com/equation?tex=%282%2B3%5Ctimes8%29%5Ctimes260%5Cmathrm%7BMB%7D+%5Capprox+6.6%5Cmathrm%7BGB%7D%5C%5C" alt="(2+3\times8)\times260\mathrm{MB} \approx 6.6\mathrm{GB}\\" eeimg="1"></p><p data-pid="z6nKdepA">再加上样本数据和各层激活值占用的内存以及操作系统占用的内存，确实跑起来很吃力。另一个例子来自论坛的问题<a href="http://link.zhihu.com/?target=https%3A//discuss.gluon.ai/t/topic/5831" class=" wrap external" target="_blank" rel="nofollow noreferrer">mxnet有在有限显存的情况下增大batchsize的方法吗？</a>，题主使用resnet152-v2模型（根据<a href="http://link.zhihu.com/?target=https%3A//keras.io/api/applications/" class=" wrap external" target="_blank" rel="nofollow noreferrer">Keras documentation: Keras Applications</a>、精度为单精度浮点数计算，模型大小约230MB。）在显存大小为6GB的GTX1060上也只能把batchsize设成16。</p><p data-pid="v98BkhOw">参考<a href="https://www.zhihu.com/question/303070254/answer/573037166" class="internal" target="_blank">PyTorch中在反向传播前为什么要手动将梯度清零？</a>，其实我们可以逐个样本进行梯度计算，只要不在每轮都进行梯度清零，梯度就会一直累加，和批处理的效果是近似的。比如每隔8个样本进行一次梯度清零和优化器的step，就相当于batchsize设成了8。这样我们可以在有限的显存条件下尽可能加大batchsize。当然这么做也是有代价的，本身并行的梯度计算变成了串行的，更额外引入了多次访存的开销，程序会变得很慢。</p><figure data-size="normal"><noscript><img src="https://pica.zhimg.com/v2-1ec53668437e31220a5196051e845338_720w.png?source=d16d100b" data-size="normal" data-rawwidth="1440" data-rawheight="89" class="origin_image zh-lightbox-thumb" width="1440" data-original="https://pica.zhimg.com/v2-1ec53668437e31220a5196051e845338_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1440'%20height='89'&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="1440" data-rawheight="89" class="origin_image zh-lightbox-thumb lazy" width="1440" data-original="https://pica.zhimg.com/v2-1ec53668437e31220a5196051e845338_720w.jpg?source=d16d100b" data-actualsrc="https://pica.zhimg.com/v2-1ec53668437e31220a5196051e845338_720w.png?source=d16d100b"><figcaption>END TIP</figcaption></figure><p data-pid="rjbpqyU7">继续分析反向传播的代码，下面这部分可谓是反向传播的精髓：</p><div class="highlight"><pre><code class="language-python"><span></span><span class="c1"># ...</span>
<span class="k">for</span> <span class="n">dep</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">dependency</span><span class="p">:</span>
    <span class="n">grad_for_dep</span> <span class="o">=</span> <span class="n">dep</span><span class="p">[</span><span class="s2">"grad_fn"</span><span class="p">](</span><span class="n">grad</span><span class="p">)</span>
    <span class="n">dep</span><span class="p">[</span><span class="s2">"tensor"</span><span class="p">]</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">grad_for_dep</span><span class="p">)</span>
</code></pre></div><p data-pid="apyB64aG">循环遍历当前张量（操作结果）对应的操作数，调用grad_fn指向的梯度传播函数，将下一个节点梯度计算出来，递归调用该节点的反向传播函数，同时传入计算好的梯度。总体上看，这是一个对计算树深度优先遍历的过程。之前我们一直说的是计算图，但是想一想常见运算一般都是单/多输入单输出，多个输出的情况很少（比如divmod，从输出节点倒推就是树形结构。</p><h3>4.3 框架设计小结</h3><p data-pid="PnbAhnz_">整理一下，自动求导的整体框架如下图所示，这里使用的例子是：</p><p data-pid="2rRuMuqI"><img src="https://www.zhihu.com/equation?tex=%5Ctext%7BTensor_o%7D%3D%5Cmathrm%7B%5Cmathbf%7Bmul%7D%7D%28%5Ctext%7BTensor_a%7D%2C+%5Ctext%7BTensor_b%7D%29%5C%5C" alt="\text{Tensor_o}=\mathrm{\mathbf{mul}}(\text{Tensor_a}, \text{Tensor_b})\\" eeimg="1"></p><figure data-size="normal"><noscript><img src="https://pica.zhimg.com/v2-71742cbf97a321fea0f8dfd8158510bf_720w.jpg?source=d16d100b" data-size="normal" data-rawwidth="1610" data-rawheight="973" class="origin_image zh-lightbox-thumb" width="1610" data-original="https://pic1.zhimg.com/v2-71742cbf97a321fea0f8dfd8158510bf_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1610'%20height='973'&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="1610" data-rawheight="973" class="origin_image zh-lightbox-thumb lazy" width="1610" data-original="https://pic1.zhimg.com/v2-71742cbf97a321fea0f8dfd8158510bf_720w.jpg?source=d16d100b" data-actualsrc="https://pica.zhimg.com/v2-71742cbf97a321fea0f8dfd8158510bf_720w.jpg?source=d16d100b"><figcaption>Autograd总体框架：绘图不易，转载请注明出处</figcaption></figure><p data-pid="sanwuuia">可以看到连接线密集的部分就是mul这个算子，这是反向传播实现的核心部分，接下来我们具体展开，看看算子如何定义，其梯度传播的grad_fn究竟长什么样子。</p><h2>5 算子设计</h2><p data-pid="LXqhM_LJ">我们把张量支持的运算函数叫做算子。为了更符合用户的使用习惯，以及表达式定义更自然，算子往往通过张量的运算符重载实现。算子函数内部既要实现正向传播时正常的运算功能，又要提供反向传播时梯度计算和传递的规则。下面我以三种最常见类型的运算为例，介绍一下算子的设计方法。</p><h3>5.1 矩阵乘法算子</h3><p data-pid="Uf8xYYJZ">我们以矩阵乘法为例。在Python的Numpy库中使用@运算符表示矩阵乘法，对应的运算符重载函数为__matmul__。</p><div class="highlight"><pre><code class="language-python"><span></span><span class="k">def</span> <span class="nf">as_tensor</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">obj</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">obj</span>

<span class="k">class</span> <span class="nc">Tensor</span><span class="p">:</span>
    <span class="c1"># ...</span>
    <span class="k">def</span> <span class="fm">__matmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="c1"># 0. make sure other is Tensor</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">as_tensor</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>

        <span class="c1"># 1. calculate forward values</span>
        <span class="n">values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span> <span class="o">@</span> <span class="n">other</span><span class="o">.</span><span class="n">values</span>

        <span class="c1"># 2. whether output tensor requires_grad</span>
        <span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">or</span> <span class="n">other</span><span class="o">.</span><span class="n">requires_grad</span>

        <span class="c1"># 3. build dependency list</span>
        <span class="n">dependency</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="k">def</span> <span class="nf">grad_fn1</span><span class="p">(</span><span class="n">grad</span><span class="p">):</span>
            	<span class="k">pass</span>  <span class="c1"># TODO HERE</span>
            <span class="n">dependency</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=</span><span class="n">grad_fn1</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">other</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="k">def</span> <span class="nf">grad_fn2</span><span class="p">(</span><span class="n">grad</span><span class="p">):</span>
                <span class="k">pass</span>  <span class="c1"># TODO HERE</span>
            <span class="n">dependency</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">other</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=</span><span class="n">grad_fn2</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">requires_grad</span><span class="p">,</span> <span class="n">dependency</span><span class="p">)</span>
</code></pre></div><p data-pid="CyvSSGeD">首先要保证另一个操作数是张量，在矩阵乘法中一般不会出现问题，但在数乘中，other可能就只是一个数字。然后values直接计算矩阵乘法结果，作为返回的张量的值。之后，两个操作数只要有一个需要计算张量，结果就需要计算张量，否则计算树就截断了。最后构造依赖项，我们需要分别定义操作结果对两个操作数的梯度求解和传递过程。如果是标量乘法，这里毫无疑问会很简单：</p><p data-pid="4Dz5josn"><img src="https://www.zhihu.com/equation?tex=o%3Df%28c%29%2C%5C+c%3Da%5Ctimes+b%5C%5C" alt="o=f(c),\ c=a\times b\\" eeimg="1">在 <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"> 的乘法运算符重载函数被调用时，只需要将 <img src="https://www.zhihu.com/equation?tex=c" alt="c" eeimg="1"> 对 <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"> 和 <img src="https://www.zhihu.com/equation?tex=b" alt="b" eeimg="1"> 的梯度计算和传递分别定义如下：</p><p data-pid="iTM-y3TL"><img src="https://www.zhihu.com/equation?tex=%5Ctext%7Bgrad_fn_1%7D%28g%29%3Dg%5Ccdot%5Cfrac%7B%5Cpartial+c%7D%7B%5Cpartial+a%7D%3Dgb%2C+%5C+%5Ctext%7Bgrad_fn_2%7D%28g%29%3Dg%5Ccdot%5Cfrac%7B%5Cpartial+c%7D%7B%5Cpartial+b%7D%3Dga%5C%5C" alt="\text{grad_fn_1}(g)=g\cdot\frac{\partial c}{\partial a}=gb, \ \text{grad_fn_2}(g)=g\cdot\frac{\partial c}{\partial b}=ga\\" eeimg="1">从 <img src="https://www.zhihu.com/equation?tex=o" alt="o" eeimg="1"> 开始反向传播，传播到 <img src="https://www.zhihu.com/equation?tex=c" alt="c" eeimg="1"> 的时候只需要把 <img src="https://www.zhihu.com/equation?tex=f" alt="f" eeimg="1"> 的梯度 <img src="https://www.zhihu.com/equation?tex=g" alt="g" eeimg="1"> 传入grad_fn函数即可，然后再由这个函数计算局部梯度，将梯度传递到操作数那一层。但如前文提到的，矩阵的链式法则有自己的规律，我们需要推导一下。这里的推导过程参考了一篇我十分佩服的文章<a href="https://zhuanlan.zhihu.com/p/24709748" class="internal" target="_blank">长躯鬼侠：矩阵求导术</a>。我们先从多元标量函数入手分析，根据标量的全微分公式、微分与梯度的关系，有：</p><p data-pid="YRq0HtDI"><img src="https://www.zhihu.com/equation?tex=df%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+x_i%7Ddx_i%3D%28%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+%5Cmathbf%7Bx%7D%7D%29%5E%5Cmathrm%7BT%7Dd%5Cmathbf%7Bx%7D%5C%5C" alt="df=\sum_{i=1}^{n}\frac{\partial f}{\partial x_i}dx_i=(\frac{\partial f}{\partial \mathbf{x}})^\mathrm{T}d\mathbf{x}\\" eeimg="1">那么类比标量，多元矩阵函数也应有如下关系：</p><p data-pid="6ZEihHmD"><img src="https://www.zhihu.com/equation?tex=df%3D%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%5Csum_%7Bj%3D1%7D%5E%7Bn%7D%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+X_%7Bij%7D%7DdX_%7Bij%7D%3Dtr%5Cleft%28%28%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+%5Cmathbf%7BX%7D%7D%29%5E%5Cmathrm%7BT%7Dd%5Cmathbf%7BX%7D%5Cright%29%5C%5C" alt="df=\sum_{i=1}^{m}\sum_{j=1}^{n}\frac{\partial f}{\partial X_{ij}}dX_{ij}=tr\left((\frac{\partial f}{\partial \mathbf{X}})^\mathrm{T}d\mathbf{X}\right)\\" eeimg="1">其中tr代表矩阵的迹，即对角线元素之和。上式很好验证，我们假设 <img src="https://www.zhihu.com/equation?tex=m%3Dn%3D2" alt="m=n=2" eeimg="1"> ，将上式右侧展开，很明显只有对角线元素的梯度和微分是对应的。</p><p data-pid="Z9KdwVPk"><img src="https://www.zhihu.com/equation?tex=%5Cleft%28%5Cbegin%7Barray%7D%7Bl%7D+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+X_%7B11%7D%7D+%26+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+X_%7B21%7D%7D%5C%5C+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+X_%7B12%7D%7D+%26+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+X_%7B22%7D%7D%5C%5C+%5Cend%7Barray%7D%5Cright%29%5Cleft%28%5Cbegin%7Barray%7D%7Bl%7D+dX_%7B11%7D+%26+dX_%7B12%7D%5C%5C+dX_%7B21%7D+%26+dX_%7B22%7D%5C%5C+%5Cend%7Barray%7D%5Cright%29%3D%5Cleft%28%5Cbegin%7Barray%7D%7Bl%7D+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+X_%7B11%7D%7DdX_%7B11%7D%2B+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+X_%7B21%7D%7DdX_%7B21%7D%26%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+X_%7B11%7D%7DdX_%7B12%7D%2B+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+X_%7B21%7D%7DdX_%7B22%7D%5C%5C+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+X_%7B12%7D%7DdX_%7B11%7D%2B+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+X_%7B22%7D%7DdX_%7B21%7D+%26+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+X_%7B12%7D%7DdX_%7B12%7D%2B+%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+X_%7B22%7D%7DdX_%7B22%7D%5C%5C+%5Cend%7Barray%7D%5Cright%29%5C%5C" alt="\left(\begin{array}{l} \frac{\partial f}{\partial X_{11}} &amp; \frac{\partial f}{\partial X_{21}}\\ \frac{\partial f}{\partial X_{12}} &amp; \frac{\partial f}{\partial X_{22}}\\ \end{array}\right)\left(\begin{array}{l} dX_{11} &amp; dX_{12}\\ dX_{21} &amp; dX_{22}\\ \end{array}\right)=\left(\begin{array}{l} \frac{\partial f}{\partial X_{11}}dX_{11}+ \frac{\partial f}{\partial X_{21}}dX_{21}&amp;\frac{\partial f}{\partial X_{11}}dX_{12}+ \frac{\partial f}{\partial X_{21}}dX_{22}\\ \frac{\partial f}{\partial X_{12}}dX_{11}+ \frac{\partial f}{\partial X_{22}}dX_{21} &amp; \frac{\partial f}{\partial X_{12}}dX_{12}+ \frac{\partial f}{\partial X_{22}}dX_{22}\\ \end{array}\right)\\" eeimg="1">接下来，我们假设如下情形：</p><p data-pid="Jx1EWSVi"><img src="https://www.zhihu.com/equation?tex=Y%3DXW%2C%5C+L%3Df%28Y%29%5C%5C" alt="Y=XW,\ L=f(Y)\\" eeimg="1">其中 <img src="https://www.zhihu.com/equation?tex=X" alt="X" eeimg="1"> 是 <img src="https://www.zhihu.com/equation?tex=n%5Ctimes+p" alt="n\times p" eeimg="1"> 的矩阵， <img src="https://www.zhihu.com/equation?tex=W" alt="W" eeimg="1"> 是 <img src="https://www.zhihu.com/equation?tex=p%5Ctimes+m" alt="p\times m" eeimg="1"> 的矩阵， <img src="https://www.zhihu.com/equation?tex=Y" alt="Y" eeimg="1"> 是 <img src="https://www.zhihu.com/equation?tex=n+%5Ctimes+m" alt="n \times m" eeimg="1"> 的矩阵， <img src="https://www.zhihu.com/equation?tex=L" alt="L" eeimg="1"> 是标量。可以理解为 <img src="https://www.zhihu.com/equation?tex=X" alt="X" eeimg="1"> 是 <img src="https://www.zhihu.com/equation?tex=n" alt="n" eeimg="1"> 个输入样本，每个样本有 <img src="https://www.zhihu.com/equation?tex=p" alt="p" eeimg="1"> 维特征，经过线性变换矩阵 <img src="https://www.zhihu.com/equation?tex=W" alt="W" eeimg="1"> 变换成高维特征图 <img src="https://www.zhihu.com/equation?tex=Y" alt="Y" eeimg="1"> ，然后通过 <img src="https://www.zhihu.com/equation?tex=f" alt="f" eeimg="1"> （非线性变换以及损失函数等）计算得到最终的 <img src="https://www.zhihu.com/equation?tex=L" alt="L" eeimg="1"> 。下面计算 <img src="https://www.zhihu.com/equation?tex=L" alt="L" eeimg="1"> 对于 <img src="https://www.zhihu.com/equation?tex=X" alt="X" eeimg="1"> 的梯度，首先求 <img src="https://www.zhihu.com/equation?tex=L" alt="L" eeimg="1"> 的全微分：</p><p data-pid="XsSRg1fH"><img src="https://www.zhihu.com/equation?tex=dL%3Dtr%5Cleft%28%5Cleft%28%5Cfrac%7B%5Cpartial+L%7D%7B%5Cpartial+Y%7D%5Cright%29%5E%5Cmathrm%7BT%7DdY%5Cright%29%5C%5C" alt="dL=tr\left(\left(\frac{\partial L}{\partial Y}\right)^\mathrm{T}dY\right)\\" eeimg="1"></p><p data-pid="bv1pA0o_">考虑对 <img src="https://www.zhihu.com/equation?tex=X" alt="X" eeimg="1"> 求梯度时， <img src="https://www.zhihu.com/equation?tex=W" alt="W" eeimg="1"> 可被看做常数，那么：</p><p data-pid="ifv_xWMh"><img src="https://www.zhihu.com/equation?tex=dY%3D%28dX%29W%2BX%28dW%29%3D%28dX%29W%5C%5C" alt="dY=(dX)W+X(dW)=(dX)W\\" eeimg="1">代入到全微分公式中，得到：</p><p data-pid="QJzPU11f"><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+dL%26%3Dtr%5Cleft%28%5Cleft%28%5Cfrac%7B%5Cpartial+L%7D%7B%5Cpartial+Y%7D%5Cright%29%5E%5Cmathrm%7BT%7D%28dX%29W%5Cright%29+%3Dtr%5Cleft%28W%5Cleft%28%5Cfrac%7B%5Cpartial+L%7D%7B%5Cpartial+Y%7D%5Cright%29%5E%5Cmathrm%7BT%7DdX%5Cright%29%5C%5C+%26%3Dtr%5Cleft%28%5Cleft%28%5Cfrac%7B%5Cpartial+L%7D%7B%5Cpartial+Y%7DW%5E%5Cmathrm%7BT%7D%5Cright%29%5E%5Cmathrm%7BT%7DdX%5Cright%29+%3Dtr%5Cleft%28%5Cleft%28%5Cfrac%7B%5Cpartial+L%7D%7B%5Cpartial+X%7D%5Cright%29%5E%5Cmathrm%7BT%7DdX%5Cright%29+%5Cend%7Baligned%7D%5C%5C" alt="\begin{aligned} dL&amp;=tr\left(\left(\frac{\partial L}{\partial Y}\right)^\mathrm{T}(dX)W\right) =tr\left(W\left(\frac{\partial L}{\partial Y}\right)^\mathrm{T}dX\right)\\ &amp;=tr\left(\left(\frac{\partial L}{\partial Y}W^\mathrm{T}\right)^\mathrm{T}dX\right) =tr\left(\left(\frac{\partial L}{\partial X}\right)^\mathrm{T}dX\right) \end{aligned}\\" eeimg="1">所以我们推导出 <img src="https://www.zhihu.com/equation?tex=L" alt="L" eeimg="1"> 对 <img src="https://www.zhihu.com/equation?tex=X" alt="X" eeimg="1"> 求梯度的链式法则：</p><p data-pid="SCtprIbu"><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L%7D%7B%5Cpartial+X%7D%3D%5Cfrac%7B%5Cpartial+L%7D%7B%5Cpartial+Y%7DW%5E%5Cmathrm+T%5C%5C" alt="\frac{\partial L}{\partial X}=\frac{\partial L}{\partial Y}W^\mathrm T\\" eeimg="1">同理，我们也能推导出 <img src="https://www.zhihu.com/equation?tex=L" alt="L" eeimg="1"> 对 <img src="https://www.zhihu.com/equation?tex=W" alt="W" eeimg="1"> 求梯度的链式法则：</p><p data-pid="tepm4Woh"><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L%7D%7B%5Cpartial+W%7D%3DX%5E%5Cmathrm%7BT%7D%5Cfrac%7B%5Cpartial+L%7D%7B%5Cpartial+Y%7D%5C%5C" alt="\frac{\partial L}{\partial W}=X^\mathrm{T}\frac{\partial L}{\partial Y}\\" eeimg="1">于是，我们可以将上述代码中的grad_fn补全：</p><div class="highlight"><pre><code class="language-python"><span></span><span class="k">def</span> <span class="nf">grad_fn_1</span><span class="p">(</span><span class="n">grad</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np_matmul</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">other</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">grad_fn_2</span><span class="p">(</span><span class="n">grad</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np_matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
</code></pre></div><p data-pid="PW0_0XrG">为了编程方便，我们将算子设计的代码中通用的部分封装起来：</p><div class="highlight"><pre><code class="language-python"><span></span><span class="k">def</span> <span class="nf">build_binary_ops</span><span class="p">(</span><span class="n">this</span><span class="p">,</span> <span class="n">that</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">grad_fn_1</span><span class="p">,</span> <span class="n">grad_fn_2</span><span class="p">):</span>
    <span class="n">requires_grad</span> <span class="o">=</span> <span class="n">this</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">or</span> <span class="n">that</span><span class="o">.</span><span class="n">requires_grad</span>
    <span class="n">dependency</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">this</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
        <span class="n">dependency</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">this</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=</span><span class="n">grad_fn_1</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">that</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
        <span class="n">dependency</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">that</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=</span><span class="n">grad_fn_2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">this</span><span class="o">.</span><span class="vm">__class__</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">requires_grad</span><span class="p">,</span> <span class="n">dependency</span><span class="p">)</span>
</code></pre></div><h3>5.2 求平均值算子</h3><p data-pid="PcHIzh3d">在上述矩阵乘法的例子中 <img src="https://www.zhihu.com/equation?tex=f" alt="f" eeimg="1"> 将最终的输出从矩阵映射成为标量，这个过程中往往先使用损失函数将矩阵变为向量，例如交叉熵损失函数：</p><p data-pid="iK1RiSxF"><img src="https://www.zhihu.com/equation?tex=H%28Y%2C%5Chat+Y%29%3D-%5Chat+Y%5Codot%5Clog+Y%5C%5C" alt="H(Y,\hat Y)=-\hat Y\odot\log Y\\" eeimg="1">之后我们需要再对一批样本的损失值向量做维度压缩，比如使用求平均值算子：</p><p data-pid="q2i7_alb"><img src="https://www.zhihu.com/equation?tex=L%3DH.%5Ctext%7Bmean%7D%28%29%3D%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5E%7Bn%7DH_i%5C%5C" alt="L=H.\text{mean}()=\frac{1}{n}\sum_{i=1}^{n}H_i\\" eeimg="1">由于上文提到输出节点必须为标量，所以这一类函数也会非常常用。</p><div class="highlight"><pre><code class="language-python"><span></span><span class="k">class</span> <span class="nc">Tensor</span><span class="p">:</span>
    <span class="c1"># ...</span>
    <span class="k">def</span> <span class="nf">reduce_mean</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">grad_fn</span><span class="p">(</span><span class="n">grad</span><span class="p">):</span>
            <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">size</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">grad</span>
        <span class="k">return</span> <span class="n">build_unary_ops</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">grad_fn</span><span class="p">)</span>
</code></pre></div><p data-pid="KPqqwlro">首先也是实现正向传播的功能，直接调用Numpy数组自带的平均值函数mean，得到输出。由于输出是一个标量，意味着上一层传来的grad也是个标量，那么这里反向传播的梯度计算和传递很简单，求和后的输出对于原始向量的每个元素的偏导数都为1/n，所以只需要新建一个与grad相同维度的数组，然后再通过数乘进行broadcast即可。</p><p data-pid="nZs04BIu">但是问题不是这么简单，像求和、求均值这类运算往往提供了一个额外的参数：运算的轴。当操作的张量高于一维，我们就需要沿着轴去分配偏导的值。直接讲分配不好理解，举个例子：</p><p data-pid="71DwNuhJ"><img src="https://www.zhihu.com/equation?tex=y%3Daw%5E%5Cmathrm+T%3Da%281%2C2%29%5E%5Cmathrm+T%2C+%5C+a%3D%5Cmathbf+X.%5Ctext%7Bmean%7D%281%29%5C%5C" alt="y=aw^\mathrm T=a(1,2)^\mathrm T, \ a=\mathbf X.\text{mean}(1)\\" eeimg="1">其中，矩阵 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf+X" alt="\mathbf X" eeimg="1"> 沿着axis=1求均值的操作可以写作：</p><p data-pid="d43J7FG-"><img src="https://www.zhihu.com/equation?tex=%5Cmathbf+X.%5Ctext%7Bmean%281%29%7D%3D%5Cfrac%7B1%7D%7B2%7D%28X_%7B11%7D%2BX_%7B12%7D%2C+X_%7B21%7D%2BX_%7B22%7D%29%5C%5C" alt="\mathbf X.\text{mean(1)}=\frac{1}{2}(X_{11}+X_{12}, X_{21}+X_{22})\\" eeimg="1">而输出 <img src="https://www.zhihu.com/equation?tex=y" alt="y" eeimg="1"> 对于 <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"> 求导是 <img src="https://www.zhihu.com/equation?tex=%5Cpartial+y%2F%5Cpartial+a%3Dw" alt="\partial y/\partial a=w" eeimg="1"> 。显然 <img src="https://www.zhihu.com/equation?tex=y" alt="y" eeimg="1"> 对矩阵 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf+X" alt="\mathbf X" eeimg="1"> 求梯度最终的结果应为：</p><p data-pid="0Khxm4_j"><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+y%7D%7B%5Cpartial+%5Cmathbf+X%7D%3D%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Cmathbf+X%7D%28X_%7B11%7D%2BX_%7B12%7D%2C2%28X_%7B21%7D%2BX_%7B22%7D%29%29%2F2%3D%5Cfrac%7B1%7D%7B2%7D%5Cleft%28%5Cbegin%7Barray%7D%7Bl%7D+1+%26+1%5C%5C+2+%26+2%5C%5C+%5Cend%7Barray%7D%5Cright%29%5C%5C" alt="\frac{\partial y}{\partial \mathbf X}=\frac{\partial}{\partial \mathbf X}(X_{11}+X_{12},2(X_{21}+X_{22}))/2=\frac{1}{2}\left(\begin{array}{l} 1 &amp; 1\\ 2 &amp; 2\\ \end{array}\right)\\" eeimg="1"></p><p data-pid="aScX3Swm">其实均值算子这里的梯度求解和传递就是：将上层传来的梯度 <img src="https://www.zhihu.com/equation?tex=w" alt="w" eeimg="1"> ，先沿着均值的轴扩展一个维度，然后再沿着这个轴进行重复，最终除以该轴的元素数量。在求均值时，比如一个2x3x4的矩阵，沿着1轴求均值，结果的形状就变成2x4，也就是沿哪轴求值，哪轴就被压缩掉。所以在反向传播时反其道，先使用Numpy的expand_dims将这个轴扩充。扩充后这个轴的长度只有1，那么要扩充为原来的形状，就需要重复，重复多少次呢，当然重复原来这个轴上的元素数量那么多次。</p><p data-pid="NmNzQNPB">有人可能疑惑，这里不也是向量对矩阵求梯度吗，为什么没出现高维矩阵？那是因为沿轴操作很特殊，是介于逐元素操作和矩阵操作之间的，不能按照传统矩阵乘法那类操作来类比。最后我们补全代码：</p><div class="highlight"><pre><code class="language-python"><span></span><span class="k">class</span> <span class="nc">Tensor</span><span class="p">:</span>
    <span class="c1"># ...</span>
    <span class="k">def</span> <span class="nf">reduce_mean</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">repeat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span>

        <span class="k">def</span> <span class="nf">grad_fn</span><span class="p">(</span><span class="n">grad</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">size</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">grad</span> <span class="o">/</span> <span class="n">repeat</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
                <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">repeat</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">grad</span>

        <span class="k">return</span> <span class="n">build_unary_ops</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">grad_fn</span><span class="p">)</span>
</code></pre></div><h3>5.3 broadcast算子</h3><p data-pid="KRfhMtnM">举一个最常见的例子，就是神经网络线性层中的偏置值。我们假设输入为 <img src="https://www.zhihu.com/equation?tex=X" alt="X" eeimg="1"> ，维度为 <img src="https://www.zhihu.com/equation?tex=n%5Ctimes+p" alt="n\times p" eeimg="1"> ，即 <img src="https://www.zhihu.com/equation?tex=n" alt="n" eeimg="1"> 个样本，每个样本 <img src="https://www.zhihu.com/equation?tex=p" alt="p" eeimg="1"> 维特征。设权重矩阵为 <img src="https://www.zhihu.com/equation?tex=W" alt="W" eeimg="1"> ，为 <img src="https://www.zhihu.com/equation?tex=p%5Ctimes+1" alt="p\times 1" eeimg="1"> 维，表示每个特征的重要性。设偏置值为 <img src="https://www.zhihu.com/equation?tex=b" alt="b" eeimg="1"> ，激活函数为 <img src="https://www.zhihu.com/equation?tex=%5Csigma" alt="\sigma" eeimg="1"> 。那么线性层的前向传播过程应为：</p><p data-pid="jrBsTb0K"><img src="https://www.zhihu.com/equation?tex=y%3D%5Csigma%28XW%2Bb%29%5C%5C" alt="y=\sigma(XW+b)\\" eeimg="1">这里 <img src="https://www.zhihu.com/equation?tex=XW" alt="XW" eeimg="1"> 的结果为 <img src="https://www.zhihu.com/equation?tex=n" alt="n" eeimg="1"> 维，如果不支持算子广播，那么就要求 <img src="https://www.zhihu.com/equation?tex=b" alt="b" eeimg="1"> 也必须是 <img src="https://www.zhihu.com/equation?tex=n" alt="n" eeimg="1"> 维。然而 <img src="https://www.zhihu.com/equation?tex=n" alt="n" eeimg="1"> 在神经网络训练时往往等同于batchsize，其大小是用户设置的，况且对于偏置值，创建 <img src="https://www.zhihu.com/equation?tex=n" alt="n" eeimg="1"> 倍的空间存储相同的值也是低效的。出于对空间效率和开发便捷性的考虑，我们就引入了广播机制。仅创建形状为(1,)的偏置张量，在相加时让这种加法操作广播到 <img src="https://www.zhihu.com/equation?tex=XW" alt="XW" eeimg="1"> 结果的每一个元素上。</p><p data-pid="-VIZe3Sc">上例中引入的支持广播的加法就是典型的broadcast算子（此外还包括数乘），概括地定义一下，就是把算子间形状相合的部分进行计算，形状不足的部分进行广播，从而降低对操作数形状上的要求，使用起来更加便捷。直接定义很难想象，现在的矩阵运算库基本都支持了算子的广播机制，我们以Numpy数组的加法为例：</p><div class="highlight"><pre><code class="language-python"><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="o">--------------------------------</span>
<span class="n">array</span><span class="p">([[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>  <span class="c1"># 3x4</span>
        <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]],</span>

       <span class="p">[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>  <span class="c1"># 3x4</span>
        <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]]])</span>
</code></pre></div><p data-pid="6IQEiCd9">先初始化一个三维全零数组，为了展示运算作用的维度，我们在各个维度选择不同的元素数量。我们分别用形状为(1,)、(4,)、(3, 1)、(3, 4)、(2, 3, 4)、(1, 3, 1)、(2,)的随机数向量与x相加，为了控制结果的可复现性，我们使用固定随机数种子的RandomState产生向量。</p><div class="highlight"><pre><code class="language-python"><span></span><span class="c1"># (1,)向量或者标量与x相加，将加在每一个元素上</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">+</span>  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,)</span>
<span class="n">array</span><span class="p">([[[</span><span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">]],</span>

       <span class="p">[[</span><span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">]]])</span>

<span class="c1"># (4,)向量x相加，将x的2x3个形状为(4,)的子数组与该向量对应位置元素相加</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">+</span>  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,)</span>
<span class="n">array</span><span class="p">([[[</span><span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.72</span><span class="p">,</span> <span class="mf">0.6</span> <span class="p">,</span> <span class="mf">0.54</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.72</span><span class="p">,</span> <span class="mf">0.6</span> <span class="p">,</span> <span class="mf">0.54</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.72</span><span class="p">,</span> <span class="mf">0.6</span> <span class="p">,</span> <span class="mf">0.54</span><span class="p">]],</span>

       <span class="p">[[</span><span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.72</span><span class="p">,</span> <span class="mf">0.6</span> <span class="p">,</span> <span class="mf">0.54</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.72</span><span class="p">,</span> <span class="mf">0.6</span> <span class="p">,</span> <span class="mf">0.54</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.72</span><span class="p">,</span> <span class="mf">0.6</span> <span class="p">,</span> <span class="mf">0.54</span><span class="p">]]])</span>

<span class="c1"># (3, 1)向量与x相加，沿着1轴对应位置元素相加，沿其他轴重复</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">+</span>  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">array</span><span class="p">([[[</span><span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.72</span><span class="p">,</span> <span class="mf">0.72</span><span class="p">,</span> <span class="mf">0.72</span><span class="p">,</span> <span class="mf">0.72</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.6</span> <span class="p">,</span> <span class="mf">0.6</span> <span class="p">,</span> <span class="mf">0.6</span> <span class="p">,</span> <span class="mf">0.6</span> <span class="p">]],</span>

       <span class="p">[[</span><span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.72</span><span class="p">,</span> <span class="mf">0.72</span><span class="p">,</span> <span class="mf">0.72</span><span class="p">,</span> <span class="mf">0.72</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.6</span> <span class="p">,</span> <span class="mf">0.6</span> <span class="p">,</span> <span class="mf">0.6</span> <span class="p">,</span> <span class="mf">0.6</span> <span class="p">]]])</span>

<span class="c1"># (3, 4)向量与x相加，将x的每一组3x4的子数组与该向量对应元素相加</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">+</span>  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">array</span><span class="p">([[[</span><span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.72</span><span class="p">,</span> <span class="mf">0.6</span> <span class="p">,</span> <span class="mf">0.54</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.42</span><span class="p">,</span> <span class="mf">0.65</span><span class="p">,</span> <span class="mf">0.44</span><span class="p">,</span> <span class="mf">0.89</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.96</span><span class="p">,</span> <span class="mf">0.38</span><span class="p">,</span> <span class="mf">0.79</span><span class="p">,</span> <span class="mf">0.53</span><span class="p">]],</span>

       <span class="p">[[</span><span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.72</span><span class="p">,</span> <span class="mf">0.6</span> <span class="p">,</span> <span class="mf">0.54</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.42</span><span class="p">,</span> <span class="mf">0.65</span><span class="p">,</span> <span class="mf">0.44</span><span class="p">,</span> <span class="mf">0.89</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.96</span><span class="p">,</span> <span class="mf">0.38</span><span class="p">,</span> <span class="mf">0.79</span><span class="p">,</span> <span class="mf">0.53</span><span class="p">]]])</span>

<span class="c1"># (2, 3, 4)向量与x相加，对应位置元素相加</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">+</span>  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">array</span><span class="p">([[[</span><span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.72</span><span class="p">,</span> <span class="mf">0.6</span> <span class="p">,</span> <span class="mf">0.54</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.42</span><span class="p">,</span> <span class="mf">0.65</span><span class="p">,</span> <span class="mf">0.44</span><span class="p">,</span> <span class="mf">0.89</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.96</span><span class="p">,</span> <span class="mf">0.38</span><span class="p">,</span> <span class="mf">0.79</span><span class="p">,</span> <span class="mf">0.53</span><span class="p">]],</span>

       <span class="p">[[</span><span class="mf">0.57</span><span class="p">,</span> <span class="mf">0.93</span><span class="p">,</span> <span class="mf">0.07</span><span class="p">,</span> <span class="mf">0.09</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.83</span><span class="p">,</span> <span class="mf">0.78</span><span class="p">,</span> <span class="mf">0.87</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.98</span><span class="p">,</span> <span class="mf">0.8</span> <span class="p">,</span> <span class="mf">0.46</span><span class="p">,</span> <span class="mf">0.78</span><span class="p">]]])</span>

<span class="c1"># (2,)向量与x相加，将报维度不匹配的错误</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">+</span>  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,)</span>
<span class="n">Traceback</span> <span class="p">(</span><span class="n">most</span> <span class="n">recent</span> <span class="n">call</span> <span class="n">last</span><span class="p">):</span>
  <span class="n">File</span> <span class="s2">"&lt;stdin&gt;"</span><span class="p">,</span> <span class="n">line</span> <span class="mi">1</span><span class="p">,</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="ne">ValueError</span><span class="p">:</span> <span class="n">operands</span> <span class="n">could</span> <span class="ow">not</span> <span class="n">be</span> <span class="n">broadcast</span> <span class="n">together</span> <span class="k">with</span> <span class="n">shapes</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span> <span class="p">(</span><span class="mi">2</span><span class="p">,)</span>
</code></pre></div><p data-pid="K5CXoZsa">由此我们可以总结，对于加法这类broadcast算子，并不像矩阵乘法要求乘数的形状完全一致，只要求至少有一个轴上的形状一致，其他轴为空或者为1，即可进行运算。</p><p data-pid="vAca6PYb">以x与形状为(2, 3, 1)的向量y的加法为例，由于x的形状为(2, 3, 4)，两个操作数0轴和1轴形状一致，而y缺少2轴（2轴形状为1，常常被认为是冗余的，可以squeeze掉的维度），那么操作的结果就是0轴和1轴形成的子矩阵的元素对应位置相加。你可以把这个子矩阵的每个元素看成是一个向量，x、y的子矩阵的元素分别是长度为4和1的向量，那么在对应位置元素相加时，又递归地发生了形状分别为(1,)和(4,)的向量的加法broadcast。当然，你也可以只取2轴的第一个切片，认为加法操作是作用在0轴和1轴形成的子矩阵上（这时每个元素就是一个数值），但这种操作沿着2轴进行重复，总共重复了4次（2轴的长度）。</p><p data-pid="mrlW09bR">接下来我们看看加法算子的逻辑具体怎么编写。前向传播直接使用加号就可以，因为我们张量使用的内部存储类型是Numpy的ndarray，天然支持广播。</p><div class="highlight"><pre><code class="language-python"><span></span><span class="k">def</span> <span class="fm">__add__</span><span class="p">(</span><span class="n">ts1</span><span class="p">,</span> <span class="n">ts2</span><span class="p">):</span>
    <span class="n">ts2</span> <span class="o">=</span> <span class="n">as_tensor</span><span class="p">(</span><span class="n">ts2</span><span class="p">)</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">ts1</span><span class="o">.</span><span class="n">values</span> <span class="o">+</span> <span class="n">ts2</span><span class="o">.</span><span class="n">values</span>
    <span class="c1"># ...</span>
</code></pre></div><p data-pid="A4zOm18D">重点是反向传播的逻辑，即grad_fn函数如何编写。在编写之前，我们要确定这种加法广播的结果如何对操作数进行求导。首先，将操作数全部转换为传统加法所要求的形状一致的情形。假如加数x、y的形状分别为(2, 3, 4)和(3, 4)，则将y的形状先扩充为(2, 3, 4)，扩充的方法即按照形状不匹配的轴（0轴）重复2次（0轴的长度），那么在相加时就相当于y广播到了其他的子矩阵上。这里「重复」的定义与Numpy的expand_dims、repeat函数的作用相同，前文在介绍平均值算子的时候提到过：</p><div class="highlight"><pre><code class="language-python"><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">y</span>
<span class="n">array</span><span class="p">([[</span><span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.72</span><span class="p">,</span> <span class="mf">0.6</span> <span class="p">,</span> <span class="mf">0.54</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">0.42</span><span class="p">,</span> <span class="mf">0.65</span><span class="p">,</span> <span class="mf">0.44</span><span class="p">,</span> <span class="mf">0.89</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">0.96</span><span class="p">,</span> <span class="mf">0.38</span><span class="p">,</span> <span class="mf">0.79</span><span class="p">,</span> <span class="mf">0.53</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span>
<span class="n">array</span><span class="p">([[[</span><span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.72</span><span class="p">,</span> <span class="mf">0.6</span> <span class="p">,</span> <span class="mf">0.54</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.42</span><span class="p">,</span> <span class="mf">0.65</span><span class="p">,</span> <span class="mf">0.44</span><span class="p">,</span> <span class="mf">0.89</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.96</span><span class="p">,</span> <span class="mf">0.38</span><span class="p">,</span> <span class="mf">0.79</span><span class="p">,</span> <span class="mf">0.53</span><span class="p">]]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span>
<span class="n">array</span><span class="p">([[[</span><span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.72</span><span class="p">,</span> <span class="mf">0.6</span> <span class="p">,</span> <span class="mf">0.54</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.42</span><span class="p">,</span> <span class="mf">0.65</span><span class="p">,</span> <span class="mf">0.44</span><span class="p">,</span> <span class="mf">0.89</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.96</span><span class="p">,</span> <span class="mf">0.38</span><span class="p">,</span> <span class="mf">0.79</span><span class="p">,</span> <span class="mf">0.53</span><span class="p">]],</span>

       <span class="p">[[</span><span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.72</span><span class="p">,</span> <span class="mf">0.6</span> <span class="p">,</span> <span class="mf">0.54</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.42</span><span class="p">,</span> <span class="mf">0.65</span><span class="p">,</span> <span class="mf">0.44</span><span class="p">,</span> <span class="mf">0.89</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.96</span><span class="p">,</span> <span class="mf">0.38</span><span class="p">,</span> <span class="mf">0.79</span><span class="p">,</span> <span class="mf">0.53</span><span class="p">]]])</span>
</code></pre></div><p data-pid="sfADmxhE">其实这种相似性很微妙，即加法的正向传播是维度填充，和sum()的反向传播相似；那么sum()的正向传播是维度缩减，就应该和加法的反向传播相似。可以想象，在求导后对应加数的局部导数必为形状与加数相同的全1矩阵，而加数扩充为(2, 3, 4)是我们假设的，其真实形状为(3, 4)，那么每个元素的权重就应该变为2倍。实际上我们反向传播时操作的是上一节点传来的全局导数，它在各个轴上的元素可能都不相等，这里就不是简单的乘法了，而是沿形状不匹配的轴（0轴）进行sum reduce。</p><p data-pid="obj2Eyoj">那么我们总结，grad_fn中就是对缺轴的操作数求导时，要将上层传来的全局导数沿着形状不匹配的轴进行求和。</p><div class="highlight"><pre><code class="language-python"><span></span><span class="k">def</span> <span class="nf">grad_fn_ts1</span><span class="p">(</span><span class="n">grad</span><span class="p">):</span>
    <span class="c1"># handle broadcasting (5, 3) + (3,) -&gt; (5, 3)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="n">ts1</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">ndim</span><span class="p">):</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="c1"># handle broadcasting (5, 3) + (1, 3) -&gt; (5, 3)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">dim</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ts1</span><span class="o">.</span><span class="n">shape</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">grad</span>
</code></pre></div><p data-pid="xLR0QEPl">注意，对于轴的长度为1的情况，我们要做sum，但不要reduce，使用keepdims=True来保持该轴不被squeeze掉。对右加数ts2的反向传播完全类似，数乘与加法完全类似，大家可以举一反三，也可以参考文末给出的代码。</p><h3>5.4 用户自定义算子</h3><p data-pid="O4QLiXR-">为了良好的扩展性，很多神经网络计算框架，例如TensorFlow，都支持用户自定义算子。我们的框架想要扩展也很简单，只需要实现以下函数原型即可：</p><div class="highlight"><pre><code class="language-python"><span></span><span class="c1"># 一元运算</span>
<span class="k">def</span> <span class="nf">unary_operation</span><span class="p">(</span><span class="n">operand</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    
    <span class="c1"># forward</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">unary_operation_forward</span><span class="p">(</span><span class="n">operand</span><span class="p">)</span>

    <span class="c1"># backward</span>
    <span class="k">def</span> <span class="nf">grad_fn</span><span class="p">(</span><span class="n">grad</span><span class="p">):</span>
        <span class="c1"># grad = ...</span>
        <span class="k">return</span> <span class="n">grad</span>

    <span class="k">return</span> <span class="n">build_unary_ops</span><span class="p">(</span><span class="n">operand</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">grad_fn</span><span class="p">)</span>

<span class="c1"># 二元运算（多元运算以此类推）</span>
<span class="k">def</span> <span class="nf">binary_operation</span><span class="p">(</span><span class="n">operand_1</span><span class="p">,</span> <span class="n">operand_2</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    
    <span class="c1"># forward</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">binary_operation_forward</span><span class="p">(</span><span class="n">operand_1</span><span class="p">,</span> <span class="n">operand_2</span><span class="p">)</span>

    <span class="c1"># backward</span>
    <span class="k">def</span> <span class="nf">grad_fn_1</span><span class="p">(</span><span class="n">grad</span><span class="p">):</span>
        <span class="c1"># grad = ...</span>
        <span class="k">return</span> <span class="n">grad</span>
    <span class="k">def</span> <span class="nf">grad_fn_2</span><span class="p">(</span><span class="n">grad</span><span class="p">):</span>
        <span class="c1"># grad = ...</span>
        <span class="k">return</span> <span class="n">grad</span>

    <span class="k">return</span> <span class="n">build_binary_ops</span><span class="p">(</span>
        <span class="n">operand_1</span><span class="p">,</span> <span class="n">operand_2</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">grad_fn_1</span><span class="p">,</span> <span class="n">grad_fn_2</span><span class="p">)</span>
</code></pre></div><h2>6 注意事项</h2><p data-pid="zwCgCl-D">在Numpy中有个很讨厌的机制：一维数组无法转置。我们的框架内部一直使用的是数组来存放values和grad，如果在梯度传递时出现了列向量与行向量做矩阵乘法的情况，本应得到矩阵，最终只会得到一个内积的标量。如果我们全部使用matrix来存储呢？则在很多情况下会出现多余的维度，需要不停的squeeze。最终我只好对@操作做了一层包装：</p><div class="highlight"><pre><code class="language-python"><span></span><span class="k">def</span> <span class="nf">np_matmul</span><span class="p">(</span><span class="n">arr1</span><span class="p">,</span> <span class="n">arr2</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">arr1</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">arr2</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">arr1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mat</span><span class="p">(</span><span class="n">arr1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
        <span class="n">arr2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mat</span><span class="p">(</span><span class="n">arr2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">arr1</span> <span class="o">@</span> <span class="n">arr2</span>
</code></pre></div><p data-pid="fRFSOlE1">这个问题是从 <a href="http://link.zhihu.com/?target=https%3A//borgwang.github.io/dl/2019/09/15/autograd.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">Automatic Differentiation Tutorial</a> 这篇文章给出的代码中发现的，大家有兴趣想复现的可以尝试一下。</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-4a4c07ce88848dd4fcdd7fb5a5c4f507_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="720" data-rawheight="779" class="origin_image zh-lightbox-thumb" width="720" data-original="https://pica.zhimg.com/v2-4a4c07ce88848dd4fcdd7fb5a5c4f507_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='720'%20height='779'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="720" data-rawheight="779" class="origin_image zh-lightbox-thumb lazy" width="720" data-original="https://pica.zhimg.com/v2-4a4c07ce88848dd4fcdd7fb5a5c4f507_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-4a4c07ce88848dd4fcdd7fb5a5c4f507_720w.jpg?source=d16d100b"></figure><h2>7 总结</h2><p data-pid="u9gsqLa4">本文首先介绍了深度学习中常用的自动求导机制的原理和实现方法：将矩阵封装成张量并重载运算符，在正向传播表达式定义的同时，将反向传播的梯度计算和传递函数注册在操作结果的dependency表中，然后从输出节点反向深度优先遍历计算树，最后将计算好的全局梯度存储在张量的grad中。本文虽长，但仍无法做到面面俱到。希望大家能有所收获，反正我在写这篇文章的时候收获颇多。欢迎来讨论~  </p><h2>附录：完整代码</h2><p data-pid="aM9QONbI">参见我fork的GitHub仓库：<a href="http://link.zhihu.com/?target=https%3A//github.com/ThomasAtlantis/toys/blob/thomas/ml-autograd/TensorLab.py" class=" wrap external" target="_blank" rel="nofollow noreferrer">toys/ml-autograd/TensorLab.py</a></p>