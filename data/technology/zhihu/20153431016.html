<blockquote data-pid="lShaPpiK">这篇文章来自 Genentech 资深机器学习科学家 Surag Nair 的博客，我在编译的过程中略有删改</blockquote><p data-pid="BhZOy-tj">这篇教程介绍了对 DeepMind 的论文 <a href="http://link.zhihu.com/?target=https%3A//deepmind.com/blog/alphago-zero-learning-scratch/" class=" wrap external" target="_blank" rel="nofollow noreferrer">AlphaGo Zero</a> 的丐版复现，它使用同步单线程、单GPU，并对棋类游戏普适。与前置工作 AlphaGo 使用专业棋手的棋局数据训练不同，AlphaGo Zero 除了游戏规则以外不借助任何人类知识，单纯地通过 self-play 训练围棋智能体，并最终轻松战胜了 AlphaGo。后来，DeepMind 也发布了后续工作 <a href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1712.01815" class=" wrap external" target="_blank" rel="nofollow noreferrer">Alpha Zero</a>，将 AlphaGo Zero 的方法推广到了国际象棋和将棋（Shogi，日本象棋）。</p><p data-pid="aZtqTirw">这篇文章的目标是提炼 AlphaGo Zero 论文中的关键思想，并通过代码来实际理解它们。因为 AlphaGo Zero 结合了神经网络和蒙特卡罗树搜索，在一个策略迭代框架中实现了稳定训练，我希望读者对机器学习和强化学习有基本的了解。如果你具备神经网络基础并熟悉蒙特卡罗树搜索，这篇教程应该不难理解。在本文之外，我仍然建议你阅读<a href="http://link.zhihu.com/?target=https%3A//www.nature.com/articles/nature24270.epdf%3Fauthor_access_token%3DVJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ" class=" wrap external" target="_blank" rel="nofollow noreferrer">原论文</a>，因为它写得很好，非常易读，还有漂亮的插图！</p><h2>神经网络</h2><p data-pid="ndwauFGy">毫不意外，算法的核心还是一个神经网络。 <img src="https://www.zhihu.com/equation?tex=f_%7B%5Ctheta%7D" alt="f_{\theta}" eeimg="1"> 是一个参数为 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="\theta" eeimg="1"> 的神经网络，以棋盘的状态 <img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1"> 作为输入。它有两个输出：从当前棋手视角出发的棋盘状态的价值 <img src="https://www.zhihu.com/equation?tex=v_%7B%5Ctheta%7D%28s%29%5Cin+%5B-1%2C1%5D" alt="v_{\theta}(s)\in [-1,1]" eeimg="1"> （连续实数），和一个策略 <img src="https://www.zhihu.com/equation?tex=%5Cvec%7Bp%7D_%7B%5Ctheta%7D%28s%29" alt="\vec{p}_{\theta}(s)" eeimg="1"> ，即在所有可能的动作上的概率向量。</p><p data-pid="cRwt93VK">在训练网络过程中，每次 self-play 的棋局结束，神经网络将获得 <img src="https://www.zhihu.com/equation?tex=%28s_t%2C%5Cvec+%5Cpi_t%2C+z_t%29" alt="(s_t,\vec \pi_t, z_t)" eeimg="1"> 形式的训练样本。 <img src="https://www.zhihu.com/equation?tex=%5Cvec%7B%5Cpi%7D_t" alt="\vec{\pi}_t" eeimg="1"> 是从状态 <img src="https://www.zhihu.com/equation?tex=s_t" alt="s_t" eeimg="1"> 中估计得到的策略（下一节将会讲解 <img src="https://www.zhihu.com/equation?tex=%5Cvec%5Cpi_t" alt="\vec\pi_t" eeimg="1"> 是怎么得到的）， <img src="https://www.zhihu.com/equation?tex=z_t%5Cin%5C%7B-1%2C1%5C%7D" alt="z_t\in\{-1,1\}" eeimg="1"> 是棋手视角下的最终棋局结果，+1 表示赢棋，-1 表示输棋。神经网络的训练过程要求最小化以下损失函数（除了正则项以外）： <img src="https://www.zhihu.com/equation?tex=l%3D%5Csum_t%28v_%5Ctheta%28s_t%29-z_t%29%5E2-%5Cvec+%5Cpi_t%5Ccdot+%5Clog%28%5Cvec+p_%5Ctheta%28s_t%29%29%5C%5C" alt="l=\sum_t(v_\theta(s_t)-z_t)^2-\vec \pi_t\cdot \log(\vec p_\theta(s_t))\\" eeimg="1">核心思想是，随着时间推移，网络将学会哪些状态最终会导致胜利或失败。此外，学好这个策略还能很好地估计在给定状态下最优的动作是什么。神经网络架构通常取决于具体的棋局种类，大多数棋类游戏（如围棋）可以使用多层卷积神经网络。在 DeepMind 的论文中，他们使用了 20 个残差模块，每个残差模块包含两个卷积层。而我在实现 6x6 版本的黑白棋（Othello）时，使用了一个包含4层卷积网络的架构，后接几层全连接网络，同样能够正常运行。</p><h2>策略学习中的蒙特卡洛树搜索</h2><p data-pid="0inHlHG0">给定一个状态 <img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1"> ，神经网络输出对策略 <img src="https://www.zhihu.com/equation?tex=%5Cvec+p_%5Ctheta" alt="\vec p_\theta" eeimg="1"> 的估计。在训练阶段，我们希望提升这些估计，这通过<a href="http://link.zhihu.com/?target=https%3A//web.archive.org/web/20180623055344/http%3A//mcts.ai/about/index.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">蒙特卡洛树搜索</a>（Monte Carlo Tree Search，MCTS）实现。在搜索树中，每个节点代表一个棋盘设置。如果一个合法的动作可以将状态从 <img src="https://www.zhihu.com/equation?tex=i" alt="i" eeimg="1"> 转换成 <img src="https://www.zhihu.com/equation?tex=j" alt="j" eeimg="1"> ，则这两个节点之间存在一条有向边 <img src="https://www.zhihu.com/equation?tex=i%5Crightarrow+j" alt="i\rightarrow j" eeimg="1"> 。从一个空的搜索树开始，我们每次扩展一个节点（状态），我们并不真的摆出这个盘面，而是通过神经网络获得。这个值会在搜索路径上传播，下面我们看一下细节。</p><p data-pid="9065HfZ9">对于树搜索，我们维护以下变量：</p><ul><li data-pid="OrRdpe9M"><img src="https://www.zhihu.com/equation?tex=Q%28s%2Ca%29" alt="Q(s,a)" eeimg="1"> ：在状态 <img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1"> 下采取动作 <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"> 期望获得的奖励，即 Q 值</li><li data-pid="_BOFzDi2"><img src="https://www.zhihu.com/equation?tex=N%28s%2Ca%29" alt="N(s,a)" eeimg="1"> ：在所有的模拟中，我们在状态 <img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1"> 下采取动作 <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"> 的频次</li><li data-pid="jOcAxNZX"><img src="https://www.zhihu.com/equation?tex=P%28s%2C%5Ccdot%29%3D%5Cvec+p_%5Ctheta%28s%29" alt="P(s,\cdot)=\vec p_\theta(s)" eeimg="1"> ：根据神经网络返回的策略得到的在状态 <img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1"> 下采取某个动作的初始估计</li></ul><p data-pid="H6TSGUzN">根据这些，我们可以计算 <img src="https://www.zhihu.com/equation?tex=U%28s%2Ca%29" alt="U(s,a)" eeimg="1"> ，Q 值的上置信界（Upper Confidence Bound，UCB）</p><p data-pid="Q1MeeHZE"><img src="https://www.zhihu.com/equation?tex=U%28s%2Ca%29%3DQ%28s%2Ca%29%2Bc_%7Bpuct%7D%5Ccdot+P%28s%2Ca%29%5Ccdot+%5Cfrac%7B%5Csqrt+%7B%5Csum_b+N%28s%2Cb%29%7D%7D%7B1%2BN%28s%2Ca%29%7D%5C%5C" alt="U(s,a)=Q(s,a)+c_{puct}\cdot P(s,a)\cdot \frac{\sqrt {\sum_b N(s,b)}}{1+N(s,a)}\\" eeimg="1">这里 <img src="https://www.zhihu.com/equation?tex=c_%7Bpuct%7D" alt="c_{puct}" eeimg="1"> 是一个超参数，用来控制探索的程度。为了使用 MCTS 优化当前神经网络返回的初始策略，我们初始化我们的空搜索树，使用 <img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1"> 作为根节点。然后我们做一次模拟：我们计算出使 <img src="https://www.zhihu.com/equation?tex=U%28s%2Ca%29" alt="U(s,a)" eeimg="1"> 最大化的动作 <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"> 。如果下一个状态 <img src="https://www.zhihu.com/equation?tex=s%27" alt="s'" eeimg="1"> （通过在状态 <img src="https://www.zhihu.com/equation?tex=s" alt="s" eeimg="1"> 上采取动作 <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"> ）在树中已经存在，就在 <img src="https://www.zhihu.com/equation?tex=s%27" alt="s'" eeimg="1"> 上递归地调用搜索过程。如果它不存在，就将其添加到树中，并根据神经网络的输出初始化 <img src="https://www.zhihu.com/equation?tex=P%28s%27%2C%5Ccdot%29%3D%5Cvec+p_%5Ctheta%28s%27%29" alt="P(s',\cdot)=\vec p_\theta(s')" eeimg="1"> 和 <img src="https://www.zhihu.com/equation?tex=v%28s%27%29%3Dv_%5Ctheta+%28s%27%29" alt="v(s')=v_\theta (s')" eeimg="1"> ，并对于所有的 <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"> ，将 <img src="https://www.zhihu.com/equation?tex=Q%28s%27%2Ca%29" alt="Q(s',a)" eeimg="1"> 和 <img src="https://www.zhihu.com/equation?tex=N%28s%27%2Ca%29" alt="N(s',a)" eeimg="1"> 都设置为0。我们接下来将 <img src="https://www.zhihu.com/equation?tex=v%28s%27%29" alt="v(s')" eeimg="1"> 沿着当前模拟中已经见过的路径进行传播。另一方面，如果遇到终止状态，我们将传入真实的奖励（+1 如果赢棋，-1 如果输棋）。</p><p data-pid="PmVkHiMq">经过多次模拟，根节点处的 <img src="https://www.zhihu.com/equation?tex=N%28s%2Ca%29" alt="N(s,a)" eeimg="1"> 值将对策略进行很好地近似。优化后的随机策略 <img src="https://www.zhihu.com/equation?tex=%5Cvec+%5Cpi%28s%29" alt="\vec \pi(s)" eeimg="1"> 简单地采用归一化的计数 <img src="https://www.zhihu.com/equation?tex=N%28s%2C%5Ccdot%29%2F%5Csum_b+N%28s%2Cb%29" alt="N(s,\cdot)/\sum_b N(s,b)" eeimg="1"> 。在 self-play 过程中，我们使用 MCTS，并从优化后的策略 <img src="https://www.zhihu.com/equation?tex=%5Cvec%5Cpi%28s%29" alt="\vec\pi(s)" eeimg="1"> 中采样一步棋。以下是搜索算法模拟过程的整体实现：</p><div class="highlight"><pre><code class="language-python"><span></span><span class="k">def</span> <span class="nf">search</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">game</span><span class="p">,</span> <span class="n">nnet</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">game</span><span class="o">.</span><span class="n">gameEnded</span><span class="p">(</span><span class="n">s</span><span class="p">):</span> <span class="k">return</span> <span class="o">-</span><span class="n">game</span><span class="o">.</span><span class="n">gameReward</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">s</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">visited</span><span class="p">:</span>
        <span class="n">visited</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">P</span><span class="p">[</span><span class="n">s</span><span class="p">],</span> <span class="n">v</span> <span class="o">=</span> <span class="n">nnet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">v</span>
  
    <span class="n">max_u</span><span class="p">,</span> <span class="n">best_a</span> <span class="o">=</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">"inf"</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span>
    <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">game</span><span class="o">.</span><span class="n">getValidActions</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">][</span><span class="n">a</span><span class="p">]</span> <span class="o">+</span> <span class="n">c_puct</span><span class="o">*</span><span class="n">P</span><span class="p">[</span><span class="n">s</span><span class="p">][</span><span class="n">a</span><span class="p">]</span><span class="o">*</span><span class="n">sqrt</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">N</span><span class="p">[</span><span class="n">s</span><span class="p">]))</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">N</span><span class="p">[</span><span class="n">s</span><span class="p">][</span><span class="n">a</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">u</span><span class="o">&gt;</span><span class="n">max_u</span><span class="p">:</span>
            <span class="n">max_u</span> <span class="o">=</span> <span class="n">u</span>
            <span class="n">best_a</span> <span class="o">=</span> <span class="n">a</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">best_a</span>
    
    <span class="n">sp</span> <span class="o">=</span> <span class="n">game</span><span class="o">.</span><span class="n">nextState</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">search</span><span class="p">(</span><span class="n">sp</span><span class="p">,</span> <span class="n">game</span><span class="p">,</span> <span class="n">nnet</span><span class="p">)</span>

    <span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">][</span><span class="n">a</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">N</span><span class="p">[</span><span class="n">s</span><span class="p">][</span><span class="n">a</span><span class="p">]</span><span class="o">*</span><span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">][</span><span class="n">a</span><span class="p">]</span> <span class="o">+</span> <span class="n">v</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">N</span><span class="p">[</span><span class="n">s</span><span class="p">][</span><span class="n">a</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">N</span><span class="p">[</span><span class="n">s</span><span class="p">][</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">v</span>
</code></pre></div><p data-pid="s39-dPgu">注意，我们返回的是该状态的负值。这是因为搜索树中的不同层交替代表不同棋手的视角。由于 <img src="https://www.zhihu.com/equation?tex=v%5Cin%5B-1%2C1%5D" alt="v\in[-1,1]" eeimg="1"> ，因此 <img src="https://www.zhihu.com/equation?tex=-v" alt="-v" eeimg="1"> 表示从对方棋手视角来看当前盘面的价值。</p><h2>通过 self-play 嵌入策略</h2><p data-pid="S8r-HNlR">信不信由你，我们现在已经拥有训练无监督棋类游戏智能体所需的全部要素了！通过 self-play 进行学习本质上是一种策略迭代算法——我们通过当前的策略（在本例中是神经网络）进行对局并计算 Q 值，然后利用计算得到的统计数据更新我们的策略。</p><p data-pid="-TFLT-gX">以下是完整的训练算法。我们用随机权重初始化神经网络，即以随机的策略网络和价值网络作为开始。在算法的每次迭代中，我们进行若干场 self-play。在每局棋的每一步棋中，我们从当前状态 <img src="https://www.zhihu.com/equation?tex=s_t" alt="s_t" eeimg="1"> 出发，执行固定次数的 MCTS 模拟。我们在改进后的策略 <img src="https://www.zhihu.com/equation?tex=%5Cvec+%5Cpi%28s%29" alt="\vec \pi(s)" eeimg="1"> 上采样得到下一步棋。这让我们得到一条训练样本 <img src="https://www.zhihu.com/equation?tex=%28s_t%2C%5Cvec+%5Cpi_t%2C%5C_%29" alt="(s_t,\vec \pi_t,\_)" eeimg="1"> 。奖励<code>_</code>将会在棋局结束时被填入，如果棋手胜利则填入+1，否则填入-1。搜索树在对弈过程中被保留。</p><p data-pid="Dtk2VYAa">在每次迭代结束时，神经网络会使用获得的训练样本进行训练。然后，用旧网络与新网络进行对战。如果新网络赢得的对局比例超过设定的阈值（DeepMind 论文中为 55%），则用新网络更新旧网络。否则，我们会进行新一轮迭代，以扩充训练样本。</p><p data-pid="6QHBsr3H">就是这样！有点神奇的是，网络几乎在每次迭代中都会有所改进，并学会更好地进行对弈。下面提供了完整训练算法的整体代码。</p><div class="highlight"><pre><code class="language-python"><span></span><span class="k">def</span> <span class="nf">policyIterSP</span><span class="p">(</span><span class="n">game</span><span class="p">):</span>
    <span class="n">nnet</span> <span class="o">=</span> <span class="n">initNNet</span><span class="p">()</span>                                       <span class="c1"># initialise random neural network</span>
    <span class="n">examples</span> <span class="o">=</span> <span class="p">[]</span>    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">numIters</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">numEps</span><span class="p">):</span>
            <span class="n">examples</span> <span class="o">+=</span> <span class="n">executeEpisode</span><span class="p">(</span><span class="n">game</span><span class="p">,</span> <span class="n">nnet</span><span class="p">)</span>          <span class="c1"># collect examples from this game</span>
        <span class="n">new_nnet</span> <span class="o">=</span> <span class="n">trainNNet</span><span class="p">(</span><span class="n">examples</span><span class="p">)</span>                  
        <span class="n">frac_win</span> <span class="o">=</span> <span class="n">pit</span><span class="p">(</span><span class="n">new_nnet</span><span class="p">,</span> <span class="n">nnet</span><span class="p">)</span>                      <span class="c1"># compare new net with previous net</span>
        <span class="k">if</span> <span class="n">frac_win</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">:</span> 
            <span class="n">nnet</span> <span class="o">=</span> <span class="n">new_nnet</span>                                 <span class="c1"># replace with new net            </span>
    <span class="k">return</span> <span class="n">nnet</span>

<span class="k">def</span> <span class="nf">executeEpisode</span><span class="p">(</span><span class="n">game</span><span class="p">,</span> <span class="n">nnet</span><span class="p">):</span>
    <span class="n">examples</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">game</span><span class="o">.</span><span class="n">startState</span><span class="p">()</span>
    <span class="n">mcts</span> <span class="o">=</span> <span class="n">MCTS</span><span class="p">()</span>                                           <span class="c1"># initialise search tree</span>
        
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">numMCTSSims</span><span class="p">):</span>
            <span class="n">mcts</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">game</span><span class="p">,</span> <span class="n">nnet</span><span class="p">)</span>
        <span class="n">examples</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">s</span><span class="p">,</span> <span class="n">mcts</span><span class="o">.</span><span class="n">pi</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="kc">None</span><span class="p">])</span>              <span class="c1"># rewards can not be determined yet </span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">mcts</span><span class="o">.</span><span class="n">pi</span><span class="p">(</span><span class="n">s</span><span class="p">)),</span> <span class="n">p</span><span class="o">=</span><span class="n">mcts</span><span class="o">.</span><span class="n">pi</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>    <span class="c1"># sample action from improved policy</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">game</span><span class="o">.</span><span class="n">nextState</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">game</span><span class="o">.</span><span class="n">gameEnded</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
            <span class="n">examples</span> <span class="o">=</span> <span class="n">assignRewards</span><span class="p">(</span><span class="n">examples</span><span class="p">,</span> <span class="n">game</span><span class="o">.</span><span class="n">gameReward</span><span class="p">(</span><span class="n">s</span><span class="p">))</span> 
            <span class="k">return</span> <span class="n">examples</span>
</code></pre></div><h2>在黑白棋（Othello）上的实验</h2><p data-pid="JXOH7QR2">我们使用单个GPU，为 6x6 棋盘的黑白棋训练了一个智能体。每次迭代包含 100 局 self-play，每次 MCTS 使用25 次模拟。需要注意的是，这远小于 AlphaGo 论文中使用的计算量（每次迭代 25000 局对弈，每步 1600 次模拟）。该模型在 NVIDIA Tesla K80 GPU 上训练了大约3天（80次迭代）后达到了饱和。我们将该模型与随机算法、贪婪算法、基于 minimax 算法的智能体、人类进行了对比评估。得模型表现相当不错，甚至学会了一些人类常用的策略。</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-4fd4b1145f1e35a2738b68e37eee2335_720w.jpg?source=d16d100b" data-size="normal" data-rawwidth="730" data-rawheight="185" class="origin_image zh-lightbox-thumb" width="730" data-original="https://picx.zhimg.com/v2-4fd4b1145f1e35a2738b68e37eee2335_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='730'%20height='185'&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="730" data-rawheight="185" class="origin_image zh-lightbox-thumb lazy" width="730" data-original="https://picx.zhimg.com/v2-4fd4b1145f1e35a2738b68e37eee2335_720w.jpg?source=d16d100b" data-actualsrc="https://pic1.zhimg.com/v2-4fd4b1145f1e35a2738b68e37eee2335_720w.jpg?source=d16d100b"><figcaption>智能体（黑棋）学会了在开局时占边和占角</figcaption></figure><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-1536edcabb04ede76f56e19d596d89eb_720w.jpg?source=d16d100b" data-size="normal" data-rawwidth="730" data-rawheight="185" class="origin_image zh-lightbox-thumb" width="730" data-original="https://pic1.zhimg.com/v2-1536edcabb04ede76f56e19d596d89eb_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='730'%20height='185'&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="730" data-rawheight="185" class="origin_image zh-lightbox-thumb lazy" width="730" data-original="https://pic1.zhimg.com/v2-1536edcabb04ede76f56e19d596d89eb_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-1536edcabb04ede76f56e19d596d89eb_720w.jpg?source=d16d100b"><figcaption>智能体（黑棋）学会了在终盘的时候跳过一步棋</figcaption></figure><h2>一些细节上的注意事项</h2><p data-pid="RHqXkPBY">这篇文章概述了 AlphaGo Zero 论文中的核心思想，为了保持清晰省略了一些细节。论文中描述了一些实现中的额外细节，包括以下几点：</p><ul><li data-pid="8am5TAos"><b>状态历史</b>：由于围棋无法仅通过当前盘面状态观察到局势，神经网络还将最近 7 个时间步的棋盘状态作为输入。这是围棋本身的特性，而其他游戏（如国际象棋和黑白棋）只需要当前盘面作为输入即可。</li><li data-pid="HPjBboct"><b>温度系数</b>：执行 MCTS 之后得到的随机策略使用指数形式的统计值，即 <img src="https://www.zhihu.com/equation?tex=%5Cvec+%5Cpi%28s%29%3DN%28s%2C%5Ccdot%29%5E%7B1%2F%5Cgamma%7D+%2F+%5Csum_b+N%28s%2Cb%29%5E%7B1%2F%5Cgamma%7D" alt="\vec \pi(s)=N(s,\cdot)^{1/\gamma} / \sum_b N(s,b)^{1/\gamma}" eeimg="1"> ，其中 <img src="https://www.zhihu.com/equation?tex=%5Cgamma" alt="\gamma" eeimg="1"> 是控制探索程度的温度系数。AlphaGo Zero 在前 30 步棋使用 <img src="https://www.zhihu.com/equation?tex=%5Cgamma%3D1" alt="\gamma=1" eeimg="1">（即简单的归一化计数），然后将其设置成无穷大（即只采取计数最大的下法）。</li><li data-pid="OTmTmT3d"><b>对称性</b>：围棋棋盘对旋转和翻转具有不变性。当 MCTS 到达叶子节点时，神经网络会调用盘面的旋转或翻转版本，以利用这一对称性。通常这可以扩展到其他棋类游戏，利用适用于该游戏的对称性。</li><li data-pid="mGBCHpz9"><b>异步 MCTS</b>：AlphaGo Zero 使用了一种异步变种的 MCTS，能够并行执行模拟。神经网络的推理是批量进行的，每个搜索线程会被加锁直到评估完成。此外，三个主要过程：self-play、神经网络训练以及旧网络与新网络的比较，都是并行进行的。</li><li data-pid="AjkiTMPD"><b>计算能力</b>：每个神经网络使用 64 个GPU和 19 个CPU进行训练。论文中没有明确说明用于执行 self-play 的算力资源大小。</li><li data-pid="z7YKBJJO"><b>神经网络设计</b>：作者对多种架构进行了尝试，包括是否带有残差连接，以及价值网络、策略网络是否共享参数。他们最好的架构使用了残差网络，并且价值网络和策略网络共享了参数。</li></ul><blockquote data-pid="VvZX6q1k">这里的温度系数是用来控制分布的平滑程度的，可以参考我的另一篇文章 <a href="https://zhuanlan.zhihu.com/p/504323465" class="internal" target="_blank">深度学习高温蒸馏：Softmax With Temperature</a>。</blockquote><p data-pid="606T-lbW">本教程中给出的代码仅仅提供了算法的整体实现。完整的、通用的实现版本可以在这个 <a href="http://link.zhihu.com/?target=https%3A//github.com/suragnair/alpha-zero-general" class=" wrap external" target="_blank" rel="nofollow noreferrer">GitHub 仓库</a>中找到，其中包含用 PyTorch、Keras 和 TensorFlow 实现的黑白棋（Othello）游戏例程。评论区目前已经不太活跃了。如果有任何意见或建议，欢迎留言 :)。对于紧急问题，可以考虑在 GitHub 上提交 <a href="http://link.zhihu.com/?target=https%3A//github.com/suragnair/alpha-zero-general/issues" class=" wrap external" target="_blank" rel="nofollow noreferrer">Issue</a>。</p><p data-pid="ufcYc71V">原文链接：<a href="http://link.zhihu.com/?target=https%3A//suragnair.github.io/posts/alphazero.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">Simple Alpha Zero</a></p>