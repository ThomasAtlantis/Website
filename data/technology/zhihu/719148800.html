<p data-pid="knTPdTts"><a href="http://link.zhihu.com/?target=https%3A//proceedings.neurips.cc/paper_files/paper/2023/file/6ceefa7b15572587b78ecfcebb2827f8-Paper-Conference.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models</a>，NeurIPS 2023</p><figure data-size="normal"><noscript><img src="https://pica.zhimg.com/v2-98126d7c2b4d4c2e75d5fab3e6d59977_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="2056" data-rawheight="245" class="origin_image zh-lightbox-thumb" width="2056" data-original="https://pic1.zhimg.com/v2-98126d7c2b4d4c2e75d5fab3e6d59977_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='2056'%20height='245'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="2056" data-rawheight="245" class="origin_image zh-lightbox-thumb lazy" width="2056" data-original="https://pic1.zhimg.com/v2-98126d7c2b4d4c2e75d5fab3e6d59977_720w.jpg?source=d16d100b" data-actualsrc="https://pica.zhimg.com/v2-98126d7c2b4d4c2e75d5fab3e6d59977_720w.jpg?source=d16d100b"></figure><h2>研究背景</h2><p data-pid="Hgv0eSeM">大型语言模型（LLM）的部署成本非常高昂。除了广泛研究的模型参数大小瓶颈和注意力层的平方计算复杂度之外，KV cache 的大小问题也越来越突出。KV cache 用于在生成过程中存储注意力中间结果的 key 和 value 以避免重新计算。一个 30B 参数的模型，输入批量大小 128，序列长度 1024，则需要 180GB 的 KV 缓存。</p><p data-pid="E-EwTZ17">一种自然的方法是限制 cache 的最大大小，就像在经典软件或硬件缓存中所做的那样。然而，在不降低准确率的情况下减少 LLM 中的 KV 缓存内存占用是一项挑战。</p><h2>研究摘要</h2><p data-pid="t_v15MXv">尽管大语言模型（LLM）能力很强，部署成本却非常高，尤其是涉及长内容生成的应用，例如对话系统、故事写作等。因为，除模型参数之外，KV cache 通常也驻留在 GPU 显存中，并且空间复杂度与序列长度、批处理大小成线性关系。为此，本文提出一种新方法来减少 KV cache 的显存占用。</p><p data-pid="xHq7BVEd">作者发现，推理过程中的注意力分数主要由一小部分 token 贡献。他们将这类 token 称为 Heavy Hitter（简称H2，可翻译为热点 token 或者显著 token）。他们发现（1）H2 token 天然存在，与文本中 token 的共现频率密切相关，并且（2）删除它们会导致性能显著下降。</p><p data-pid="4EcoD4-N">据此，作者提出了Heavy Hitter Oracle（简称H2O），一种 KV 缓存的驱逐（eviction）策略，动态地保持了 cache 中最近 token 和 H2 token 的比例平衡。作者将 KV 缓存驱逐建模为动态次模优化问题，并提供了保守假设下的理论证明。</p><p data-pid="Extkksc9">文章使用 OPT、LLaMA 和 GPT-NeoX 在各种任务中验证了算法的准确性。在 OPT-6.7B 和 OPT-30B 上实现了具有 20% H2 token 的 H2O 算法，在三个领先的推理系统 DeepSpeed Zero-Inference、Hugging Face Accelerate 和 FlexGen 上，分别将吞吐率提高了 29 倍、29 倍和 3 倍。在批处理大小相同的情况下，H2O 可以将延迟最多降低 1.9 倍。代码参见 <a href="http://link.zhihu.com/?target=https%3A//github.com/FMInference/H2O" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">https://</span><span class="visible">github.com/FMInference/</span><span class="invisible">H2O</span><span class="ellipsis"></span></a> </p><h2>问题建模</h2><p data-pid="cP9llQS1">接下来先粗糙地定义一下 KV cache 大小受限情况下的生成过程。</p><p data-pid="yR82PENC">用 <img src="https://www.zhihu.com/equation?tex=Q%5Cin%5Cmathbb+R%5E%7Bn%5Ctimes+d%7D" alt="Q\in\mathbb R^{n\times d}" eeimg="1"> 和 <img src="https://www.zhihu.com/equation?tex=K%5Cin%5Cmathbb%7BR%7D%5E%7Bn%5Ctimes+d%7D" alt="K\in\mathbb{R}^{n\times d}" eeimg="1"> 分别表示 query 矩阵和 key 矩阵。 <img src="https://www.zhihu.com/equation?tex=Q_%7Bi%2C%2A%7D" alt="Q_{i,*}" eeimg="1"> 表示 <img src="https://www.zhihu.com/equation?tex=Q" alt="Q" eeimg="1"> 的第 <img src="https://www.zhihu.com/equation?tex=i" alt="i" eeimg="1"> 行， <img src="https://www.zhihu.com/equation?tex=K_%7B%5Cleq+i%2C+%2A%7D" alt="K_{\leq i, *}" eeimg="1"> 表示 <img src="https://www.zhihu.com/equation?tex=K" alt="K" eeimg="1"> 的前 <img src="https://www.zhihu.com/equation?tex=i" alt="i" eeimg="1"> 行。令 cache 的空间预算为 <img src="https://www.zhihu.com/equation?tex=k" alt="k" eeimg="1"> ，并且 <img src="https://www.zhihu.com/equation?tex=k+%5Clt+n" alt="k \lt n" eeimg="1"> 。为了方便，用 <img src="https://www.zhihu.com/equation?tex=K_%7BS_i%2C%2A%7D%28%5Cin+%5Cmathbb+R%5E%7Bi%5Ctimes+d%7D%29" alt="K_{S_i,*}(\in \mathbb R^{i\times d})" eeimg="1"> 表示 <img src="https://www.zhihu.com/equation?tex=K" alt="K" eeimg="1"> 的 <img src="https://www.zhihu.com/equation?tex=S_i" alt="S_i" eeimg="1"> 下标的行组成的子矩阵（对于没有被选中的行 <img src="https://www.zhihu.com/equation?tex=%5Bi%5D+%5Cbackslash+S_i" alt="[i] \backslash S_i" eeimg="1"> ，我们将所有元素置零）。</p><p data-pid="ZoCSru_w">这里 <img src="https://www.zhihu.com/equation?tex=%5Bi%5D%3A%3D%5C%7B1%2C2%2C%5Cdots%2Ci%5C%7D" alt="[i]:=\{1,2,\dots,i\}" eeimg="1"> 的符号用法貌似不太常见，文中也没有提示。另外， <img src="https://www.zhihu.com/equation?tex=S_i" alt="S_i" eeimg="1"> 其实表示的是 KV cache 中存储的 KV 对应的 token 索引集合。它之所以有个下标 <img src="https://www.zhihu.com/equation?tex=i" alt="i" eeimg="1"> ，是因为本文考虑自回归生成过程， <img src="https://www.zhihu.com/equation?tex=i" alt="i" eeimg="1"> 既表示当前 KV 的最后一行，也表示当前的时间步。此外，因为 <img src="https://www.zhihu.com/equation?tex=K_%7BS_i%2C%2A%7D" alt="K_{S_i,*}" eeimg="1"> 中被选中的行只是置零而不是删除，所以生成到第 <img src="https://www.zhihu.com/equation?tex=i" alt="i" eeimg="1"> 个 token 时的矩阵大小仍然为 <img src="https://www.zhihu.com/equation?tex=i%5Ctimes+d" alt="i\times d" eeimg="1"> 。</p><p data-pid="d38FS3bp"><b>定义 2.1</b>（驱逐策略，非正式）令 <img src="https://www.zhihu.com/equation?tex=S_%7Bi-1%7D" alt="S_{i-1}" eeimg="1"> 表示源集合、 <img src="https://www.zhihu.com/equation?tex=S_i" alt="S_i" eeimg="1"> 表示目标集合。定义驱逐策略 <img src="https://www.zhihu.com/equation?tex=g%3AS_%7Bi-1%7D%5Crightarrow+S_i" alt="g:S_{i-1}\rightarrow S_i" eeimg="1"> ，使得</p><ul><li data-pid="1CV-KFpq"><img src="https://www.zhihu.com/equation?tex=%7CS_i%7C%3Dk" alt="|S_i|=k" eeimg="1"> （KV cache 的大小不随时间变化）</li><li data-pid="JIqRX4e0"><img src="https://www.zhihu.com/equation?tex=%7CS_i+%5Cbackslash+S_%7Bi-1%7D%7C%5Cleq+1" alt="|S_i \backslash S_{i-1}|\leq 1" eeimg="1"> 或者与之等价的 <img src="https://www.zhihu.com/equation?tex=%7CS_i+%5Ccap+S_%7Bi-1%7D%7C%5Cge+k+-+1" alt="|S_i \cap S_{i-1}|\ge k - 1" eeimg="1">（从KV cache中最多可以驱逐1个KV）</li></ul><p data-pid="OQs-1Q_n"><b>定义 2.2</b>（带驱逐策略的生成过程，非正式）令 KV cache 的大小为 <img src="https://www.zhihu.com/equation?tex=k" alt="k" eeimg="1"> ，对于每个 <img src="https://www.zhihu.com/equation?tex=i+%5Cin+%5Bn%5D" alt="i \in [n]" eeimg="1"> ，对第 <img src="https://www.zhihu.com/equation?tex=i" alt="i" eeimg="1"> 个 token</p><ul><li data-pid="mWOC9tAB">令 <img src="https://www.zhihu.com/equation?tex=S_i+%5Csubset+%5Bn%5D" alt="S_i \subset [n]" eeimg="1"> 表示在预测第 <img src="https://www.zhihu.com/equation?tex=i" alt="i" eeimg="1"> 个 token 时的 KV cache</li><li data-pid="95s-XkPA">我们所拥有的信息是一个长度为 <img src="https://www.zhihu.com/equation?tex=i" alt="i" eeimg="1"> 的向量 <img src="https://www.zhihu.com/equation?tex=o_i+%3A%3DD_i%5E%7B-1%7D+%5Ccdot%28+%5Cexp+%28Q_%7Bi%2C+%2A%7D%28K_%7BS_i%2C+%2A%7D%29%5E%5Ctop%29+-+%7B1%7D_%7B%5Bi%5D%5Cbackslash+S_i%7D%29" alt="o_i :=D_i^{-1} \cdot( \exp (Q_{i, *}(K_{S_i, *})^\top) - {1}_{[i]\backslash S_i})" eeimg="1"> （归一化的注意力）</li><ul><li data-pid="wvzVntyR">标量 <img src="https://www.zhihu.com/equation?tex=D_i+%3A%3D%28%5Cexp+%28Q_%7Bi%2C+%2A%7D%28K_%7BS_i%2C+%2A%7D%29%5E%5Ctop%29+-+%7B1%7D_%7B%5Bi%5D%5Cbackslash+S_i%7D%29%5Ccdot+%5Cmathbf+1_i" alt="D_i :=(\exp (Q_{i, *}(K_{S_i, *})^\top) - {1}_{[i]\backslash S_i})\cdot \mathbf 1_i" eeimg="1">（被驱逐的 KV 被设置为0，exp 计算的结果就是1，我们需要在计算归一化的时候减掉它们。 <img src="https://www.zhihu.com/equation?tex=+%7B1%7D_%7B%5Bi%5D%5Cbackslash+S_i%7D" alt=" {1}_{[i]\backslash S_i}" eeimg="1"> 表示下标 <img src="https://www.zhihu.com/equation?tex=%5Bi%5D%5Cbackslash+S_i" alt="[i]\backslash S_i" eeimg="1"> 处为1、其余为0的向量，而 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf+1_i" alt="\mathbf 1_i" eeimg="1"> 表示长度为 <img src="https://www.zhihu.com/equation?tex=i" alt="i" eeimg="1"> 的全1向量。）</li><li data-pid="YzVcazUh">如果将以上 <img src="https://www.zhihu.com/equation?tex=o_i" alt="o_i" eeimg="1"> 和 <img src="https://www.zhihu.com/equation?tex=D_i" alt="D_i" eeimg="1"> 定义中的 <img src="https://www.zhihu.com/equation?tex=S_i" alt="S_i" eeimg="1"> 替换为 <img src="https://www.zhihu.com/equation?tex=%5Bi%5D" alt="[i]" eeimg="1"> 就得到了标准的生成过程</li></ul><li data-pid="omzGSLIK">驱逐策略（定义2.1）根据 <img src="https://www.zhihu.com/equation?tex=S_%7Bi-1%7D" alt="S_{i-1}" eeimg="1"> 和与之对应的信息 <img src="https://www.zhihu.com/equation?tex=o_%7Bi-1%7D" alt="o_{i-1}" eeimg="1"> 来得到更新后的 <img src="https://www.zhihu.com/equation?tex=S_i" alt="S_i" eeimg="1"> 。</li></ul><p data-pid="v3pZKxMH"><b>要点 2.3</b> 本文的目标是找到一种 KV cache 的驱逐策略，使得生成过程的输出结果与不限制 cache 大小时相近或可比较。</p><h2>观察发现</h2><p data-pid="E6KJ5EGv"><b>注意力稀疏性使小 cache 成为可能</b></p><p data-pid="5uzmHB3h">受已有工作（<sup data-text="On the Expressive Power of Self-Attention Matrices" data-url="https://arxiv.org/pdf/2106.03764" data-draft-node="inline" data-draft-type="reference" data-numero="1">[1]</sup>揭示了 <a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1910.01108" class=" wrap external" target="_blank" rel="nofollow noreferrer">DistilBert</a> 中存在注意力的稀疏性，<sup data-text="Inductive Biases and Variable Creation in Self-Attention Mechanisms" data-url="https://proceedings.mlr.press/v162/edelman22a/edelman22a.pdf" data-draft-node="inline" data-draft-type="reference" data-numero="2">[2]</sup>中提到有界归一化的 Transformer 网络创造了稀疏变量，一个单独的自注意力头可以表征输入序列的一个稀疏函数）的启发，本文首次展示了预训练 LLM 中的注意力稀疏性。接下来探讨这个观察是如何让削减 KV cache 的大小同时不损害精度成为可能。给定归一化的注意力分数 <img src="https://www.zhihu.com/equation?tex=%5Ctext%7Bsoftmax%7D%28QK%5E%5Ctop%29" alt="\text{softmax}(QK^\top)" eeimg="1"> ，我们设定剪枝阈值为每行最大值的1%，并计算对应的稀疏度。</p><p data-pid="wANrfE4D">我们使用预训练的 OPT 模型在验证集 Wiki-Text-103 上运行一次性剪枝。图 1-(a) 展示了注意力模块在各层中的稀疏度，并可视化了归一化的注意力分数矩阵。</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-cf6af4a9bb5eb80d4f672f291babb735_720w.jpg?source=d16d100b" data-size="normal" data-rawwidth="3630" data-rawheight="854" class="origin_image zh-lightbox-thumb" width="3630" data-original="https://pic1.zhimg.com/v2-cf6af4a9bb5eb80d4f672f291babb735_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='3630'%20height='854'&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="3630" data-rawheight="854" class="origin_image zh-lightbox-thumb lazy" width="3630" data-original="https://pic1.zhimg.com/v2-cf6af4a9bb5eb80d4f672f291babb735_720w.jpg?source=d16d100b" data-actualsrc="https://pic1.zhimg.com/v2-cf6af4a9bb5eb80d4f672f291babb735_720w.jpg?source=d16d100b"><figcaption>图1</figcaption></figure><p data-pid="I-BMqAnu">我们观察到，尽管大模型是稠密训练的，得到的注意力分数矩阵却是高度稀疏的，几乎在所有层都具有 95% 以上的稀疏度。注意力模块的稀疏性说明生成下一个 token 时访问此前所有的 key 和 value 的 embedding 是没有必要的。这说明我们在生成过程中可以驱逐一些非必要的 KV embedding 来减少 KV cache 的空间需求。</p><p data-pid="naVpcsHO"><b>Heavy-Hitter 使高命中率成为可能</b></p><p data-pid="q34zHyql">寻找保持生成精度的最优驱逐策略仍然是一项组合数难度的问题。虽然 Belady 的算法<sup data-text="A study of replacement algorithms for a virtual-storage computer" data-url="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=eacfdf93e03d9dbbbaa2d01250939d9f94fb16a4" data-draft-node="inline" data-draft-type="reference" data-numero="3">[3]</sup>对于标准的 cache（离线）是最优且容易计算的，但它并不适用于 KV cache 设计。一旦驱逐了重要 KV 就可能破坏 LLM 的性能，因为 LLM 的生成过程具有序列依赖性。</p><p data-pid="LCmrX2jH">幸运的是，我们发现注意力模块内所有 token 的累积注意力分数遵循幂律分布，如图 1-(b) 所示。这表明存在一小组在生成过程中至关重要的 token，称其为 heavy-hitters (H2)。为了验证其重要性，我们将去除 H2 后的 LLM 生成质量与原始模型对比。如图 1-(c) 所示，准确率大幅下降，证实了这些 token 的重要性。此外，我们可以看到每个 token 的累积注意力分数（红点）与它们在数据中的共现频率（灰色曲线）高度相关。</p><p data-pid="7GvJNrEi">首先，H2 提供了一个绕过组合优化问题的机会。我们设计了一种 KV cache 驱逐策略：仅保留 cache 中的 H2 和最近期的 KV embedding（因为直觉上最近的 token 通常与当前 token 表现出更强的相关性）。我们通过预训练的 OPT-30B 和 6 个下游任务来评估该驱逐策略的有效性。如图 1-(4) 所示，很明显，基于 H2 的驱逐策略可以大大减少 KV 缓存大小，而不会降低 OPT-30B 的性能。</p><p data-pid="nop0ya40">此外，在后期分析中，受<sup data-text="Resurrecting submodularity for neural text generation" data-url="https://openreview.net/pdf?id=FVhZIBWqykk" data-draft-node="inline" data-draft-type="reference" data-numero="4">[4]</sup>的启发，我们发现基于 H2 的策略与经典贪婪算法（具有证明保证的多项式时间算法）相关，前提是注意力机制符合次模性质。附录 D 中提供了更详细的信息。</p><p data-pid="p1hJrSq4"><b>引理 3.1</b>（非正式）假设注意力机制是次模的，那么贪心地构造集合 <img src="https://www.zhihu.com/equation?tex=S_i" alt="S_i" eeimg="1">（没有缓存大小限制）满足次模的近似最优性质。</p><h2>算法设计</h2><p data-pid="QIi5drwA">上文展示了一个简单有效的基于 H2 的 KV 驱逐策略，然而这个算法无法真正地部署，因为我们没法访问未来生成的 token。幸运的是，我们在实验中观察到局部 H2 现象，在每一步解码中只需要使用局部数据计算。累加此前所有 token 的注意力分数，与考虑未来 token 注意力的方法有效等价。接下来，我们正式地将这种动态注意力分数的计算（在 cache 有限的约束下）定义为一个新的动态次模优化问题。</p><p data-pid="4U3l9hNO"><b>定义 4.1</b>（动态次模框架）定义函数 <img src="https://www.zhihu.com/equation?tex=F%3A2%5E%7B%5Bn%5D%7D%5Ctimes+2%5E%7B%5Bn%5D%7D+%5Crightarrow+%5Cmathbb+R" alt="F:2^{[n]}\times 2^{[n]} \rightarrow \mathbb R" eeimg="1">，对任意集合 <img src="https://www.zhihu.com/equation?tex=Z+%5Csubset+%5Bn%5D" alt="Z \subset [n]" eeimg="1">，假设 <img src="https://www.zhihu.com/equation?tex=F%28Z%2C+%5Ccdot%29%3A2+%5E%7B%5Bn%5D%7D+%5Crightarrow+%5Cmathbb+R" alt="F(Z, \cdot):2 ^{[n]} \rightarrow \mathbb R" eeimg="1">  是一个关于 <img src="https://www.zhihu.com/equation?tex=Z" alt="Z" eeimg="1"> 的次模函数，且满足 <img src="https://www.zhihu.com/equation?tex=Z%5Csubset+X+%5Csubset+Y+%5Csubset+%5Bn%5D" alt="Z\subset X \subset Y \subset [n]" eeimg="1">，<img src="https://www.zhihu.com/equation?tex=x+%5Cin+%5Bn%5D%5Cbackslash+Y" alt="x \in [n]\backslash Y" eeimg="1"> ，我们有</p><p data-pid="w5OvX01W"><img src="https://www.zhihu.com/equation?tex=f%28X%5Ccup+%5C%7Bx%5C%7D%29-f%28X%29%5Cgeq+f%28Y%5Ccup+%5C%7Bx%5C%7D%29-f%28Y%29++%5C%5C" alt="f(X\cup \{x\})-f(X)\geq f(Y\cup \{x\})-f(Y)  \\" eeimg="1"></p><p data-pid="LAAsNiPB">其中 <img src="https://www.zhihu.com/equation?tex=f%28%5Ccdot%29%3A%3DF%28Z%2C%5Ccdot%29+" alt="f(\cdot):=F(Z,\cdot) " eeimg="1"> 。</p><p data-pid="jP1feI9O"><b>要点 4.2</b> <img src="https://www.zhihu.com/equation?tex=X" alt="X" eeimg="1"> 表示 KV cache 中已经存在的单词，<img src="https://www.zhihu.com/equation?tex=Y" alt="Y" eeimg="1"> 是 <img src="https://www.zhihu.com/equation?tex=X" alt="X" eeimg="1"> 的任意超集。<img src="https://www.zhihu.com/equation?tex=x" alt="x" eeimg="1"> 可以被看作新加入的单词 <img src="https://www.zhihu.com/equation?tex=i" alt="i" eeimg="1"> 或者将要从 cache 中删除的单词。<img src="https://www.zhihu.com/equation?tex=f" alt="f" eeimg="1"> 可以用来表示注意力分数。</p><p data-pid="QzUSdbLJ">如果将序列 <img src="https://www.zhihu.com/equation?tex=S_1%2CS_2%2C%5Cdots%2CS_n" alt="S_1,S_2,\dots,S_n" eeimg="1">（我们保证 <img src="https://www.zhihu.com/equation?tex=%7CS_i%7C+%5Cleq+k" alt="|S_i| \leq k" eeimg="1"> 并且 <img src="https://www.zhihu.com/equation?tex=%7CS_i%5Cbackslash+S_%7Bi-1%7D%7C%5Cleq+1" alt="|S_i\backslash S_{i-1}|\leq 1" eeimg="1"> ）带入定义4.1，即对于每个 <img src="https://www.zhihu.com/equation?tex=i%5Cin+%5Bn%5D" alt="i\in [n]" eeimg="1">，我们选择 <img src="https://www.zhihu.com/equation?tex=Z%3DS_i" alt="Z=S_i" eeimg="1">，于是它变成了一个动态次模优化的特定的例子。</p><p data-pid="Bo7XV7wU"><b>定义4.3</b>（H2O 驱逐策略）在定义 2.1 的基础上，令 <img src="https://www.zhihu.com/equation?tex=F_%7B%5Ctext%7Bscore%7D%7D%3A2%5E%7B%5Bn%5D%7D%5Crightarrow+%5Cmathbb%7BR%7D" alt="F_{\text{score}}:2^{[n]}\rightarrow \mathbb{R}" eeimg="1"> 表示特定的得分函数。我们定义驱逐策略 <img src="https://www.zhihu.com/equation?tex=g%3AS_%7Bi-1%7D%5Crightarrow+S_i" alt="g:S_{i-1}\rightarrow S_i" eeimg="1">。将 <img src="https://www.zhihu.com/equation?tex=S_i+%5Cleftarrow+%28S_%7Bi-1%7D%5Ccup+%5C%7Bi%5C%7D%29%5Cbackslash+%5C%7Bu%5C%7D" alt="S_i \leftarrow (S_{i-1}\cup \{i\})\backslash \{u\}" eeimg="1"> 构造为 <img src="https://www.zhihu.com/equation?tex=u%5Cleftarrow+%7B%5Carg%5Cmax+%7D_%7Bv%5Cin+%28S_%7Bi-1%7D%5Ccup+%5C%7Bi%5C%7D%29%7DF_%7B%5Ctext%7Bscore%7D%7D%28S_%7Bi-1%7D%5Ccup+%5C%7Bi%5C%7D%5Cbackslash+%5C%7Bv%5C%7D%29+%5C%5C" alt="u\leftarrow {\arg\max }_{v\in (S_{i-1}\cup \{i\})}F_{\text{score}}(S_{i-1}\cup \{i\}\backslash \{v\}) \\" eeimg="1">把以上流程综合在一起，选用一种特定的 <img src="https://www.zhihu.com/equation?tex=F_%7B%5Ctext%7Bscore%7D%7D" alt="F_{\text{score}}" eeimg="1">：给定 token 集合的注意力分数之和，得到算法如下</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-4e29b8afda64fa8619f82432e5226930_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="2107" data-rawheight="1083" class="origin_image zh-lightbox-thumb" width="2107" data-original="https://picx.zhimg.com/v2-4e29b8afda64fa8619f82432e5226930_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='2107'%20height='1083'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="2107" data-rawheight="1083" class="origin_image zh-lightbox-thumb lazy" width="2107" data-original="https://picx.zhimg.com/v2-4e29b8afda64fa8619f82432e5226930_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-4e29b8afda64fa8619f82432e5226930_720w.jpg?source=d16d100b"></figure><p data-pid="d9LoOlcO">以上伪代码的第 10 行其实写得不甚了了。<img src="https://www.zhihu.com/equation?tex=o_i" alt="o_i" eeimg="1"> 算出来应当是长度为 <img src="https://www.zhihu.com/equation?tex=i-1" alt="i-1" eeimg="1"> 的向量（那么第 8 行应当是 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf+1_%7Bi-1%7D" alt="\mathbf 1_{i-1}" eeimg="1"> ，并且与定义 2.2 中冲突，应以此处为准），表示当前第 <img src="https://www.zhihu.com/equation?tex=i" alt="i" eeimg="1"> 个 token 与历史的 <img src="https://www.zhihu.com/equation?tex=i-1" alt="i-1" eeimg="1"> 个 token 的相关程度。那么，第 10 行的定义应为 <img src="https://www.zhihu.com/equation?tex=F_%7B%5Ctext+%7Bscore%7D%7D%28T%29%3D%5Csum_%7Bs%5Cin+T%7D%5Csum_%7Bk%5Cin+%5Bi%5D%7D+o_%7Bks%7D" alt="F_{\text {score}}(T)=\sum_{s\in T}\sum_{k\in [i]} o_{ks}" eeimg="1">。在 <img src="https://www.zhihu.com/equation?tex=i" alt="i" eeimg="1"> 时刻，可以假定 <img src="https://www.zhihu.com/equation?tex=o_k%28k%5Cle+i%29" alt="o_k(k\le i)" eeimg="1"> 也都是长度为 <img src="https://www.zhihu.com/equation?tex=i" alt="i" eeimg="1"> 的向量，只不过 <img src="https://www.zhihu.com/equation?tex=o_%7Bkj%7D%28k+%5Clt+j+%5Cleq+i%29" alt="o_{kj}(k \lt j \leq i)" eeimg="1"> 都被 mask 掉了。这与文章的<a href="http://link.zhihu.com/?target=https%3A//github.com/FMInference/H2O/blob/ac75c2a8a9e76832b2a4139b9363373b56336bfb/h2o_hf/utils_hh/modify_llama.py%23L122" class=" wrap external" target="_blank" rel="nofollow noreferrer">代码实现</a>一致：</p><div class="highlight"><pre><code class="language-python"><span></span><span class="c1"># attn_weights (BS, heads, q-tokens, k-tokens)</span>
<span class="n">current_scores_sum</span> <span class="o">=</span> <span class="n">attn_weights</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># (heads, k-tokens)</span>
</code></pre></div><p data-pid="qwzZ4KUR">代码比较偷懒，没有实现自回归生成过程，每一步都是用全部 token 的 query 去查询的。如果要实现真的自回归生成，就要缓存以上的各个 token 的累积注意力分数。文章绕来绕去搞这么复杂，其实就一句话：<b>把累积注意力分数最低的 token 驱逐出 cache</b>。</p><p data-pid="pr9S-YEE">不禁要吐槽一下，这篇文章很多公式和符号定义都有疏漏，而且非正式建模和正式建模虽然循循善诱，但重复内容太多，把一个大家普遍使用的剪枝方法包装这么复杂，就是为了用次模问题去解释这种贪心算法的合理性。然而文章究竟是怎么把贪心地添加元素转换为贪心地去除元素的，我还是没看懂。</p><p data-pid="HgXpFSNg">另外，作者还提到了实际实现中的细节。比如，为了保证I/O效率，我们在驱逐存储的KV时不会交换内存，而是直接填充新添加的KV。</p><h2>实验结果</h2><p data-pid="N9MW9Csc">选用了3个具有代表性的 LLM 模型系列：OPT、LLaMA 和 GPT-NeoX-20B。选取了8个评估任务：COPA、MathQA、OpenBookQA、PiQA、RTE、Winogrande、XSUM 和 CNN/Daily Mail。实验的硬件采用NVIDIA A100 80GB GPU。由于 H2O 既缓存 H2 也缓存最近期的 token，这里除了完整的KV缓存（Full），还将只缓存近期 token 的策略（Local）作为基线方法。<br></p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-3ff943f1b966bdb43e38788989b14d14_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="2122" data-rawheight="1542" class="origin_image zh-lightbox-thumb" width="2122" data-original="https://picx.zhimg.com/v2-3ff943f1b966bdb43e38788989b14d14_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='2122'%20height='1542'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="2122" data-rawheight="1542" class="origin_image zh-lightbox-thumb lazy" width="2122" data-original="https://picx.zhimg.com/v2-3ff943f1b966bdb43e38788989b14d14_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-3ff943f1b966bdb43e38788989b14d14_720w.jpg?source=d16d100b"></figure><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-1c305fad7b09af297585b954294b9d9e_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="1356" data-rawheight="585" class="origin_image zh-lightbox-thumb" width="1356" data-original="https://picx.zhimg.com/v2-1c305fad7b09af297585b954294b9d9e_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1356'%20height='585'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1356" data-rawheight="585" class="origin_image zh-lightbox-thumb lazy" width="1356" data-original="https://picx.zhimg.com/v2-1c305fad7b09af297585b954294b9d9e_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-1c305fad7b09af297585b954294b9d9e_720w.jpg?source=d16d100b"></figure><p data-pid="UUDxnbkA"><br>由上面的图和表可知：在不同的 KV cache 预算下，本文提出的方法（H2O）在各种不同条件的测试中都优于 Local 策略。同时，在低于 20% 的 KV 缓存预算之下，H2O 实现了与 Full 相当的性能，且在更具挑战性的长序列生成任务、XSUM 和 CNN/Daily Mail 中表现良好。</p>