<p data-pid="llmOnHC_">本文主要讨论PyTorch模型训练中的<b>两种可复现性</b>：一种是在完全不改动代码的情况下重复运行，获得相同的准确率曲线；另一种是改动有限的代码，改动部分不影响训练过程的前提下，获得相同的曲线。</p><p data-pid="LfPSPyye"><b>#1 第一种情况，浅显地讲，我们只需要固定所有随机数种子就行。</b></p><p data-pid="Hitwg77w">我们知道，计算机一般会使用混合线性同余法来生成伪随机数序列。在我们每次调用rand()函数时，就会执行一次或若干次下面的递推公式：</p><p data-pid="E0HtnN_P"><img src="https://www.zhihu.com/equation?tex=x_%7Bn%2B1%7D%3D%28ax_n%2Bc%29+%5C+%5Ctext%7Bmod%7D%5C+%28m%29%5C%5C" alt="x_{n+1}=(ax_n+c) \ \text{mod}\ (m)\\" eeimg="1"> </p><p data-pid="94590zf9">当 <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"> 、 <img src="https://www.zhihu.com/equation?tex=c" alt="c" eeimg="1"> 和 <img src="https://www.zhihu.com/equation?tex=m" alt="m" eeimg="1"> 满足一定条件时，可以近似地认为 <img src="https://www.zhihu.com/equation?tex=x" alt="x" eeimg="1"> 序列中的每一项符合均匀分布，通过 <img src="https://www.zhihu.com/equation?tex=x%2Fm" alt="x/m" eeimg="1"> 我们可以得到0到1之间的随机数。这类算法都有一个特点，就是一旦固定了序列的初始值 <img src="https://www.zhihu.com/equation?tex=x_0" alt="x_0" eeimg="1"> ，整个随机数序列也就固定了，这个初始值就被我们称作种子。也就是说，我们在程序的起始位置设定好随机数种子，程序单次执行中第 <img src="https://www.zhihu.com/equation?tex=n" alt="n" eeimg="1"> 次调用到rand()得到的数值将会是固定的，<b>一旦程序中rand()函数的调用顺序固定</b>，无论程序重复运行多少遍，结果都将是稳定的。在Minecraft中我们可以通过特定的种子生成一模一样的世界就是这个原理。</p><p data-pid="XGQtaLMn">在深度学习中，我们常用Dropout减轻过拟合现象，在训练时会<b>随机</b>抑制一定比例的神经元（将激活值设定为零）；常用RandomFlip、RandomCrop等方法处理训练集，引入一些<b>随机</b>噪声来提高模型泛化能力；常用shuffle的方式从训练集中<b>随机</b>抽取batch，一方面可以稳定训练，一方面也可以减轻过拟合。这些方法都引入了训练的随机性。我们在炼丹调参的时候肯定希望特定的超参数对应固定的性能，否则就不能肯定模型效果是超参数带来的还是随机性带来的了。</p><p data-pid="6zmdZI-0">在PyTorch中我们一般使用如下方法固定随机数种子。这个函数的调用尽量放在所有import之后，其他代码之前。</p><div class="highlight"><pre><code class="language-python"><span></span><span class="k">def</span> <span class="nf">seed_everything</span><span class="p">(</span><span class="n">seed</span><span class="p">):</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>       <span class="c1"># Current CPU</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>  <span class="c1"># Current GPU</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>          <span class="c1"># Numpy module</span>
    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>             <span class="c1"># Python random module</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>    <span class="c1"># Close optimization</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span> <span class="c1"># Close optimization</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span> <span class="c1"># All GPU (Optional)</span>
</code></pre></div><p data-pid="jKxjBGD6">有些工具库中已经给出了类似的函数，但效果需要自己实验确定，比如pytorch_lightning.seed_everything中就没有去除cudnn对于卷积操作的优化，很多情况下仍然无法复现。建议使用上面给出的代码，至少在我的实验中一直是可以实现稳定复现的。</p><p data-pid="AlmyjOvv"><b>#2 第二种情况，总的来说，一定要万分确定改动的代码没有影响random()的调用顺序。</b></p><p data-pid="FB3bOsY9">重复运行的可复现性早有讨论，但修改代码的可复现性其实是更大的陷阱。如果你觉得，这么简单的问题会有人犯错吗？连自己的代码有没有影响训练都不知道吗？我们看如下问题：<b>在固定随机数种子的前提下，你写了一个训练模型的代码，输出了训练的loss和准确率并绘制了图像。突然你想在每轮训练之后再测一下测试准确率，于是小心翼翼地修改了代码，那么问题来了，训练的loss和准确率会和之前一样吗？</b></p><p data-pid="-wL0ojFz">如果你没有加入额外的操作，答案是<b>一定会不一样！</b>我最近的实验中就发现，模型测试的次数会很明显地影响准确率本身，测的次数不一样，准确率也不一样，有时候训练结束的效果甚至会波动1%这么大。我实验中要验证的算法是两个模型协同训练的，其中一个模型应该与Baseline性能曲线完全相同，现在实验结果却差了1%，尴尬了！</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-5febc0f861cc5169483ffc3457b2ebc8_720w.jpg?source=d16d100b" data-size="normal" data-rawwidth="550" data-rawheight="186" class="origin_image zh-lightbox-thumb" width="550" data-original="https://pic1.zhimg.com/v2-5febc0f861cc5169483ffc3457b2ebc8_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='550'%20height='186'&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="550" data-rawheight="186" class="origin_image zh-lightbox-thumb lazy" width="550" data-original="https://pic1.zhimg.com/v2-5febc0f861cc5169483ffc3457b2ebc8_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-5febc0f861cc5169483ffc3457b2ebc8_720w.jpg?source=d16d100b"><figcaption>海森堡测不准原理</figcaption></figure><p data-pid="ag6zlKoY">首先排除其他因素，比如我们在测试时确定使用了model.eval()，避免了前向传播时Dropout层起作用，也避免了BatchNorm层对数据的均值方差进行滑动平均，可以认为我们避免了一切直接影响模型参数的操作。那究竟是什么在作祟？</p><p data-pid="Vce5HZPj">首先要清楚我提到的固定随机数种子对可复现性起作用的前提：<b>rand()函数调用的次序固定</b>。也就是说，假如在某次rand()调用之前我们插入了其他的rand()操作，那这次的结果必然不同。</p><div class="highlight"><pre><code class="language-python"><span></span><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">torch</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="n">seed_everything</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">seed_everything</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span><span class="mf">0.4963</span><span class="p">,</span> <span class="mf">0.7682</span><span class="p">,</span> <span class="mf">0.0885</span><span class="p">,</span> <span class="mf">0.1320</span><span class="p">,</span> <span class="mf">0.3074</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">seed_everything</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span><span class="mf">0.7682</span><span class="p">,</span> <span class="mf">0.0885</span><span class="p">,</span> <span class="mf">0.1320</span><span class="p">,</span> <span class="mf">0.3074</span><span class="p">,</span> <span class="mf">0.6341</span><span class="p">])</span>
</code></pre></div><p data-pid="ZK_YEmFV">我们再反思一下，模型测试中唯一不敢确定的就是DataLoader了。按照常规设置，训练时一般使用带shuffle的DataLoader，而测试时使用不带shuffle的，那既然不带shuffle，为啥还是会出错？我们写一个最小样例复现一下这个问题：</p><div class="highlight"><pre><code class="language-python"><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">TensorDataset</span><span class="p">,</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="n">seed_everything</span>

<span class="n">seed_everything</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="c1"># tensor([0.5263, 0.2437, 0.5846, 0.0332, 0.1387])</span>

<span class="n">seed_everything</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
    <span class="k">pass</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="n">tensor</span><span class="p">([</span><span class="mf">0.5846</span><span class="p">,</span> <span class="mf">0.0332</span><span class="p">,</span> <span class="mf">0.1387</span><span class="p">,</span> <span class="mf">0.2422</span><span class="p">,</span> <span class="mf">0.8155</span><span class="p">])</span>
</code></pre></div><p data-pid="CcofPqQL">然后研读一下Pytorch中DataLoader的源码就会发现问题所在。</p><p data-pid="vVT0TX2-">Python的in操作符会先调用后面的迭代器中的__iter__魔法函数。每次遍历数据集时，DataLoader的__iter__()都会返回一个新的生成器，无论上次遍历是否中途break，它都会重新从头开始。这个生成器底层有一个_index_sampler，shuffle设置为真时它使用BatchSampler(RandomSampler)，随机抽取batchsize个数据索引，如果为假则使用BatchSampler(SequentialSampler)顺序抽取。</p><p data-pid="omKGSmPK">上面所说的生成器的基类叫做_BaseDataLoaderIter，在它的初始化函数中唯一调用了一次随机数函数，用以确定全局随机数种子。</p><div class="highlight"><pre><code class="language-python"><span></span><span class="k">class</span> <span class="nc">_BaseDataLoaderIter</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="o">...</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_base_seed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="n">generator</span><span class="o">=</span><span class="n">loader</span><span class="o">.</span><span class="n">generator</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="o">...</span>
</code></pre></div><p data-pid="swc4lW5j">这里的_base_seed将会是一个长整型标量随机数。这个种子会在哪里使用呢？目前只在其子类_MultiProcessingDataLoaderIter中使用。当我们将DataLoader的worker数量设置为大于0时，将使用多进程的方式加载数据。在这个子类的初始化函数中会新建n个进程，然后将_base_seed作为进程参数传入：</p><div class="highlight"><pre><code class="language-python"><span></span><span class="o">...</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">multiprocessing_context</span><span class="o">.</span><span class="n">Process</span><span class="p">(</span>
    <span class="n">target</span><span class="o">=</span><span class="n">_utils</span><span class="o">.</span><span class="n">worker</span><span class="o">.</span><span class="n">_worker_loop</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_dataset_kind</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dataset</span><span class="p">,</span> <span class="n">index_queue</span><span class="p">,</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_worker_result_queue</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_workers_done_event</span><span class="p">,</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_auto_collation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collate_fn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_drop_last</span><span class="p">,</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_base_seed</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_worker_init_fn</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_workers</span><span class="p">,</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_persistent_workers</span><span class="p">))</span>
<span class="n">w</span><span class="o">.</span><span class="n">daemon</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">w</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
<span class="o">...</span>
</code></pre></div><p data-pid="XI5H9DwC">worker进程内部实际使用到这个种子的地方如下</p><div class="highlight"><pre><code class="language-python"><span></span><span class="k">def</span> <span class="nf">_worker_loop</span><span class="p">(</span><span class="n">dataset_kind</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">index_queue</span><span class="p">,</span> <span class="n">data_queue</span><span class="p">,</span> <span class="n">done_event</span><span class="p">,</span>
                 <span class="n">auto_collation</span><span class="p">,</span> <span class="n">collate_fn</span><span class="p">,</span> <span class="n">drop_last</span><span class="p">,</span> <span class="n">base_seed</span><span class="p">,</span> <span class="n">init_fn</span><span class="p">,</span> <span class="n">worker_id</span><span class="p">,</span>
                 <span class="n">num_workers</span><span class="p">,</span> <span class="n">persistent_workers</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="n">seed</span> <span class="o">=</span> <span class="n">base_seed</span> <span class="o">+</span> <span class="n">worker_id</span>
    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">HAS_NUMPY</span><span class="p">:</span>
        <span class="n">np_seed</span> <span class="o">=</span> <span class="n">_generate_state</span><span class="p">(</span><span class="n">base_seed</span><span class="p">,</span> <span class="n">worker_id</span><span class="p">)</span>
        <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">np_seed</span><span class="p">)</span>
    <span class="o">...</span>
</code></pre></div><p data-pid="9MYs9Pjt">这些操作将会在init_fn之前，控制每个进程起始的随机数种子。但据我观察这些操作已经在RandomSampler初始化之后了，所以不知道它们是怎么解决<a href="https://zhuanlan.zhihu.com/p/523239005" class="internal" target="_blank">serendipity：可能95%的人还在犯的PyTorch错误</a>这篇文章提到的低版本PyTorch中DataLoader随机序列重复的问题的。但这些不是重点，按照PyTorch向后兼容的设计理念，这里无论谁继承_BaseDataLoaderIter这个基类，无论子类是否用到_base_seed这个种子，随机数函数都是会被调用的。调用关系梳理如下：</p><div class="highlight"><pre><code class="language-python"><span></span><span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">DataLoader</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="k">pass</span>
<span class="c1"># in操作符会调用如下</span>
<span class="n">DataLoader</span><span class="p">()</span>
    <span class="n">DataLoader</span><span class="o">.</span><span class="n">self</span><span class="o">.</span><span class="fm">__iter__</span><span class="p">()</span>
        <span class="n">DataLoader</span><span class="o">.</span><span class="n">self</span><span class="o">.</span><span class="n">_get_iterator</span><span class="p">()</span>
            <span class="n">_MultiProcessingDataLoaderIter</span><span class="p">(</span><span class="n">DataLoader</span><span class="o">.</span><span class="n">self</span><span class="p">)</span>
                <span class="n">_BaseDataLoaderIter</span><span class="p">(</span><span class="n">DataLoader</span><span class="o">.</span><span class="n">self</span><span class="p">)</span>
                    <span class="n">_BaseDataLoaderIter</span><span class="o">.</span><span class="n">self</span><span class="o">.</span><span class="n">_base_seed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
                        <span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="n">generator</span><span class="o">=</span><span class="n">DataLoader</span><span class="o">.</span><span class="n">generator</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="c1"># 一般来说generator是None，我们不指定，random_没有from和to时，会取数据类型最大范围，这里相当于随机生成一个大整数</span>
</code></pre></div><p data-pid="gStjr2HU">那么如何解决呢？我尝试过使用DataLoader的generator参数去指定一个随机数序列，但发现这样只会屏蔽遍历数据操作以外的随机数调用的影响。也就是说，这种情况下，只要调用DataLoader的次数变化，还是无法复现。那么最简单有效的方法就是在每次DataLoader的in操作调用之前都固定一下随机数种子。</p><div class="highlight"><pre><code class="language-python"><span></span><span class="k">def</span> <span class="nf">stable</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
    <span class="n">seed_everything</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dataloader</span>

<span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">stable</span><span class="p">(</span><span class="n">DataLoader</span><span class="p">(</span><span class="o">...</span><span class="p">),</span> <span class="n">seed</span><span class="p">):</span>
    <span class="k">pass</span>
</code></pre></div><p data-pid="aKb5cMbq"><b>这里需要格外注意的是，stable函数会使训练时每个epoch内部的shuffle规律相同！</b>之前我们提到shuffle训练集可以减轻模型过拟合，是至关重要的，当每个epoch内部第i个batch的内容都对应相同时，模型会训不起来。所以，一个简单的技巧，在传入随机数种子的时候加上一个epoch序号。</p><div class="highlight"><pre><code class="language-python"><span></span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">MAX_EPOCH</span><span class="p">):</span>  <span class="c1"># training</span>
    <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">stable</span><span class="p">(</span><span class="n">DataLoader</span><span class="p">(</span><span class="o">...</span><span class="p">),</span> <span class="n">seed</span> <span class="o">+</span> <span class="n">epoch</span><span class="p">):</span>
        <span class="k">pass</span>
</code></pre></div><p data-pid="Rj8QWE_O">这时随机数种子的设定和in操作绑定成了类似的原子操作，所有涉及到random()调用的新增代码都不会影响到准确率曲线的复现了。</p><p data-pid="kVZ6Y7lC"><b>#3 本文未讨论的其他随机性</b></p><p data-pid="Zx8t1Xmt">按照本文所说的方法就一定能实现可复现性了吗？不一定。因为随机性还体现在方方面面：比如超参数，当我们改变DataLoader的worker数量时，显然会引入随机性；比如系统配置，同样的代码在不同架构和精度的CPU、GPU上运行，底层优化或者截断误差都可能带来随机性。在我之前做硬件工作的时候，电池电量不同都可能导致同一个程序在同一块板子上跑出完全不同的结果。可复现性其实是学术界广泛关注的一个专门的研究领域，本文只是为日常模型训练提供一些直观的技巧。</p><hr><p data-pid="eV0FG2j7"><b>2022年7月1日更新</b></p><p data-pid="PkWIxtuo">评论区有位朋友指出PyTorch官方给出了一种消除随机性的方法，参考<a href="http://link.zhihu.com/?target=https%3A//pytorch.org/docs/stable/notes/randomness.html%23dataloader" class=" wrap external" target="_blank" rel="nofollow noreferrer">Reproducibility-DataLoader</a>，以及后续的改进封装<a href="http://link.zhihu.com/?target=https%3A//github.com/Zeqiang-Lai/torchlight/blob/ef9d6a872b21127a58cf9985c226ec9603a0f19a/torchlight/nn/reproducibility.py%23L35" class=" wrap external" target="_blank" rel="nofollow noreferrer">SeedDataLoader</a>，但这份代码并不能解决本文提到的第二种随机性。我先来说说这份代码本来是用来解决什么问题的。</p><p data-pid="BoXyaDQu">根据官方文档给出的说明，PyTorch的Data Loader采用了<a href="http://link.zhihu.com/?target=https%3A//pytorch.org/docs/stable/data.html%23data-loading-randomness" class=" wrap external" target="_blank" rel="nofollow noreferrer">Randomness in multi-process data loading</a>的reseed算法，即在每次调用data loader实例的__iter__()方法时，会通过多进程迭代器创建n个worker，每个worker的随机种子由主进程传入base_seed，然后通过base_seed+worker_id的方式生成子进程专属的种子。</p><p data-pid="CH_CUQk6">上面这些文档应该没有及时更新，只提到这里设置了torch自己的随机数，但从本文前面给出的源码中可以看到，它也包含了对random库和numpy.random库的随机种子的初始化。但假如我们定制化地改写了DataLoader，用到与这些库独立的其他随机算法库，且我们的程序使用fork方法创建子进程时，如果不加设置，这些子进程的随机性就会完全相同。</p><p data-pid="ABmCX9gE">因此，PyTorch设置了worker_init_fn这个函数，我们可以定制化地在这个函数中设定额外的随机函数库的种子。将这个函数作为DataLoader的参数传入，然后它会在前面所说的torch自带的子进程随机种子初始化的后面执行，也就是说，除了补充额外的种子，我们也可以选择覆盖掉torch的设定。</p><p data-pid="KU41cjXM">这个问题在本文提到的<a href="https://zhuanlan.zhihu.com/p/523239005" class="internal" target="_blank">serendipity：可能95%的人还在犯的PyTorch错误</a>这篇文章中已经详细讲解，在1.9以上的PyTorch版本中已经修复。由于刚刚提到torch已经在子进程中做过常用随机库的初始化了，所以使用文档中给出的例程是多此一举。可能是本文用来验证问题的实验不清晰，这里再给一段验证代码，来说明以上文档中的例程不能解决我们的问题。</p><div class="highlight"><pre><code class="language-python"><span></span><span class="kn">import</span> <span class="nn">random</span><span class="o">,</span> <span class="nn">numpy</span><span class="o">,</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">TensorDataset</span>
<span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="n">seed_everything</span>

<span class="n">seed_everything</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">NUM_WORKERS</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>

<span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span>
<span class="n">g</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">seed_worker</span><span class="p">(</span><span class="n">worker_id</span><span class="p">):</span>
    <span class="n">worker_seed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">initial_seed</span><span class="p">()</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">**</span> <span class="mi">32</span>
    <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">worker_seed</span><span class="p">)</span>
    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">worker_seed</span><span class="p">)</span>

<span class="n">train_data</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">NUM_WORKERS</span><span class="p">,</span>
    <span class="n">worker_init_fn</span><span class="o">=</span><span class="n">seed_worker</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="n">NUM_WORKERS</span><span class="p">,</span>
    <span class="n">worker_init_fn</span><span class="o">=</span><span class="n">seed_worker</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">test</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">:</span>
        <span class="k">pass</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_data</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
        <span class="k">break</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">"__main__"</span><span class="p">:</span>
    <span class="c1"># case 1</span>
    <span class="c1"># Result: tensor([0.8174, 0.1753, 0.5049, 0.8947, 0.8472, 0.2588, 0.2568, 0.7127])</span>
    <span class="n">train</span><span class="p">()</span>
    
    <span class="c1"># case 2</span>
    <span class="c1"># Result: tensor([0.8947, 0.1753, 0.7802, 0.2161, 0.9094, 0.7335, 0.3245, 0.6152])</span>
    <span class="n">test</span><span class="p">()</span>
    <span class="n">train</span><span class="p">()</span>
</code></pre></div><p data-pid="LM-9cPTX">分别运行case 1和2，会发现结果并不相同。</p><p data-pid="lpxlAoqP">除此之外，通过实验我还发现，程序不变动的前提下修改num_workers这个超参并不会影响结果，不知道底层是如何让多个worker有序分工的。我分析出来之后会持续更新，欢迎大家关注，也欢迎大家继续与我讨论。</p>