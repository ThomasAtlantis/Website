<p data-pid="5kHHNpPq">联邦学习的意义是一体两面的，如果以数据总量不变为前提，联邦就是把本应收集起来的数据保留在用户本地，相当于增强隐私保护能力；如果以用户数据不能上传为前提，那么联邦就是增大了训练数据量。</p><p data-pid="EByvWYaO">政策上一般不会对隐私有着模棱两可的定义，不会去考察一个算法的 DP 性质，只能规定数据是否上传（当然考虑到大模型领域的军备竞赛，这种政策在当今中国必然不会出现）。<b>而联邦训练想要防护的，也主要是原始数据在通信链路传输时被窃听，而不是恶意终端或恶意服务器。</b>我们说现有的联邦学习其实不安全，因为参与通信的模型梯度可能遭遇反演攻击。</p><p data-pid="o3IiZb6T"><b>但大模型必然不会以模型全量参数作为通信内容</b>，这样通信开销、计算开销都承受不了。现有的方法大多会冻结大模型，从大模型中蒸馏或抽取出一个小的代理模型，可能是某种 Adapter，例如 LoRA。然后<b>客户端在这个代理模型上联邦微调。从这种低秩模型中套取出训练语料要困难得多。</b></p><p data-pid="DCYb8xLU">从大模型的发展来看，公域数据逐渐枯竭，私域数据是推动 scaling law 冲破数据瓶颈的必要手段，而联邦可以提供一种思路。同时，分布式甚至众包的训练方式也可以一定程度上缓解 scaling law 的算力瓶颈。实际上，在大模型爆发之前，一些联邦学习研究就不再以隐私作为前提了，而是在优化开销。</p><p data-pid="NnoG4U-Q">目前我们也在做一些推理上 Prefilling-Decoding 分离的工作，也是想利用边缘设备去缓解服务器压力，好让服务器算力被真正用在刀刃上。相信联邦学习也会有这方面的考虑。</p><p></p>