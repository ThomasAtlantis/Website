<p data-pid="JnfkocYL">注意 <img src="https://www.zhihu.com/equation?tex=D_%7BKL%7D%28P%5CVert+Q%29" alt="D_{KL}(P\Vert Q)" eeimg="1"> 是非对称的，所以仅用「重叠」去描述并不准确，它存在有几个前提：</p><ol><li data-pid="vHjF8T4F"><img src="https://www.zhihu.com/equation?tex=P" alt="P" eeimg="1"> 和 <img src="https://www.zhihu.com/equation?tex=Q" alt="Q" eeimg="1"> 都要求是概率分布，即各项非负且总和为1；</li><li data-pid="jM9UvqvX">预测分布 <img src="https://www.zhihu.com/equation?tex=Q" alt="Q" eeimg="1"> 能够完全「覆盖」真实分布 <img src="https://www.zhihu.com/equation?tex=P" alt="P" eeimg="1"> ，即对于任意 <img src="https://www.zhihu.com/equation?tex=P%28i%29%3E0" alt="P(i)&gt;0" eeimg="1"> 要求 <img src="https://www.zhihu.com/equation?tex=Q%28i%29%3E0" alt="Q(i)&gt;0" eeimg="1"> 。</li></ol><p data-pid="mqXnTBlT">只要不满足第二个条件，KL散度就趋于无穷大。</p><p data-pid="XqIAlxjK"><img src="https://www.zhihu.com/equation?tex=0%5Clog%5Cfrac%7B0%7D%7BQ%28i%29%7D%3D0%2C+%5C+P%28i%29%5Clog%5Cfrac%7BP%28i%29%7D%7B0%7D%3D%2B%5Cinfty%5C%5C" alt="0\log\frac{0}{Q(i)}=0, \ P(i)\log\frac{P(i)}{0}=+\infty\\" eeimg="1"></p><p data-pid="VopFbJzV">从信息论的角度解释就很有意思：</p><blockquote data-pid="1N2QaKQP">KL散度是指使用预测分布Q而不是真实分布P来编码一些数据所需的额外比特数量。如果P中缺少Q中的一个字母，也不会发生什么糟糕的事情，这只意味着你在浪费比特，这将在KL散度值中反映。但是，如果P中的一个字母没有出现在Q中，那么宇宙中的所有比特都不足以对数据进行编码——你正试图用「橙子」去编码一个「苹果」！</blockquote><h3>连续分布</h3><p data-pid="uynCW0do">对于 <img src="https://www.zhihu.com/equation?tex=%5B-%5Cinfty%2C+%2B+%5Cinfty%5D" alt="[-\infty, + \infty]" eeimg="1"> 上定义的连续分布，只需要真实分布 <img src="https://www.zhihu.com/equation?tex=P" alt="P" eeimg="1"> 比预测分布 <img src="https://www.zhihu.com/equation?tex=Q" alt="Q" eeimg="1"> 长尾（矮胖）一些，很容易得出两个分布的KL散度为无穷。举个简单的例子，真实分布 <img src="https://www.zhihu.com/equation?tex=P" alt="P" eeimg="1"> 为柯西分布而预测分布 <img src="https://www.zhihu.com/equation?tex=Q" alt="Q" eeimg="1"> 为高斯分布，即：</p><p data-pid="URhcsvHw"><img src="https://www.zhihu.com/equation?tex=P%3D%5Cfrac%7B1%7D%7B%5Cpi%7D%5Cfrac%7B1%7D%7B%281%2Bx%29%5E2%2B1%7D%2C+%5C+Q%3D%5Cfrac%7B1%7D%7B%5Csqrt+%7B2%5Cpi%7D%7De%5E%7B-%5Cfrac%7Bx%5E2%7D%7B2%7D%7D%5C%5C" alt="P=\frac{1}{\pi}\frac{1}{(1+x)^2+1}, \ Q=\frac{1}{\sqrt {2\pi}}e^{-\frac{x^2}{2}}\\" eeimg="1"></p><p data-pid="5TcCqS8y">证明过程见文末。用<a href="https://link.zhihu.com/?target=https%3A//www.geogebra.org/calculator" class=" wrap external" target="_blank" rel="nofollow noreferrer">Geogebra</a>画一下函数图像，能直观地看出问题。因为 <img src="https://www.zhihu.com/equation?tex=Q" alt="Q" eeimg="1"> 的分布比 <img src="https://www.zhihu.com/equation?tex=P" alt="P" eeimg="1"> 更加集中，所以在横坐标趋于无穷时， <img src="https://www.zhihu.com/equation?tex=Q" alt="Q" eeimg="1"> 更快地趋近于0，从而近似于不满足上述第二个条件。</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/50/v2-b570f6426b7407a656c88116d26b017b_720w.jpg?source=c8b7c179" data-size="normal" data-rawwidth="779" data-rawheight="208" data-original-token="v2-ddc6471e9b894094ad6b38bca6aaba26" data-default-watermark-src="https://picx.zhimg.com/50/v2-2892d6115338bf34021eae84c67440ec_720w.jpg?source=c8b7c179" class="origin_image zh-lightbox-thumb" width="779" data-original="https://picx.zhimg.com/v2-b570f6426b7407a656c88116d26b017b_r.jpg?source=c8b7c179"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='779'%20height='208'&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="779" data-rawheight="208" data-original-token="v2-ddc6471e9b894094ad6b38bca6aaba26" data-default-watermark-src="https://picx.zhimg.com/50/v2-2892d6115338bf34021eae84c67440ec_720w.jpg?source=c8b7c179" class="origin_image zh-lightbox-thumb lazy" width="779" data-original="https://picx.zhimg.com/v2-b570f6426b7407a656c88116d26b017b_r.jpg?source=c8b7c179" data-actualsrc="https://picx.zhimg.com/50/v2-b570f6426b7407a656c88116d26b017b_720w.jpg?source=c8b7c179"><figcaption>真实分布更加平缓，整体来看差别不大</figcaption></figure><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/50/v2-98a374bc66dbdf569568c152e7e01919_720w.jpg?source=c8b7c179" data-size="normal" data-rawwidth="804" data-rawheight="294" data-original-token="v2-e3b1d8f57abe0aff6725c66e6d4c40cc" data-default-watermark-src="https://pic1.zhimg.com/50/v2-61c373c0835c5aab9ee0be6259d57bfb_720w.jpg?source=c8b7c179" class="origin_image zh-lightbox-thumb" width="804" data-original="https://pica.zhimg.com/v2-98a374bc66dbdf569568c152e7e01919_r.jpg?source=c8b7c179"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='804'%20height='294'&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="804" data-rawheight="294" data-original-token="v2-e3b1d8f57abe0aff6725c66e6d4c40cc" data-default-watermark-src="https://pic1.zhimg.com/50/v2-61c373c0835c5aab9ee0be6259d57bfb_720w.jpg?source=c8b7c179" class="origin_image zh-lightbox-thumb lazy" width="804" data-original="https://pica.zhimg.com/v2-98a374bc66dbdf569568c152e7e01919_r.jpg?source=c8b7c179" data-actualsrc="https://pic1.zhimg.com/50/v2-98a374bc66dbdf569568c152e7e01919_720w.jpg?source=c8b7c179"><figcaption>在x趋于正无穷和负无穷时真实分布始终是高于预测分布的</figcaption></figure><h3>离散分布</h3><p data-pid="X608fqnY">机器学习中更常见的是维度有限的离散分布，这时只需要有一个维度没有覆盖住，结果就是无穷。一个简单的实验如下：我们先生成一个长度为8的正数向量，然后分别在它前面和后面填充相同维度的0值，会发现尽管两个分布只错开了一点，KL散度已经失效。</p><div class="highlight"><pre><code class="language-python"><span></span><span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">special</span><span class="p">,</span> <span class="n">stats</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">shift</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">shared</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shift</span><span class="p">),</span> <span class="n">shared</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">shared</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shift</span><span class="p">)])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">special</span><span class="o">.</span><span class="n">rel_entr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>  <span class="c1"># inf</span>
<span class="nb">print</span><span class="p">(</span><span class="n">special</span><span class="o">.</span><span class="n">rel_entr</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>  <span class="c1"># inf</span>
</code></pre></div><p data-pid="GHDok6Zu">而我们常认为的两个分布完全一致对于KL散度为零其实是充分不必要条件。只要在真实分布有值的地方，预测分布与其一致即可。我们把预测分布某一维置零，作为真实分布，二者KL散度仍为0。</p><div class="highlight"><pre><code class="language-python"><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="nb">print</span><span class="p">(</span><span class="n">special</span><span class="o">.</span><span class="n">rel_entr</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>  <span class="c1"># 0.0</span>
</code></pre></div><h3>解决方法</h3><p data-pid="08aJdw4n">那么如何在包含0值的情况下度量两个分布的距离？</p><ol><li data-pid="r3SdtpjH">使用其他距离度量代替KL散度，如L范数距离、Wasserstein距离和Hellinger距离；</li><li data-pid="EH3EJpEK">使用smoothing方法将原值映射到一个非零空间。最常见的方法是Laplace平滑化，又叫加一平滑，即分子加1，分母加上维度数量。对于频次统计数据 <img src="https://www.zhihu.com/equation?tex=f_i%3Dn_i%2F%5Csum_i+n_i" alt="f_i=n_i/\sum_i n_i" eeimg="1"> ，平滑后为 <img src="https://www.zhihu.com/equation?tex=f%27_i%3D%28n_i+%2B1%29%2F%28%5Csum_i%5Ek+n_i%2Bk%29" alt="f'_i=(n_i +1)/(\sum_i^k n_i+k)" eeimg="1"> 。另外也可以采用用和先验分布进行凸组合平滑，例如 <img src="https://www.zhihu.com/equation?tex=f_i%27%3D%5Calpha+U%28i%29%2B%281-%5Calpha%29f_i" alt="f_i'=\alpha U(i)+(1-\alpha)f_i" eeimg="1"> ，其中 <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="\alpha" eeimg="1"> 是权重系数， <img src="https://www.zhihu.com/equation?tex=U%28i%29" alt="U(i)" eeimg="1"> 是假设的均匀分布。工程上更常见的方法就是简单地为所有值加上一个很小的 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"> ，其实也可以看做是与均匀分布进行加权组合。</li><li data-pid="jSU6Lihq">甚至很多人只是把出现零值的维度都去掉，这么做不太科学，并不推荐。</li></ol><hr><p><br></p><h3>附录：柯西分布与高斯分布KL散度为0的证明过程</h3><p data-pid="XvPbJHrw"><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+D_%7BKL%7D%28P%5CVert+Q%29%26%3D%5Cint+%5BP%28x%29%5Clog+P%28x%29+-P%28x%29%5Clog+Q%28x%29%5Ddx%5C%5C+%26%3D%5Cint+P%28x%29%5Clog+P%28x%29dx+%2B%5Cint+P%28x%29%28%5Cfrac%7B1%7D%7B2%7D%5Clog2%5Cpi%2B%5Cfrac%7Bx%5E2%7D%7B2%7D%29dx%5C%5C+%26%3D%5Cfrac%7B1%7D%7B2%7D%5Clog2%5Cpi%2B%5Cint+P%28x%29%28%5Clog+P%28x%29+%2B+%5Cfrac%7Bx%5E2%7D%7B2%7D%29dx%5C%5C+%26%3D%5Cint+P%28x%29%28%5Cfrac%7Bx%5E2%7D%7B2%7D-%5Clog%28%281%2Bx%29%5E2+%2B+1%29%29dx+%2B+%5Cfrac%7B1%7D%7B2%7D%5Clog2%5Cpi+-+%5Clog+%5Cpi%5C%5C+%26%3DM%2BC+%5Cend%7Balign%2A%7D%5C%5C" alt="\begin{align*} D_{KL}(P\Vert Q)&amp;=\int [P(x)\log P(x) -P(x)\log Q(x)]dx\\ &amp;=\int P(x)\log P(x)dx +\int P(x)(\frac{1}{2}\log2\pi+\frac{x^2}{2})dx\\ &amp;=\frac{1}{2}\log2\pi+\int P(x)(\log P(x) + \frac{x^2}{2})dx\\ &amp;=\int P(x)(\frac{x^2}{2}-\log((1+x)^2 + 1))dx + \frac{1}{2}\log2\pi - \log \pi\\ &amp;=M+C \end{align*}\\" eeimg="1">原式拆解为积分项 <img src="https://www.zhihu.com/equation?tex=M" alt="M" eeimg="1"> 和常数项 <img src="https://www.zhihu.com/equation?tex=C" alt="C" eeimg="1"> 。我们对积分项 <img src="https://www.zhihu.com/equation?tex=M" alt="M" eeimg="1"> 进行放缩：</p><p data-pid="g8SlI81O"><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+M%26%3E%5Cint+P%28x%29%28%5Cfrac%7Bx%5E2%7D%7B2%7D-%5Csqrt%7B%28x%2B1%29%5E2%2B1%7D%2B%5Cfrac%7B1%7D%7B%5Csqrt%7B%28x%2B1%29%5E2%2B1%7D%7D%29dx%5C%5C+%26%3E%5Cint+P%28x%29%28%5Cfrac%7Bx%5E2%7D%7B2%7D-x-2%29dx+%2B%5Cint%5Csqrt+%5Cpi+P%28x%29%5E%7B%5Cfrac%7B3%7D%7B2%7D%7Ddx%5C%5C+%26%3D%5Cfrac%7B%5Cpi%7D%7B2%7D%5Cint+%5Cfrac%7Bx%5E2-2x-4%7D%7Bx%5E2%2B2x%2B2%7Ddx%2B%5Cint%5Csqrt+%5Cpi+P%28x%29%5E%7B%5Cfrac%7B3%7D%7B2%7D%7Ddx%5C%5C+%5Cend%7Balign%2A%7D%5C%5C" alt="\begin{align*} M&amp;&gt;\int P(x)(\frac{x^2}{2}-\sqrt{(x+1)^2+1}+\frac{1}{\sqrt{(x+1)^2+1}})dx\\ &amp;&gt;\int P(x)(\frac{x^2}{2}-x-2)dx +\int\sqrt \pi P(x)^{\frac{3}{2}}dx\\ &amp;=\frac{\pi}{2}\int \frac{x^2-2x-4}{x^2+2x+2}dx+\int\sqrt \pi P(x)^{\frac{3}{2}}dx\\ \end{align*}\\" eeimg="1">显然，当 <img src="https://www.zhihu.com/equation?tex=x%3C1-%5Csqrt+5" alt="x&lt;1-\sqrt 5" eeimg="1"> 和 <img src="https://www.zhihu.com/equation?tex=x%3E1%2B%5Csqrt+5" alt="x&gt;1+\sqrt 5" eeimg="1"> 时被积函数都是大于0的，在此之间积分有界。而当 <img src="https://www.zhihu.com/equation?tex=x" alt="x" eeimg="1"> 趋近于正无穷或负无穷时，被积函数趋近于 <img src="https://www.zhihu.com/equation?tex=%5Cpi%2F2" alt="\pi/2" eeimg="1"> ，因此积分的值趋于无穷。那么 <img src="https://www.zhihu.com/equation?tex=M%2BC" alt="M+C" eeimg="1"> 自然也趋近于无穷。</p><p></p>