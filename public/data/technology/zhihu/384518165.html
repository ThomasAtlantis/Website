<p data-pid="iurYNW4b">本文是<a href="https://zhuanlan.zhihu.com/p/384463998" class="internal" target="_blank">清川：白嫖华为云耀服务器配置Hadoop伪集群</a>的后续。这一系列文章主要记录了并行计算课程期末大作业的实验过程，也作为对分布式计算的入门级涉猎。作业题目要求如下：</p><blockquote data-pid="EKodbD9I"><a href="http://link.zhihu.com/?target=http%3A//ram-n.github.io/weatherData/" class=" wrap external" target="_blank" rel="nofollow noreferrer">weatherData</a>是一个从网站获取温度的项目。要求使用MapReduce实现一段程序，分别获得数据集文件夹下所有文件中的最高和最低温度的统计数据。</blockquote><p data-pid="BTUAWjhq">这个开源项目是4年前的了，感觉空有其表，几乎无人维护，issue已经叠成山。好像也不是很出名，不知道这是不是老师以前的某个学生开发的，强制植入到课程里了。</p><h2>1 数据集获取与预处理</h2><p data-pid="-Q14bueE">我们从官方网站中下载该项目：<a href="http://link.zhihu.com/?target=https%3A//codeload.github.com/Ram-N/weatherData/legacy.tar.gz/master" class=" wrap external" target="_blank" rel="nofollow noreferrer">下载链接</a>，解压后得到的文件结构大致如下：</p><div class="highlight"><pre><code class="language-text"><span></span>Ram-N-weatherData-4326bdc/
├── data
│   ├── IntlWxStations.rda
│   ~
│   └── USAirportWeatherStations.rda
├── DESCRIPTION
├── man
│   ├── checkDataAvailabilityForDateRange.Rd
│   ~
│   └── weatherData-package.Rd
├── NAMESPACE
├── NEWS.md
├── R
│   ├── data_description.R
│   ~
│   └── wrapper_functions.R
├── README.md
├── tests
│   ├── testthat
│   │   ├── test-CustomColumns.R
│   │   ~
│   │   └── test-Wrappers.R
│   └── testthat.R
└── weatherData.Rproj
</code></pre></div><p data-pid="Pa9zvaQg">可以猜测出这应该是个R语言的project，之所以是猜测是因为我从来没见过<b>R语言</b>「为数学研究工作者设计的一种数学编程语言，主要用于统计分析、绘图、数据挖掘，与 C 语言都是贝尔实验室的研究成果」网站中给出了一些R语言的语法示例，可以发现这个库按理说是可以直接计算最高、最低气温的。课程作业的意图应该是让我们仅仅使用这个数据集，用MapReduce的方法重新实现这个功能。</p><p data-pid="4OCryzaF">数据文件存储在<code>data</code>文件夹下，是一些二进制的<code>.rda</code> 文件，下面我们使用R语言将它们转存成为Java可以读取的<code>.csv</code>文本文件。首先在Linux服务器上使用以下命令安装R语言，参考我交镜像库中的描述：<a href="http://link.zhihu.com/?target=https%3A//mirrors.sjtug.sjtu.edu.cn/cran/" class=" wrap external" target="_blank" rel="nofollow noreferrer">Ubuntu Packages For R - Brief Instructions</a>。</p><div class="highlight"><pre><code class="language-bash"><span></span>apt install --no-install-recommends r-base
</code></pre></div><p data-pid="JASiH-Wk">之后我们可以使用两种方法执行R语言：</p><ul><li data-pid="w6Sa8luR"><b>动态交互，</b>在命令行中输入<code>R</code>进入交互环境，使用<code>q()</code> 退出。</li><li data-pid="IVAifqA7"><b>执行文件，</b>编写<code>x.R</code> 文件，使用<code>Rscript x.R</code>执行。 </li></ul><p data-pid="tz1w6ZqQ">由于R也是一种解释运行的语言，所以感觉和Python的执行方式很像。经过半个小时的照猫画虎，参考<a href="http://link.zhihu.com/?target=https%3A//www.runoob.com/r/r-tutorial.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">R语言教程 - 菜鸟教程</a>实现了以下程序。程序扫描data文件夹下的所有文件，将其读入并导出为csv。rda文件的读取也很奇怪，中文网站上很少有讲明白的，参考<a href="http://link.zhihu.com/?target=https%3A//stackoverflow.com/questions/7270544/how-to-see-data-from-rdata-file" class=" wrap external" target="_blank" rel="nofollow noreferrer">SO大神回答</a>。</p><div class="highlight"><pre><code class="language-cpp"><span></span><span class="n">path</span> <span class="o">=</span> <span class="s">"Ram-N-weatherData-4326bdc/data/"</span>
<span class="n">list</span><span class="p">.</span><span class="n">files</span><span class="p">(</span><span class="n">path</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">file_list</span>
<span class="k">for</span> <span class="p">(</span><span class="n">file</span> <span class="n">in</span> <span class="n">file_list</span><span class="p">)</span> <span class="p">{</span>
	<span class="n">load</span><span class="p">(</span><span class="n">paste</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">file</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s">""</span><span class="p">),</span> <span class="k">new</span><span class="p">.</span><span class="n">env</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">ex</span><span class="p">)</span>
	<span class="n">names</span><span class="p">(</span><span class="n">ex</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">name</span>
	<span class="n">get</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">envir</span><span class="o">=</span><span class="n">ex</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">data</span>
	<span class="n">max</span><span class="p">(</span><span class="n">data</span><span class="err">$</span><span class="n">Temperature</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">max_</span>
	<span class="n">min</span><span class="p">(</span><span class="n">data</span><span class="err">$</span><span class="n">Temperature</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">min_</span>
	<span class="n">print</span><span class="p">(</span><span class="n">sprintf</span><span class="p">(</span><span class="s">"%15s MIN: %5.1f MAX: %5.1f"</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">min_</span><span class="p">,</span> <span class="n">max_</span><span class="p">))</span>
	<span class="n">write</span><span class="p">.</span><span class="n">csv</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">paste</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s">".csv"</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s">""</span><span class="p">))</span>
<span class="p">}</span>
</code></pre></div><p data-pid="zlo4_MoB">注意以下三个文件中储存的内容比较混乱，为了简洁我们直接删掉它们</p><div class="highlight"><pre><code class="language-bash"><span></span>IntlWxStations.rda USAirportWeatherStations.rda SFO2013Summarized.rda

head -n <span class="m">4</span> IntlWxStation.csv
---------------------------------------------------------------------------------------------
<span class="s2">""</span>,<span class="s2">"CD..STATION.........ICAO..IATA..SYNOP...LAT.....LONG...ELEV...M..N..V..U..A..C"</span>
<span class="s2">"1"</span>,<span class="s2">"USA AK ADAK NAS         PADK  ADK   70454  51 53N  176 39W    4   X     T          7 US"</span>
<span class="s2">"2"</span>,<span class="s2">"USA AK AKHIOK           PAKH  AKK          56 56N  154 11W   14   X                8 US"</span>
<span class="s2">"3"</span>,<span class="s2">"USA AK AMBLER           PAFM  AFM          67 06N  157 51W   88   X                7 US"</span>

head -n <span class="m">4</span> USAirportWeatherStations.csv
---------------------------------------------------------------------------------------------
<span class="s2">""</span>,<span class="s2">"Station"</span>,<span class="s2">"State"</span>,<span class="s2">"airportCode"</span>,<span class="s2">"Lat"</span>,<span class="s2">"Lon"</span>,<span class="s2">"Elevation"</span>,<span class="s2">"WMO"</span>
<span class="s2">"1"</span>,<span class="s2">"Central"</span>,<span class="s2">"AK"</span>,<span class="s2">"PARL"</span>,65.57,-144.8,292,99999
<span class="s2">"2"</span>,<span class="s2">"Atka"</span>,<span class="s2">"AK"</span>,<span class="s2">"PAAK"</span>,52.22,-174.2,17,99999
<span class="s2">"3"</span>,<span class="s2">"Buckland"</span>,<span class="s2">"AK"</span>,<span class="s2">"PABL"</span>,65.99,-161.12,0,99999

head -n <span class="m">4</span> SFO2013Summarized.csv
---------------------------------------------------------------------------------------------
<span class="c1"># 这里存储了SFO2013每天的最高温度、最低温度和平均温度，实际在SFO2013.csv中已经包含</span>
<span class="s2">""</span>,<span class="s2">"Date"</span>,<span class="s2">"Max_TemperatureF"</span>,<span class="s2">"Mean_TemperatureF"</span>,<span class="s2">"Min_TemperatureF"</span>
<span class="s2">"1"</span>,2013-01-01,53,48,42
<span class="s2">"2"</span>,2013-01-02,55,46,37
<span class="s2">"3"</span>,2013-01-03,53,45,37
</code></pre></div><p data-pid="KlNAZ9ON">将以上程序保存为<code>x.R</code> 文件并运行，将最高、最低温度的正确结果输出到answer中保存。</p><div class="highlight"><pre><code class="language-bash"><span></span>Rscript x.R &gt; answer <span class="o">&amp;&amp;</span> cat answer
-------------------------------------------------------------------------
<span class="o">[</span><span class="m">1</span><span class="o">]</span> <span class="s2">"     London2013 MIN:  23.0 MAX:  91.4"</span>
<span class="o">[</span><span class="m">1</span><span class="o">]</span> <span class="s2">"     Mumbai2013 MIN:  53.0 MAX: 102.2"</span>
<span class="o">[</span><span class="m">1</span><span class="o">]</span> <span class="s2">"    NewYork2013 MIN:  12.0 MAX:  99.0"</span>
<span class="o">[</span><span class="m">1</span><span class="o">]</span> <span class="s2">"        SFO2012 MIN:  36.0 MAX:  91.9"</span>
<span class="o">[</span><span class="m">1</span><span class="o">]</span> <span class="s2">"        SFO2013 MIN:  35.1 MAX:  88.0"</span>

ls *.csv
-------------------------------------------------------------------------
London2013.csv  Mumbai2013.csv  NewYork2013.csv  SFO2012.csv  SFO2013.csv
</code></pre></div><h2>2 MapReduce求解气温最值</h2><h3>2.1 操作步骤</h3><p data-pid="NFIBDb5t">参考Hadoop提供的<a href="http://link.zhihu.com/?target=http%3A//hadoop.apache.org/docs/r3.3.1/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">MapReduce官方例程</a>，改写java程序中的map和reduce函数内容。本小节先给出最终的操作方法，下一节中将详细介绍具体的实现原理。</p><p data-pid="90ofBNqx">首先接上一篇文章末尾的WordCount实验，在workspace中新建<code>Statistics.java</code> 程序，注意Java的语法要求文件名和里面定义的主类名一致，尽量不要更改文件名。程序内容如下：</p><div class="highlight"><pre><code class="language-java"><span></span><span class="kn">import</span> <span class="nn">java.io.IOException</span><span class="p">;</span>
<span class="kn">import</span> <span class="nn">java.util.StringTokenizer</span><span class="p">;</span>

<span class="kn">import</span> <span class="nn">org.apache.hadoop.conf.Configuration</span><span class="p">;</span>
<span class="kn">import</span> <span class="nn">org.apache.hadoop.fs.Path</span><span class="p">;</span>
<span class="kn">import</span> <span class="nn">org.apache.hadoop.io.FloatWritable</span><span class="p">;</span>
<span class="kn">import</span> <span class="nn">org.apache.hadoop.io.Text</span><span class="p">;</span>
<span class="kn">import</span> <span class="nn">org.apache.hadoop.mapreduce.Job</span><span class="p">;</span>
<span class="kn">import</span> <span class="nn">org.apache.hadoop.mapreduce.Mapper</span><span class="p">;</span>
<span class="kn">import</span> <span class="nn">org.apache.hadoop.mapreduce.Reducer</span><span class="p">;</span>
<span class="kn">import</span> <span class="nn">org.apache.hadoop.mapreduce.lib.input.FileInputFormat</span><span class="p">;</span>
<span class="kn">import</span> <span class="nn">org.apache.hadoop.mapreduce.lib.input.FileSplit</span><span class="p">;</span>
<span class="kn">import</span> <span class="nn">org.apache.hadoop.mapreduce.lib.output.FileOutputFormat</span><span class="p">;</span>

<span class="kd">public</span> <span class="kd">class</span> <span class="nc">Statistics</span> <span class="p">{</span>

    <span class="kd">public</span> <span class="kd">static</span> <span class="kd">class</span> <span class="nc">TokenizerMapper</span> <span class="kd">extends</span> <span class="n">Mapper</span><span class="o">&lt;</span><span class="n">Object</span><span class="p">,</span> <span class="n">Text</span><span class="p">,</span> <span class="n">Text</span><span class="p">,</span> <span class="n">FloatWritable</span><span class="o">&gt;</span><span class="p">{</span>

        <span class="kd">private</span> <span class="n">String</span> <span class="n">word</span><span class="p">;</span>

        <span class="nd">@Override</span>
        <span class="kd">public</span> <span class="kt">void</span> <span class="nf">map</span><span class="p">(</span><span class="n">Object</span> <span class="n">key</span><span class="p">,</span> <span class="n">Text</span> <span class="n">value</span><span class="p">,</span> <span class="n">Context</span> <span class="n">context</span><span class="p">)</span> <span class="kd">throws</span> <span class="n">IOException</span><span class="p">,</span> <span class="n">InterruptedException</span> <span class="p">{</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">key</span><span class="p">.</span><span class="na">toString</span><span class="p">().</span><span class="na">equals</span><span class="p">(</span><span class="s">"0"</span><span class="p">))</span> <span class="k">return</span><span class="p">;</span>
            <span class="n">String</span> <span class="n">name</span> <span class="o">=</span> <span class="p">((</span><span class="n">FileSplit</span><span class="p">)</span> <span class="n">context</span><span class="p">.</span><span class="na">getInputSplit</span><span class="p">()).</span><span class="na">getPath</span><span class="p">().</span><span class="na">getName</span><span class="p">();</span>
            <span class="n">name</span> <span class="o">=</span> <span class="n">name</span><span class="p">.</span><span class="na">substring</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">name</span><span class="p">.</span><span class="na">lastIndexOf</span><span class="p">(</span><span class="s">"."</span><span class="p">));</span>
            <span class="n">StringTokenizer</span> <span class="n">itr</span> <span class="o">=</span> <span class="k">new</span> <span class="n">StringTokenizer</span><span class="p">(</span><span class="n">value</span><span class="p">.</span><span class="na">toString</span><span class="p">(),</span> <span class="s">","</span><span class="p">);</span>
            <span class="k">while</span> <span class="p">(</span><span class="n">itr</span><span class="p">.</span><span class="na">hasMoreTokens</span><span class="p">())</span> <span class="n">word</span> <span class="o">=</span> <span class="n">itr</span><span class="p">.</span><span class="na">nextToken</span><span class="p">();</span>
            <span class="n">context</span><span class="p">.</span><span class="na">write</span><span class="p">(</span><span class="k">new</span> <span class="n">Text</span><span class="p">(</span><span class="n">name</span><span class="p">),</span> <span class="k">new</span> <span class="n">FloatWritable</span><span class="p">(</span><span class="n">Float</span><span class="p">.</span><span class="na">parseFloat</span><span class="p">(</span><span class="n">word</span><span class="p">)));</span>
        <span class="p">}</span>
    <span class="p">}</span>

    <span class="kd">public</span> <span class="kd">static</span> <span class="kd">class</span> <span class="nc">MinMaxReducer</span> <span class="kd">extends</span> <span class="n">Reducer</span><span class="o">&lt;</span><span class="n">Text</span><span class="p">,</span> <span class="n">FloatWritable</span><span class="p">,</span> <span class="n">Text</span><span class="p">,</span> <span class="n">FloatWritable</span><span class="o">&gt;</span> <span class="p">{</span>

        <span class="nd">@Override</span>
        <span class="kd">public</span> <span class="kt">void</span> <span class="nf">reduce</span><span class="p">(</span><span class="n">Text</span> <span class="n">key</span><span class="p">,</span> <span class="n">Iterable</span><span class="o">&lt;</span><span class="n">FloatWritable</span><span class="o">&gt;</span> <span class="n">values</span><span class="p">,</span> <span class="n">Context</span> <span class="n">context</span><span class="p">)</span> <span class="kd">throws</span> <span class="n">IOException</span><span class="p">,</span> <span class="n">InterruptedException</span> <span class="p">{</span>
            <span class="kt">float</span> <span class="n">max</span> <span class="o">=</span> <span class="n">Float</span><span class="p">.</span><span class="na">MIN_VALUE</span><span class="p">;</span>
            <span class="kt">float</span> <span class="n">min</span> <span class="o">=</span> <span class="n">Float</span><span class="p">.</span><span class="na">MAX_VALUE</span><span class="p">;</span>
            <span class="k">for</span> <span class="p">(</span><span class="n">FloatWritable</span> <span class="n">value</span><span class="p">:</span> <span class="n">values</span><span class="p">)</span> <span class="p">{</span>
                <span class="n">max</span> <span class="o">=</span> <span class="n">Float</span><span class="p">.</span><span class="na">max</span><span class="p">(</span><span class="n">value</span><span class="p">.</span><span class="na">get</span><span class="p">(),</span> <span class="n">max</span><span class="p">);</span>
                <span class="n">min</span> <span class="o">=</span> <span class="n">Float</span><span class="p">.</span><span class="na">min</span><span class="p">(</span><span class="n">value</span><span class="p">.</span><span class="na">get</span><span class="p">(),</span> <span class="n">min</span><span class="p">);</span>
            <span class="p">}</span>
            <span class="n">context</span><span class="p">.</span><span class="na">write</span><span class="p">(</span><span class="k">new</span> <span class="n">Text</span><span class="p">(</span><span class="n">key</span><span class="p">.</span><span class="na">toString</span><span class="p">()</span> <span class="o">+</span> <span class="s">"_MAX"</span><span class="p">),</span> <span class="k">new</span> <span class="n">FloatWritable</span><span class="p">(</span><span class="n">max</span><span class="p">));</span>
            <span class="n">context</span><span class="p">.</span><span class="na">write</span><span class="p">(</span><span class="k">new</span> <span class="n">Text</span><span class="p">(</span><span class="n">key</span><span class="p">.</span><span class="na">toString</span><span class="p">()</span> <span class="o">+</span> <span class="s">"_MIN"</span><span class="p">),</span> <span class="k">new</span> <span class="n">FloatWritable</span><span class="p">(</span><span class="n">min</span><span class="p">));</span>
        <span class="p">}</span>
    <span class="p">}</span>

    <span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="p">(</span><span class="n">String</span><span class="o">[]</span> <span class="n">args</span><span class="p">)</span> <span class="kd">throws</span> <span class="n">Exception</span> <span class="p">{</span>
        <span class="n">Configuration</span> <span class="n">conf</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Configuration</span><span class="p">();</span>
        <span class="n">Job</span> <span class="n">job</span> <span class="o">=</span> <span class="n">Job</span><span class="p">.</span><span class="na">getInstance</span><span class="p">(</span><span class="n">conf</span><span class="p">,</span> <span class="s">"Min Max Temperature"</span><span class="p">);</span>
        <span class="n">job</span><span class="p">.</span><span class="na">setJarByClass</span><span class="p">(</span><span class="n">Statistics</span><span class="p">.</span><span class="na">class</span><span class="p">);</span>
        <span class="n">job</span><span class="p">.</span><span class="na">setMapperClass</span><span class="p">(</span><span class="n">TokenizerMapper</span><span class="p">.</span><span class="na">class</span><span class="p">);</span>
        <span class="n">job</span><span class="p">.</span><span class="na">setMapOutputKeyClass</span><span class="p">(</span><span class="n">Text</span><span class="p">.</span><span class="na">class</span><span class="p">);</span>
        <span class="n">job</span><span class="p">.</span><span class="na">setMapOutputValueClass</span><span class="p">(</span><span class="n">FloatWritable</span><span class="p">.</span><span class="na">class</span><span class="p">);</span>
        <span class="n">job</span><span class="p">.</span><span class="na">setReducerClass</span><span class="p">(</span><span class="n">MinMaxReducer</span><span class="p">.</span><span class="na">class</span><span class="p">);</span>
        <span class="n">job</span><span class="p">.</span><span class="na">setOutputKeyClass</span><span class="p">(</span><span class="n">Text</span><span class="p">.</span><span class="na">class</span><span class="p">);</span>
        <span class="n">job</span><span class="p">.</span><span class="na">setOutputValueClass</span><span class="p">(</span><span class="n">FloatWritable</span><span class="p">.</span><span class="na">class</span><span class="p">);</span>
        <span class="n">FileInputFormat</span><span class="p">.</span><span class="na">addInputPath</span><span class="p">(</span><span class="n">job</span><span class="p">,</span> <span class="k">new</span> <span class="n">Path</span><span class="p">(</span><span class="s">"/user/input"</span><span class="p">));</span>
        <span class="n">FileOutputFormat</span><span class="p">.</span><span class="na">setOutputPath</span><span class="p">(</span><span class="n">job</span><span class="p">,</span> <span class="k">new</span> <span class="n">Path</span><span class="p">(</span><span class="s">"/user/output"</span><span class="p">));</span>
        <span class="n">System</span><span class="p">.</span><span class="na">exit</span><span class="p">(</span><span class="n">job</span><span class="p">.</span><span class="na">waitForCompletion</span><span class="p">(</span><span class="kc">true</span><span class="p">)</span> <span class="o">?</span> <span class="mi">0</span> <span class="p">:</span> <span class="mi">1</span><span class="p">);</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div><p data-pid="n6f5faut">为了实验方便，新建以下脚本文件</p><div class="highlight"><pre><code class="language-bash"><span></span>vim run.sh
<span class="o">=======================================</span>
hadoop com.sun.tools.javac.Main <span class="nv">$1</span>.java
jar cf _<span class="nv">$1</span>.jar <span class="nv">$1</span>*.class
hadoop fs -rm -r -f /user/output
hadoop jar _<span class="nv">$1</span>.jar <span class="nv">$1</span>
</code></pre></div><p data-pid="MnwuAu5x">其中<code>$1</code>是我们接下来运行这个脚本时传入的参数：Java程序去除后缀的文件名。第一行是对程序进行编译，第二行将字节码文件打包成jar，第三行删除HDFS中可能存在的输出文件夹，最后使用jar命令运行程序。</p><p data-pid="0kDnKQci">运行程序前我们还需要将csv文件存放到HDFS的输入文件夹</p><div class="highlight"><pre><code class="language-bash"><span></span>hadoop fs -put -f ./*.csv /user/input/
</code></pre></div><p data-pid="RMzBlMYb">然后我们运行脚本，成功后会显示以下内容</p><div class="highlight"><pre><code class="language-bash"><span></span>bash run.sh Statistics
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Deleted /user/output
<span class="m">2021</span>-06-28 <span class="m">23</span>:08:26,784 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at node0/127.0.1.1:8032
<span class="m">2021</span>-06-28 <span class="m">23</span>:08:27,015 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
<span class="m">2021</span>-06-28 <span class="m">23</span>:08:27,048 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding <span class="k">for</span> path: /tmp/hadoop-yarn/staging/root/.staging/job_1624892807228_0001
<span class="m">2021</span>-06-28 <span class="m">23</span>:08:27,719 INFO input.FileInputFormat: Total input files to process : <span class="m">0</span>
<span class="m">2021</span>-06-28 <span class="m">23</span>:08:28,192 INFO mapreduce.JobSubmitter: number of splits:0
<span class="m">2021</span>-06-28 <span class="m">23</span>:08:28,737 INFO mapreduce.JobSubmitter: Submitting tokens <span class="k">for</span> job: job_1624892807228_0001
<span class="m">2021</span>-06-28 <span class="m">23</span>:08:28,737 INFO mapreduce.JobSubmitter: Executing with tokens: <span class="o">[]</span>
<span class="m">2021</span>-06-28 <span class="m">23</span>:08:28,990 INFO conf.Configuration: resource-types.xml not found
<span class="m">2021</span>-06-28 <span class="m">23</span>:08:28,990 INFO resource.ResourceUtils: Unable to find <span class="s1">'resource-types.xml'</span>.
<span class="m">2021</span>-06-28 <span class="m">23</span>:08:29,171 INFO impl.YarnClientImpl: Submitted application application_1624892807228_0001
<span class="m">2021</span>-06-28 <span class="m">23</span>:08:29,204 INFO mapreduce.Job: The url to track the job: http://andy-OptiPlex-7070:8088/proxy/application_1624892807228_0001/
<span class="m">2021</span>-06-28 <span class="m">23</span>:08:29,205 INFO mapreduce.Job: Running job: job_1624892807228_0001
<span class="m">2021</span>-06-28 <span class="m">23</span>:08:35,337 INFO mapreduce.Job: Job job_1624892807228_0001 running in uber mode : <span class="nb">false</span>
<span class="m">2021</span>-06-28 <span class="m">23</span>:08:35,337 INFO mapreduce.Job:  map <span class="m">0</span>% reduce <span class="m">0</span>%
<span class="m">2021</span>-06-28 <span class="m">23</span>:08:39,403 INFO mapreduce.Job:  map <span class="m">0</span>% reduce <span class="m">100</span>%
<span class="m">2021</span>-06-28 <span class="m">23</span>:08:40,436 INFO mapreduce.Job: Job job_1624892807228_0001 completed successfully
<span class="m">2021</span>-06-28 <span class="m">23</span>:08:40,485 INFO mapreduce.Job: Counters: <span class="m">41</span>
...
	Shuffle Errors
		<span class="nv">BAD_ID</span><span class="o">=</span><span class="m">0</span>
		<span class="nv">CONNECTION</span><span class="o">=</span><span class="m">0</span>
		<span class="nv">IO_ERROR</span><span class="o">=</span><span class="m">0</span>
		<span class="nv">WRONG_LENGTH</span><span class="o">=</span><span class="m">0</span>
		<span class="nv">WRONG_MAP</span><span class="o">=</span><span class="m">0</span>
		<span class="nv">WRONG_REDUCE</span><span class="o">=</span><span class="m">0</span>
	File Output Format Counters
		Bytes <span class="nv">Written</span><span class="o">=</span><span class="m">0</span>
</code></pre></div><p data-pid="3lSw2-Ga">如果提示删除文件夹失败，节点处于安全模式</p><div class="highlight"><pre><code class="language-text"><span></span>rm: Cannot delete /user/output. Name node is in safe mode.
</code></pre></div><p data-pid="y6AXh-8d">我们关闭安全模式再重新尝试运行</p><div class="highlight"><pre><code class="language-bash"><span></span>hdfs dfsadmin -safemode leave
-----------------------------
Safe mode is OFF
</code></pre></div><p data-pid="PH1W0gnr">接下来查看程序的输出</p><div class="highlight"><pre><code class="language-bash"><span></span>hadoop fs -cat /user/output/part-r-00000
----------------------------------------
London2013_MAX	<span class="m">91</span>.4
London2013_MIN	<span class="m">23</span>.0
Mumbai2013_MAX	<span class="m">102</span>.2
Mumbai2013_MIN	<span class="m">53</span>.0
NewYork2013_MAX	<span class="m">99</span>.0
NewYork2013_MIN	<span class="m">12</span>.0
SFO2012_MAX	<span class="m">91</span>.9
SFO2012_MIN	<span class="m">36</span>.0
SFO2013_MAX	<span class="m">88</span>.0
SFO2013_MIN	<span class="m">35</span>.1
</code></pre></div><p data-pid="qsrYsaPm">对照answer的内容验证结果的正确性。至此，我们的实验就完成了！</p><h3>2.2 实验原理</h3><ul><li data-pid="n5ysWgTS"><b>MapReduce</b></li></ul><p data-pid="9C3Rrg76">MapReduce实际上大体包含三个过程：Map、Combine和Reduce。如果对Hadoop不熟悉，可以联想Python的<a href="http://link.zhihu.com/?target=https%3A//www.runoob.com/python/python-func-map.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">map</a>和<a href="http://link.zhihu.com/?target=https%3A//www.runoob.com/python/python-func-reduce.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">reduce</a>函数，这类函数式编程的思想都是共通的。</p><p data-pid="UnHf_7r3"><b>Map</b>顾名思义，是个映射函数，将输入的变量进行一顿操作，得到一些个输出变量。这里的输入在Hadoop里面指的是文件中的一行，即Hadoop是按行操作的。Tokenize就是一个典型的Map操作，对文件中的一行进行分词和数据类型转换，得到多个变量的输出。和Reduce相比，Map的特点是只依赖当前处理的变量，不做变量之间的运算，即Map是单变元函数。</p><p data-pid="k4AXvEk2"><b>Reduce</b>的字面意思是减少数量。相比于前面的Map（一般文件每行至少有一个返回值），Reduce可以迭代地进行两个变量之间的操作，运算结果往往更少，比如求解1000个数的最大值，Map可能只做数据预处理，仍然返回1000个数，而Reduce对Map返回的结果进行两两比较，最终得到的最大值只有一个数。</p><p data-pid="vC4XOObZ"><b>Combine</b>是可选的操作，在Map和Reduce之间执行，目的是为了减少这两者通信的数据量。Combine可以看做是一个小型的Reduce操作，针对每个计算节点先对Map的结果进行Reduce，得到的中间结果再传输给其他节点进行最终的Reduce。从代码中可以看到，Combine和Reduce都继承的是Reducer这个类。</p><p data-pid="dA8fyhQF">具体地，在Hadoop中，实现Map操作需要我们继承Mapper类：</p><div class="highlight"><pre><code class="language-java"><span></span><span class="kd">public</span> <span class="kd">static</span> <span class="kd">class</span> <span class="nc">TokenizerMapper</span> <span class="kd">extends</span> <span class="n">Mapper</span><span class="o">&lt;</span><span class="n">Object</span><span class="p">,</span> <span class="n">Text</span><span class="p">,</span> <span class="n">Text</span><span class="p">,</span> <span class="n">FloatWritable</span><span class="o">&gt;</span><span class="p">{</span>
    <span class="nd">@Override</span>
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">map</span><span class="p">(</span><span class="n">Object</span> <span class="n">key</span><span class="p">,</span> <span class="n">Text</span> <span class="n">value</span><span class="p">,</span> <span class="n">Context</span> <span class="n">context</span><span class="p">)</span> <span class="kd">throws</span> <span class="n">IOException</span><span class="p">,</span> <span class="n">InterruptedException</span> <span class="p">{</span>
        <span class="n">context</span><span class="p">.</span><span class="na">write</span><span class="p">(</span><span class="k">new</span> <span class="n">Text</span><span class="p">(...),</span> <span class="k">new</span> <span class="n">FloatWritable</span><span class="p">(...));</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div><p data-pid="EEKJ7KhA">其具体定义和用法参考<a href="http://link.zhihu.com/?target=https%3A//hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapreduce/Mapper.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">Java API文档</a>。Mapper使用模板定义了输入和输出的类型：</p><div class="highlight"><pre><code class="language-java"><span></span><span class="n">Class</span> <span class="n">Mapper</span><span class="o">&lt;</span><span class="n">KEYIN</span><span class="p">,</span><span class="n">VALUEIN</span><span class="p">,</span><span class="n">KEYOUT</span><span class="p">,</span><span class="n">VALUEOUT</span><span class="o">&gt;</span>
</code></pre></div><p data-pid="F4VVvQrB">这里定义输入输出都是字典的键值对。<b>输入的Key是Object类型，实际上是当前数据分片在原文件中的偏移量</b>，我们可以在map函数的开头添加以下代码跳过csv文件的表头：</p><div class="highlight"><pre><code class="language-java"><span></span><span class="k">if</span> <span class="p">(</span><span class="n">key</span><span class="p">.</span><span class="na">toString</span><span class="p">().</span><span class="na">equals</span><span class="p">(</span><span class="s">"0"</span><span class="p">))</span> <span class="k">return</span><span class="p">;</span>
</code></pre></div><p data-pid="RT_OOVdl">因为Hadoop默认是按行处理，表头处的偏移量又为0。<b>输入的Value就是这一行的文本了</b>，注意这里是Text而不是String。Hadoop定义了一系列的数据类型，它们往往可以和Java的数据类型互相转换，比如String和Text的转换：</p><div class="highlight"><pre><code class="language-java"><span></span><span class="n">String</span> <span class="n">str</span> <span class="o">=</span> <span class="n">text</span><span class="p">.</span><span class="na">toString</span><span class="p">();</span> <span class="c1">// Text =&gt; String</span>
<span class="n">Text</span> <span class="n">text</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Text</span><span class="p">(</span><span class="n">str</span><span class="p">);</span>    <span class="c1">// String =&gt; Text (or `text.set(str)`)</span>
</code></pre></div><p data-pid="sFysvSr2">更多类型可以查看文档或者下文我会介绍本地配置Hadoop的代码自动提示。输出的类型可以自己定义，我们这里使用Text类型存放文件名用做关键字，使用FloatWritable类型存放温度值用作待查值。如果你不想按行读取文件，也可以重写一个类来实现InputFormat接口。</p><p data-pid="3Hwvk6kh">Mapper的具体调用流程如下：</p><div class="highlight"><pre><code class="language-java"><span></span><span class="kd">public</span> <span class="kt">void</span> <span class="nf">run</span><span class="p">(</span><span class="n">Context</span> <span class="n">context</span><span class="p">)</span> <span class="kd">throws</span> <span class="n">IOException</span><span class="p">,</span> <span class="n">InterruptedException</span> <span class="p">{</span>
    <span class="n">setup</span><span class="p">(</span><span class="n">context</span><span class="p">);</span>
    <span class="k">try</span> <span class="p">{</span>
        <span class="k">while</span> <span class="p">(</span><span class="n">context</span><span class="p">.</span><span class="na">nextKeyValue</span><span class="p">())</span> <span class="p">{</span>
            <span class="n">map</span><span class="p">(</span><span class="n">context</span><span class="p">.</span><span class="na">getCurrentKey</span><span class="p">(),</span> <span class="n">context</span><span class="p">.</span><span class="na">getCurrentValue</span><span class="p">(),</span> <span class="n">context</span><span class="p">);</span>
        <span class="p">}</span>
    <span class="p">}</span> <span class="k">finally</span> <span class="p">{</span>
        <span class="n">cleanup</span><span class="p">(</span><span class="n">context</span><span class="p">);</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div><p data-pid="UDzF3dya">我们可以重写setup、map和cleanup方法。setup和cleanup分别在开始和结束时执行，而map函数是核心功能。Mapper迭代地获取context中的每一个键值对，然后调用map函数进行处理。在map函数中我们使用StringTokenizer对文件的一行进行分割，并将最后一个单词转换为float类型。我们使用以下方法获取到map正在处理的文件的名字「参考<a href="http://link.zhihu.com/?target=https%3A//blog.csdn.net/sinat_29581293/article/details/78112371" class=" wrap external" target="_blank" rel="nofollow noreferrer">hadoop 代码中获取文件名</a>」：</p><div class="highlight"><pre><code class="language-java"><span></span><span class="n">String</span> <span class="n">name</span> <span class="o">=</span> <span class="p">((</span><span class="n">FileSplit</span><span class="p">)</span> <span class="n">context</span><span class="p">.</span><span class="na">getInputSplit</span><span class="p">()).</span><span class="na">getPath</span><span class="p">().</span><span class="na">getName</span><span class="p">();</span>  <span class="c1">// 获取文件名</span>
<span class="n">name</span> <span class="o">=</span> <span class="n">name</span><span class="p">.</span><span class="na">substring</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">name</span><span class="p">.</span><span class="na">lastIndexOf</span><span class="p">(</span><span class="s">"."</span><span class="p">));</span>  <span class="c1">// 去除扩展名</span>
</code></pre></div><p data-pid="VehAm_BI">然后我们将文件名作为key，温度值作为value写入context：</p><div class="highlight"><pre><code class="language-java"><span></span><span class="n">context</span><span class="p">.</span><span class="na">write</span><span class="p">(</span><span class="k">new</span> <span class="n">Text</span><span class="p">(</span><span class="n">name</span><span class="p">),</span> <span class="k">new</span> <span class="n">FloatWritable</span><span class="p">(</span><span class="n">Float</span><span class="p">.</span><span class="na">parseFloat</span><span class="p">(</span><span class="n">word</span><span class="p">)));</span>
</code></pre></div><p data-pid="a7Vq_LA_">这里的context是Mapper.Context类型，用来写入中间结果，两个参数分别是键和值。</p><p data-pid="Qm8fwR7e">类似的，实现Reduce功能，我们需要继承Reducer类：</p><div class="highlight"><pre><code class="language-java"><span></span><span class="kd">public</span> <span class="kd">static</span> <span class="kd">class</span> <span class="nc">MinMaxCombiner</span> <span class="kd">extends</span> <span class="n">Reducer</span><span class="o">&lt;</span><span class="n">Text</span><span class="p">,</span> <span class="n">FloatWritable</span><span class="p">,</span> <span class="n">Text</span><span class="p">,</span> <span class="n">FloatWritable</span><span class="o">&gt;</span> <span class="p">{</span>
    <span class="nd">@Override</span>
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">reduce</span><span class="p">(</span><span class="n">Text</span> <span class="n">key</span><span class="p">,</span> <span class="n">Iterable</span><span class="o">&lt;</span><span class="n">FloatWritable</span><span class="o">&gt;</span> <span class="n">values</span><span class="p">,</span> <span class="n">Context</span> <span class="n">context</span><span class="p">)</span> <span class="kd">throws</span> <span class="n">IOException</span><span class="p">,</span> <span class="n">InterruptedException</span> <span class="p">{</span>
        <span class="p">...</span>
        <span class="n">context</span><span class="p">.</span><span class="na">write</span><span class="p">(</span><span class="k">new</span> <span class="n">Text</span><span class="p">(...),</span> <span class="k">new</span> <span class="n">FloatWritable</span><span class="p">(...));</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div><p data-pid="Yks8DnUf">相对于Mapper的map函数，Reducer的reduce函数的输入稍有不同。在map结束后，程序会自动将相同key的值合并成一个可迭代对象。<b>Reducer的输入键值对类型要与Map的输出键值对类型相同</b>，而这里reduce的输入值则变为对应类型的Iterable对象。</p><p data-pid="R8qIIsj8">与上述Mapper的run的调用流程相似，Reducer也是每次取一个键值对。在本文中，键仍然是文件名，值成为文件中所有温度的列表。我们只需要在reduce中遍历这个列表求最值即可。</p><p data-pid="FoMnC3Gj">我在实现这个函数时出了一个幼稚的错误，我将维护的max和min变量设为了类变量，本以为Reducer每次会新建一个实例执行reduce，但实际上一个Reducer的reduce函数会被执行多遍，这时我的max和min的初始值已经改变，最终结果就变成了多个文件的全局最值了。为了这个问题，我调试了4个小时，血的教训告诉我们，在调试BUG时不仅要多做系统环境假设，也要做代码逻辑假设。</p><p data-pid="h15HYlgF">接下来在main函数中新建Job，对Job的参数进行绑定，最后运行。</p><div class="highlight"><pre><code class="language-java"><span></span><span class="n">Configuration</span> <span class="n">conf</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Configuration</span><span class="p">();</span>
<span class="n">Job</span> <span class="n">job</span> <span class="o">=</span> <span class="n">Job</span><span class="p">.</span><span class="na">getInstance</span><span class="p">(</span><span class="n">conf</span><span class="p">,</span> <span class="s">"Min Max Temperature"</span><span class="p">);</span>  <span class="c1">// 新建作业</span>
<span class="n">job</span><span class="p">.</span><span class="na">setJarByClass</span><span class="p">(</span><span class="n">Statistics</span><span class="p">.</span><span class="na">class</span><span class="p">);</span>              <span class="c1">// 绑定运行的主类，就是这份代码最外层的类</span>
<span class="n">job</span><span class="p">.</span><span class="na">setMapperClass</span><span class="p">(</span><span class="n">TokenizerMapper</span><span class="p">.</span><span class="na">class</span><span class="p">);</span>        <span class="c1">// 设置Mapper类</span>
<span class="n">job</span><span class="p">.</span><span class="na">setMapOutputKeyClass</span><span class="p">(</span><span class="n">Text</span><span class="p">.</span><span class="na">class</span><span class="p">);</span>             <span class="c1">// 设置Mapper输出键类型</span>
<span class="n">job</span><span class="p">.</span><span class="na">setMapOutputValueClass</span><span class="p">(</span><span class="n">FloatWritable</span><span class="p">.</span><span class="na">class</span><span class="p">);</span>  <span class="c1">// 设置Mapper输出值类型</span>
<span class="c1">// job.setCombinerClass(MinMaxCombiner.class);    你可以自定义一个Combiner</span>
<span class="n">job</span><span class="p">.</span><span class="na">setReducerClass</span><span class="p">(</span><span class="n">MinMaxCombiner</span><span class="p">.</span><span class="na">class</span><span class="p">);</span>        <span class="c1">// 设置Reducer类</span>
<span class="n">job</span><span class="p">.</span><span class="na">setOutputKeyClass</span><span class="p">(</span><span class="n">Text</span><span class="p">.</span><span class="na">class</span><span class="p">);</span>                <span class="c1">// 设置Reducer输出键类型</span>
<span class="n">job</span><span class="p">.</span><span class="na">setOutputValueClass</span><span class="p">(</span><span class="n">FloatWritable</span><span class="p">.</span><span class="na">class</span><span class="p">);</span>     <span class="c1">// 设置Reducer输出值类型</span>
<span class="n">FileInputFormat</span><span class="p">.</span><span class="na">addInputPath</span><span class="p">(</span><span class="n">job</span><span class="p">,</span> <span class="k">new</span> <span class="n">Path</span><span class="p">(</span><span class="s">"/user/input"</span><span class="p">));</span>    <span class="c1">// * 这里如果指定了输入输出</span>
<span class="n">FileOutputFormat</span><span class="p">.</span><span class="na">setOutputPath</span><span class="p">(</span><span class="n">job</span><span class="p">,</span> <span class="k">new</span> <span class="n">Path</span><span class="p">(</span><span class="s">"/user/output"</span><span class="p">));</span> <span class="c1">// * 命令行运行时就不需要参数</span>
<span class="n">System</span><span class="p">.</span><span class="na">exit</span><span class="p">(</span><span class="n">job</span><span class="p">.</span><span class="na">waitForCompletion</span><span class="p">(</span><span class="kc">true</span><span class="p">)</span> <span class="o">?</span> <span class="mi">0</span> <span class="p">:</span> <span class="mi">1</span><span class="p">);</span> <span class="c1">// 运行并等待作业完成、获取返回值</span>
</code></pre></div><ul><li data-pid="9Shl3IZ7"><b>奇怪的BUG实际不奇怪</b></li></ul><p data-pid="TMkqm8PR">在最初修改Hadoop官方例程时，我将Reducer的输出键设为NullWritable，而输入键与Mapper输出键一致，为Text类型，结果在运行时报错：</p><div class="highlight"><pre><code class="language-java"><span></span><span class="kd">public</span> <span class="kd">static</span> <span class="kd">class</span> <span class="nc">MinMaxCombiner</span> <span class="kd">extends</span> <span class="n">Reducer</span><span class="o">&lt;</span><span class="n">Text</span><span class="p">,</span> <span class="n">FloatWritable</span><span class="p">,</span> <span class="n">NullWritable</span><span class="p">,</span> <span class="n">FloatWritable</span><span class="o">&gt;</span> <span class="p">{</span>
    <span class="nd">@Override</span>
    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">reduce</span><span class="p">(</span><span class="n">Text</span> <span class="n">key</span><span class="p">,</span> <span class="n">Iterable</span><span class="o">&lt;</span><span class="n">FloatWritable</span><span class="o">&gt;</span> <span class="n">values</span><span class="p">,</span> <span class="n">Context</span> <span class="n">context</span><span class="p">)</span> <span class="kd">throws</span> <span class="n">IOException</span><span class="p">,</span> <span class="n">InterruptedException</span> <span class="p">{</span>
        <span class="p">...</span>
        <span class="n">context</span><span class="p">.</span><span class="na">write</span><span class="p">(</span><span class="n">NullWritable</span><span class="p">.</span><span class="na">get</span><span class="p">(),</span> <span class="k">new</span> <span class="n">FloatWritable</span><span class="p">(...));</span>
        <span class="c1">// ^</span>
        <span class="c1">// Type mismatch in key from reduce: expected Text, recieved org.apache.hadoop.io.NullWritable</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div><p data-pid="EYE5_2va">定义的不就是NullWritable吗？我写入的不就是NullWritable吗？我Mapper输出和Reducer输入键值对类型相同啊？我满脑子的</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-01d6f8a26e4f84333b0c4f83d9c850bd_720w.png?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="181" data-rawheight="172" class="content_image" width="181"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='181'%20height='172'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="181" data-rawheight="172" class="content_image lazy" width="181" data-actualsrc="https://pic1.zhimg.com/v2-01d6f8a26e4f84333b0c4f83d9c850bd_720w.png?source=d16d100b"></figure><p data-pid="Q-yC95Zu">参考了 <a href="http://link.zhihu.com/?target=https%3A//blog.csdn.net/panjiao119/article/details/80100452" class=" wrap external" target="_blank" rel="nofollow noreferrer">hadoop：Type mismatch in key from map</a> 这篇文章，然而并没有用。又调试了4个小时，我猜想Reducer要求输入键值对和输出键值对一致，虽然没什么道理，但我观察到的正确的例程都是一致的。测试一下，发现真的是这样！</p><p data-pid="1giGfMTF">除此之外，我一直观察到程序执行了两次reduce，直到我了解了combine过程，才明白这其实是由于例程中将Combiner设置成Reducer的同一个类了。原本流程：</p><div class="highlight"><pre><code class="language-java"><span></span><span class="nl">map:</span>     <span class="p">(</span><span class="n">K1</span><span class="p">,</span> <span class="n">V1</span><span class="p">)</span> <span class="err">→</span> <span class="p">(</span><span class="n">K2</span><span class="p">,</span> <span class="n">V2</span><span class="p">)</span>
<span class="nl">combine:</span> <span class="p">(</span><span class="n">K2</span><span class="p">,</span> <span class="n">list</span><span class="p">(</span><span class="n">V2</span><span class="p">))</span> <span class="err">→</span> <span class="p">(</span><span class="n">K3</span><span class="p">,</span> <span class="n">list</span><span class="p">(</span><span class="n">V3</span><span class="p">))</span>
<span class="nl">reduce:</span>  <span class="p">(</span><span class="n">K3</span><span class="p">,</span> <span class="n">list</span><span class="p">(</span><span class="n">V3</span><span class="p">))</span> <span class="err">→</span> <span class="p">(</span><span class="n">K4</span><span class="p">,</span> <span class="n">list</span><span class="p">(</span><span class="n">V4</span><span class="p">))</span>
</code></pre></div><p data-pid="JD02IBFI">现在变成了</p><div class="highlight"><pre><code class="language-java"><span></span><span class="nl">map:</span>     <span class="p">(</span><span class="n">K1</span><span class="p">,</span> <span class="n">V1</span><span class="p">)</span> <span class="err">→</span> <span class="p">(</span><span class="n">K2</span><span class="p">,</span> <span class="n">V2</span><span class="p">)</span>
<span class="nl">combine:</span> <span class="p">(</span><span class="n">K2</span><span class="p">,</span> <span class="n">list</span><span class="p">(</span><span class="n">V2</span><span class="p">))</span> <span class="err">→</span> <span class="p">(</span><span class="n">K3</span><span class="p">,</span> <span class="n">list</span><span class="p">(</span><span class="n">V3</span><span class="p">))</span>
<span class="nl">reduce:</span>  <span class="p">(</span><span class="n">K2</span><span class="p">,</span> <span class="n">list</span><span class="p">(</span><span class="n">V2</span><span class="p">))</span> <span class="err">→</span> <span class="p">(</span><span class="n">K3</span><span class="p">,</span> <span class="n">list</span><span class="p">(</span><span class="n">V3</span><span class="p">))</span>
</code></pre></div><p data-pid="748upqrT">这里当然就要求 (K3, V3) 和 (K2, V2) 相同了！而且combine执行一遍，reduce又执行一遍，这就要求我们的操作是满足结合律的，而且对键的内容不能改变，即K2和K3不仅类型一样，值也要一样。解决这个问题的办法很简单，要么重写一个Combiner类，要么删除combine的绑定，只使用Reducer。结合律可以理解为如下，对于N个键值对和m个Combiner，先进行combine (C)、再进行reduce (R)和直接reduce的结果应一致。</p><p data-pid="yiHF75Pt"><img src="https://www.zhihu.com/equation?tex=R%28C%5E1%28%3Ck%5E1_1%2Cv%5E1_1%3E%2C%5Cdots%2C%3Ck%5E1_n%2Cv%5E1_n%3E%29%2C%5Cdots%2CC%5Em%28%3Ck%5Em_1%2Cv%5Em_1%3E%2C%5Cdots%2C%3Ck%5Em_n%2Cv%5Em_n%3E%29%29+%5C%5C%5CLeftrightarrow+R%28%3Ck_1%2Cv_1%3E%2C%5Cdots%2C%3Ck_N%2Cv_N%3E%29%5C%5C" alt="R(C^1(&lt;k^1_1,v^1_1&gt;,\dots,&lt;k^1_n,v^1_n&gt;),\dots,C^m(&lt;k^m_1,v^m_1&gt;,\dots,&lt;k^m_n,v^m_n&gt;)) \\\Leftrightarrow R(&lt;k_1,v_1&gt;,\dots,&lt;k_N,v_N&gt;)\\" eeimg="1"> </p><p data-pid="uq0y8VjU">求最大值、最小值和词频统计都满足结合律，但求中位数就不满足。</p><ul><li data-pid="ynN6Ka2Z"><b>IntelliJ配置本地Hadoop自动提示</b></li></ul><p data-pid="RmjbDmVu">直接编写调用Hadoop库的Java程序很容易出错，我们可以在本地IDE「我选用的是JetBrains公司的<a href="http://link.zhihu.com/?target=https%3A//www.jetbrains.com/idea/" class=" wrap external" target="_blank" rel="nofollow noreferrer">IntelliJ</a>」进行编程，再推到云端运行测试。参考这篇博客：<a href="http://link.zhihu.com/?target=https%3A//blog.csdn.net/programmer_wei/article/details/45286749" class=" wrap external" target="_blank" rel="nofollow noreferrer">intellij idea本地开发调试hadoop的方法</a>，主要注意包要导入全，Hadoop3.3.1依赖的是Java1.8，版本比较高。虽然我最后也没能配出可以本地运行的环境，但自动提示是没问题的。如果我本地用IntelliJ就可以运行的话，我前面在干嘛？？？</p><ul><li data-pid="amOoIP7s"><b>查看Hadoop的标准输出流内容</b></li></ul><p data-pid="bU_YaS8u">这里讲一点MapReduce的调试技巧。我们在Mapper和Reducer中使用System.out.println是无法在运行时直接看到输出的，这给调试带来了巨大的困难，因为output只有在Hadoop正确运行结束后才能得到。参考<a href="http://link.zhihu.com/?target=https%3A//www.cnblogs.com/TiestoRay/p/6113248.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">如何查看MapReduce执行的程序中的输出日志</a>，我们其实是可以看到输出内容的，例如以下命令：</p><div class="highlight"><pre><code class="language-bash"><span></span>cat /usr/local/software/hadoop-3.3.1/logs/userlogs/application_1624932149538_0001/container_1624932149538_0001_01_000001/stdout
</code></pre></div><p data-pid="mi8ncqeG">另外我们也可以使用log4j，稍微需要配置一下：<a href="http://link.zhihu.com/?target=https%3A//blog.csdn.net/monkey131499/article/details/45560703" class=" wrap external" target="_blank" rel="nofollow noreferrer">Hadoop自定义输出日志log4j</a>。</p><ul><li data-pid="zKLpfs1_"><b>动用钞能力解决服务器卡顿问题</b></li></ul><p data-pid="_raapJjb">1核2G的默认配置要运行hadoop其实非常卡顿，在监控面板中可以看到，CPU使用率经常飙升到90%以上。另外默认带宽是1Mbit，也就是125KB，加上监控程序会有一些固定的通信overhead，输入命令都有可能卡死。最后忍无可忍，动用了200元代金券，把配置升到了2核4G，8Mbit带宽，瞬间变得流畅。</p><ul><li data-pid="7uaQiVat"><b>华为云服务器遭受攻击，公网IP被冻结</b></li></ul><p data-pid="NXsv0Fcw">当天中午11点收到邮件，报告我的服务器对外攻击</p><figure data-size="normal"><noscript><img src="https://pica.zhimg.com/v2-d3f4b4f29811632bc1fbeb0c774e31fc_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="1041" data-rawheight="706" class="origin_image zh-lightbox-thumb" width="1041" data-original="https://picx.zhimg.com/v2-d3f4b4f29811632bc1fbeb0c774e31fc_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1041'%20height='706'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1041" data-rawheight="706" class="origin_image zh-lightbox-thumb lazy" width="1041" data-original="https://picx.zhimg.com/v2-d3f4b4f29811632bc1fbeb0c774e31fc_720w.jpg?source=d16d100b" data-actualsrc="https://pica.zhimg.com/v2-d3f4b4f29811632bc1fbeb0c774e31fc_720w.jpg?source=d16d100b"></figure><p data-pid="6W1H7B9B">让我采取措施，我？？？除了SSH和wget下载Hadoop，我服务器和外界都没有网络交互，怎么会对外攻击，我就没管。晚上20:40刚刚把调了一天的程序顺利调通，还没来得及记录，IP就被冻结了，打电话给客服，他说之前的邮件是警告，24小时之内不整改就冻结了。这口气说的我好像犯法了似的。最终提交工单，后台工程师很及时地解决了问题。其实他也什么都没做，只是给我看了一眼8088端口在过去1个小时内有200W的网络连接，然后就把IP解封了，让我去配置安全组协议。</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-288fb2f4a69c65a535183b5330532cf4_720w.jpg?source=d16d100b" data-size="normal" data-rawwidth="1917" data-rawheight="467" class="origin_image zh-lightbox-thumb" width="1917" data-original="https://picx.zhimg.com/v2-288fb2f4a69c65a535183b5330532cf4_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1917'%20height='467'&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="1917" data-rawheight="467" class="origin_image zh-lightbox-thumb lazy" width="1917" data-original="https://picx.zhimg.com/v2-288fb2f4a69c65a535183b5330532cf4_720w.jpg?source=d16d100b" data-actualsrc="https://pic1.zhimg.com/v2-288fb2f4a69c65a535183b5330532cf4_720w.jpg?source=d16d100b"><figcaption>默认的安全组协议</figcaption></figure><p data-pid="ffcliGfE">上篇文章中讲到，华为云耀服务器配置时安全组协议有default、webserver和fully-access。我当时还打算配置多节点的集群，害怕出问题就选了fully-access。你既然有这个选项，凭什么要让用户对网络攻击负责任？后台工程师就可以拿着我的root密码在服务器上闲逛，而且一切损失由我自己承担？为什么不提示一下安全组协议的风险？为什么管遭受外部攻击叫做对外攻击？为什么网络连接数爆表却不告诉我端口？最后我还是消气了，22:30把服务器重新配好开始继续做实验。</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-c66658a0d417b7750e11a74b21b1d42d_720w.jpg?source=d16d100b" data-size="normal" data-rawwidth="813" data-rawheight="290" class="origin_image zh-lightbox-thumb" width="813" data-original="https://pic1.zhimg.com/v2-c66658a0d417b7750e11a74b21b1d42d_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='813'%20height='290'&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="813" data-rawheight="290" class="origin_image zh-lightbox-thumb lazy" width="813" data-original="https://pic1.zhimg.com/v2-c66658a0d417b7750e11a74b21b1d42d_720w.jpg?source=d16d100b" data-actualsrc="https://pic1.zhimg.com/v2-c66658a0d417b7750e11a74b21b1d42d_720w.jpg?source=d16d100b"><figcaption>毕竟这也不是客服和后台工程师个人的错，加班加到这么晚，大家都不容易啊</figcaption></figure><p data-pid="52N_BjI-">工单结束后立刻修改了root的密码，并删除了安全组高权限的入站规则，只保留了9870、8088等常用TCP端口，保留了SSH需要的22端口。注意要将22的允许IP设置成0.0.0.0/0，否则很有可能连接不上服务器。</p><ul><li data-pid="XSo-ZNLS"><b>不知道密码的情况下如何连接本地linux PC</b></li></ul><p data-pid="sOAGkRtV">在服务器冻结期间，为了继续赶工期，实验室的兄弟借我用他的台式Linux。我看了一下sshd服务已经开启，但是密码不正确。就在本机创建了一个文件夹，把ssh公钥放进去，然后在文件夹下开了一个http服务器程序</p><div class="highlight"><pre><code class="language-bash"><span></span>python -m http.server <span class="m">80</span>
</code></pre></div><p data-pid="j71q0NZr">另外开一个窗口，查一下本机的局域网IP</p><div class="highlight"><pre><code class="language-bash"><span></span>ifconfig <span class="p">|</span> grep <span class="s2">"inet"</span>
-------------------------------------------------------------------
inet <span class="m">127</span>.0.0.1 netmask 0xff000000
inet6 ::1 prefixlen <span class="m">128</span>
inet6 fe80::1%lo0 prefixlen <span class="m">64</span> scopeid 0x1
inet6 fe80::aede:48ff:fe00:1122%en5 prefixlen <span class="m">64</span> scopeid 0x4
inet6 fe80::1c80:dd17:3748:124%en0 prefixlen <span class="m">64</span> secured scopeid 0x6
inet <span class="m">192</span>.168.1.68 netmask 0xffffff00 broadcast <span class="m">192</span>.168.1.255
inet6 fe80::5c6f:4bff:fee9:5f1%awdl0 prefixlen <span class="m">64</span> scopeid 0x7
inet6 fe80::5c6f:4bff:fee9:5f1%llw0 prefixlen <span class="m">64</span> scopeid 0x8
inet6 fe80::35a5:a66f:b764:94b3%utun0 prefixlen <span class="m">64</span> scopeid 0xe
inet6 fe80::e136:dae9:59cc:1b71%utun1 prefixlen <span class="m">64</span> scopeid 0xf
</code></pre></div><p data-pid="w0ptogAD">可以看到IP地址为<code>192.168.1.68</code> 。然后在兄弟的电脑上将我的公钥下载下来</p><div class="highlight"><pre><code class="language-bash"><span></span>wget http://192.168.1.68/id_rsa.pub
</code></pre></div><p data-pid="8kxBjM11">将id_rsa.pub的内容拷贝到authorized_users里面就可以免密ssh连接了。</p>