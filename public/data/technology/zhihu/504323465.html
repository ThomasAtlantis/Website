<h2>问题来源</h2><p data-pid="Mbh5okTH">最近读到一篇模型蒸馏的文章<sup data-text="Group knowledge transfer: Federated learning of large cnns at the edge" data-url="https://proceedings.neurips.cc/paper/2020/file/a1d4c20b182ad7137ab3606f0e3fc8a4-Paper.pdf" data-draft-node="inline" data-draft-type="reference" data-numero="1">[1]</sup>，其中在设计软标签的损失函数时使用了一种特殊的softmax：</p><p data-pid="GAvLFSjP"><img src="https://www.zhihu.com/equation?tex=q_i%3D%5Cfrac%7B%5Cexp%28z_i%2FT%29%7D%7B%5Csum_%7Bj%7D%5Cexp%28z_j%2FT%29%7D%5C%5C" alt="q_i=\frac{\exp(z_i/T)}{\sum_{j}\exp(z_j/T)}\\" eeimg="1"> </p><p data-pid="j2X2tGhK">文章中只是简单的提了一下，其中 <img src="https://www.zhihu.com/equation?tex=T" alt="T" eeimg="1"> 是softmax函数的温度超参数，而没有做过多解释。这说明这种用法并非其首创，应该是流传已久。经过一番调研和学习，发现知乎上最高赞的文章 <a href="https://zhuanlan.zhihu.com/p/132785733" class="internal" target="_blank">酱紫啊：深度学习中的temperature parameter是什么</a> 对超参数 <img src="https://www.zhihu.com/equation?tex=T" alt="T" eeimg="1"> 的讲解具有很强的误导性，所以在此重新写一篇文章为其正名。</p><p data-pid="o95woCSg">本文的标题有两个双关。一个是知识蒸馏的方法用于深度学习，同时也需要深入学习；另一个则是本文的核心：蒸馏中如何合理运用温度，让隐藏的知识更好地挥发和凝结。下面我将详细讲解以上softmax公式中温度系数的由来以及它起到的作用。</p><h2>模型蒸馏</h2><p data-pid="i6TgbAN_">模型蒸馏或知识蒸馏，最早在2006年由Buciluǎ在文章<a href="http://link.zhihu.com/?target=https%3A//dl.acm.org/doi/abs/10.1145/1150402.1150464" class=" wrap external" target="_blank" rel="nofollow noreferrer">Model Compression</a>中提出（很多博主把人名都写错了。其后，Hinton进行了归纳和发展，并在2015年发表了经典之作<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1503.02531" class=" wrap external" target="_blank" rel="nofollow noreferrer">Distilling the Knowledge in a Neural Network</a>。正是在这篇文章<sup data-text="Distilling the Knowledge in a Neural Network" data-url="https://arxiv.org/abs/1503.02531" data-draft-node="inline" data-draft-type="reference" data-numero="2">[2]</sup>中，Hinton首次提出了Softmax with Temperature的方法。</p><p data-pid="kCLqEMXa">先简要概括一下模型蒸馏在做什么。出于计算资源的限制或效率的要求，深度学习模型在部署推断时往往需要进行压缩，模型蒸馏是其中一种常见方法。将原始数据集上训练的重量级（cumbersome）模型作为教师，让一个相对更轻量的模型作为学生。对于相同的输入，让学生输出的概率分布尽可能的逼近教师输出的分布，则大模型的知识就通过这种监督训练的方式「蒸馏」到了小模型里。小模型的准确率往往下降很小，却能大幅度减少参数量，从而降低推断时对CPU、内存、能耗等资源的需求。</p><p data-pid="u8DTcDUz">对于传统的监督训练，损失函数可以写为KL-散度 <img src="https://www.zhihu.com/equation?tex=KL%28p%7C%7Cq%29" alt="KL(p||q)" eeimg="1"> ，表示用分布 <img src="https://www.zhihu.com/equation?tex=q" alt="q" eeimg="1"> 拟合分布 <img src="https://www.zhihu.com/equation?tex=p" alt="p" eeimg="1"> 带来的误差。其中 <img src="https://www.zhihu.com/equation?tex=p" alt="p" eeimg="1"> 是输出的真实分布，我们的数据集的标签 <img src="https://www.zhihu.com/equation?tex=y" alt="y" eeimg="1"> 就从这个分布中采样而来，对于一个 <img src="https://www.zhihu.com/equation?tex=K" alt="K" eeimg="1"> 分类问题， <img src="https://www.zhihu.com/equation?tex=y_i" alt="y_i" eeimg="1"> 常常会表示为one-hot向量，包含1个1和 <img src="https://www.zhihu.com/equation?tex=K+-+1" alt="K - 1" eeimg="1"> 个0。对于模型蒸馏，损失函数可以表示为 <img src="https://www.zhihu.com/equation?tex=KL%28q_t%7C%7Cq_s%29" alt="KL(q_t||q_s)" eeimg="1"> ，表示用学生模型的输出 <img src="https://www.zhihu.com/equation?tex=q_s" alt="q_s" eeimg="1"> 来拟合教师模型的输出 <img src="https://www.zhihu.com/equation?tex=q_t" alt="q_t" eeimg="1"> 。</p><p data-pid="3Ml0Jk_-">我们知道模型在训练收敛后，往往通过softmax的输出不会是完全符合one-hot向量那种极端分布的，而是在各个类别上均有概率，推断时通过argmax取得概率最大的类别。Hinton的文章就指出，教师模型中在这些负类别（非正确类别）上输出的概率分布包含了一定的隐藏信息。比如MNIST手写数字识别，标签为7的样本在输出时，类别7的概率虽然最大，但和类别1的概率更加接近，这就说明1和7很像，这是模型已经学到的隐藏的知识。</p><p data-pid="PsLYqRgn">我们在使用softmax的时候往往会将一个差别不大的输出变成很极端的分布，用一个三分类模型的输出举例：</p><p data-pid="iU3ftMz9"><img src="https://www.zhihu.com/equation?tex=%5B10%2C+11%2C+12%5D%5Crightarrow+%5B0.0900%2C+0.2447%2C+0.6652%5D%5C%5C" alt="[10, 11, 12]\rightarrow [0.0900, 0.2447, 0.6652]\\" eeimg="1"> </p><p data-pid="Nk71cZOs">可以看到原本的分布很接近均匀分布，但经过softmax，不同类别的概率相差很大。这就导致类别间的隐藏的相关性信息不再那么明显，有谁知道0.09和0.24对应的类别很像呢？为了解决这个问题，我们就引入了温度系数。</p><h2>温度系数</h2><p data-pid="CLyu_wm1">我们看看对于随机生成的相同的模型输出，经过不同的函数处理，分布会如何变化：</p><figure data-size="normal"><noscript><img src="https://pica.zhimg.com/v2-9ca70c0be16f1867678fadc8f57c9efd_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="2000" data-rawheight="1000" class="origin_image zh-lightbox-thumb" width="2000" data-original="https://picx.zhimg.com/v2-9ca70c0be16f1867678fadc8f57c9efd_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='2000'%20height='1000'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="2000" data-rawheight="1000" class="origin_image zh-lightbox-thumb lazy" width="2000" data-original="https://picx.zhimg.com/v2-9ca70c0be16f1867678fadc8f57c9efd_720w.jpg?source=d16d100b" data-actualsrc="https://pica.zhimg.com/v2-9ca70c0be16f1867678fadc8f57c9efd_720w.jpg?source=d16d100b"></figure><p data-pid="osZGIAsK">最左边是我们随机生成的分布来模拟模型的输出： <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bz%7D%5Cin+%5Cmathbb%7BR%7D+%5E%7B10%7D+%5Csim+N%2810%2C+2%29" alt="\mathbf{z}\in \mathbb{R} ^{10} \sim N(10, 2)" eeimg="1"> 。中间五幅图是使用softmax得到的结果；其中温度系数 <img src="https://www.zhihu.com/equation?tex=T%3D1" alt="T=1" eeimg="1"> 时相当于原始的softmax；右侧对比了argmax得到的结果。可以看出，从左到右，这些输出结果逐渐从均匀分布向尖锐分布过渡，其中保留的除正确类别以外的信息越来越少。下图<sup data-text="PR-009: Distilling the Knowledge in a Neural Network (Slide: English, Speaking: Korean)" data-url="https://www.youtube.com/watch?v=tOItokBZSfU" data-draft-node="inline" data-draft-type="reference" data-numero="3">[3]</sup>更加直观地展示了不同的温度系数 <img src="https://www.zhihu.com/equation?tex=T" alt="T" eeimg="1"> 对输出分布的影响。</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-261ac1f886861ab40a7d5c4a566d4746_720w.jpg?source=d16d100b" data-size="normal" data-rawwidth="2000" data-rawheight="1000" class="origin_image zh-lightbox-thumb" width="2000" data-original="https://picx.zhimg.com/v2-261ac1f886861ab40a7d5c4a566d4746_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='2000'%20height='1000'&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="2000" data-rawheight="1000" class="origin_image zh-lightbox-thumb lazy" width="2000" data-original="https://picx.zhimg.com/v2-261ac1f886861ab40a7d5c4a566d4746_720w.jpg?source=d16d100b" data-actualsrc="https://pic1.zhimg.com/v2-261ac1f886861ab40a7d5c4a566d4746_720w.jpg?source=d16d100b"><figcaption>灵感来源：https://www.youtube.com/watch?v=tOItokBZSfU</figcaption></figure><p data-pid="0ab7TyFh">不同的曲线代表不同类别上的概率输出，同样 <img src="https://www.zhihu.com/equation?tex=T%3D1" alt="T=1" eeimg="1"> 时代表传统的softmax，在 <img src="https://www.zhihu.com/equation?tex=T%3C1" alt="T&lt;1" eeimg="1"> 时，分布逐渐极端化，最终等价于argmax，在 <img src="https://www.zhihu.com/equation?tex=T%3E1" alt="T&gt;1" eeimg="1"> 时，分布逐渐趋于均匀分布，10个类别的概率都趋近于1/10。</p><p data-pid="S3pz9S8b">这两幅图很好的说明了softmax的本质。相对于argmax这种直接取最大的「hardmax」，softmax采用更温和的方式，将正确类别的概率一定程度地突显出来。而引入温度系数的本质目的，就是让softmax的soft程度变成可以调节的超参数。</p><p data-pid="6ajdzpsC">而至于这个系数为啥叫Temperature，其实很有深意。我们知道这个场景最早用于模型蒸馏，一般来说<b>蒸馏需要加热，而加热会导致熵增</b>。我们发现，<b>提高温度系数会导致输出分布的信息熵增大！</b><sup data-text="What is the role of temperature in Softmax?" data-url="https://stats.stackexchange.com/questions/527080/what-is-the-role-of-temperature-in-softmax#answer-527082" data-draft-node="inline" data-draft-type="reference" data-numero="4">[4]</sup>而在Hinton的这篇论文里，为了充分利用教师模型负类别的dark信息，一般会选用一个较高的温度系数，这也是本文标题叫做高温蒸馏的原因。</p><p data-pid="PdOMQ5BV">我们可以轻松地推导出 <img src="https://www.zhihu.com/equation?tex=T" alt="T" eeimg="1"> 趋于无穷大时，分布将趋于均匀分布，此时信息熵趋于最大</p><p data-pid="yDVimh_e"><img src="https://www.zhihu.com/equation?tex=%5Clim_%7BT%5Crightarrow+%2B%5Cinfty%7D%5Cfrac%7B%5Cexp%28z_i%2FT%29%7D%7B%5Csum_%7Bj%7D%5Cexp%28z_j%2FT%29%7D%3D%5Cfrac%7B1%7D%7B%5Csum_%7Bj%7D1%7D%3D%5Cfrac%7B1%7D%7BK%7D%5C%5C" alt="\lim_{T\rightarrow +\infty}\frac{\exp(z_i/T)}{\sum_{j}\exp(z_j/T)}=\frac{1}{\sum_{j}1}=\frac{1}{K}\\" eeimg="1"> </p><p data-pid="83vzTJW9">而当 <img src="https://www.zhihu.com/equation?tex=T" alt="T" eeimg="1"> 趋于0时，正确类别的概率接近1，softmax的效果逼近argmax</p><p data-pid="O4Ech1ND"><img src="https://www.zhihu.com/equation?tex=%5Clim_%7BT%5Crightarrow+0%7D%5Cfrac%7B%5Cexp%28z_%7Bm%7D%2FT%29%7D%7B%5Csum_%7Bj%7D%5Cexp%28z_j%2FT%29%7D%3D%5Clim_%7BT%5Crightarrow+0%7D%5Cfrac%7B1%7D%7B1%2B%5Csum_%7Bj%5Cneq+m%7D%5Cexp%28%5Cfrac%7B1%7D%7BT%7D%28z_j-z_m%29%29%7D%3D1%5C%5C" alt="\lim_{T\rightarrow 0}\frac{\exp(z_{m}/T)}{\sum_{j}\exp(z_j/T)}=\lim_{T\rightarrow 0}\frac{1}{1+\sum_{j\neq m}\exp(\frac{1}{T}(z_j-z_m))}=1\\" eeimg="1"> </p><h2>反对意见</h2><p data-pid="jFnVP7F3">在最高赞的那篇文章中提到</p><blockquote data-pid="fnp7uycy">如果我们在训练时将t设置比较大，那么预测的概率分布会比较平滑，<b>那么loss会很大</b></blockquote><p data-pid="9OacBFJ3">首先，如果原文考虑的问题中数据的标签是one-hot向量而不是蒸馏这种软标签， <img src="https://www.zhihu.com/equation?tex=T" alt="T" eeimg="1"> 较大时loss确实会较大，因为输出分布比较均匀，不能很好地凸显正类别上的概率优势。但在蒸馏时并非如此，Hinton给出的Loss函数如下图<sup data-text="Knowledge Distillation on NNI" data-url="https://nni.readthedocs.io/en/stable/sharings/kd_example.html" data-draft-node="inline" data-draft-type="reference" data-numero="5">[5]</sup>所示，分为两项：</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-bf1728889401a0cde2ceedaf6f67299d_720w.jpg?source=d16d100b" data-size="normal" data-rawwidth="2310" data-rawheight="740" class="origin_image zh-lightbox-thumb" width="2310" data-original="https://picx.zhimg.com/v2-bf1728889401a0cde2ceedaf6f67299d_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='2310'%20height='740'&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="2310" data-rawheight="740" class="origin_image zh-lightbox-thumb lazy" width="2310" data-original="https://picx.zhimg.com/v2-bf1728889401a0cde2ceedaf6f67299d_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-bf1728889401a0cde2ceedaf6f67299d_720w.jpg?source=d16d100b"><figcaption>图源：https://nni.readthedocs.io/en/stable/sharings/kd_example.html</figcaption></figure><p data-pid="RSaN73w5">第一项 <img src="https://www.zhihu.com/equation?tex=L_1" alt="L_1" eeimg="1"> 是教师模型与学生模型的输出之间的交叉熵，第二项 <img src="https://www.zhihu.com/equation?tex=L_2" alt="L_2" eeimg="1"> 是学生模型与真实标签之间的交叉熵。传统训练模型时只有 <img src="https://www.zhihu.com/equation?tex=L_2" alt="L_2" eeimg="1"> 项，所以 <img src="https://www.zhihu.com/equation?tex=L_1" alt="L_1" eeimg="1"> 可以看做是引入的正则项。文中指出这个正则项使得学生模型能够学到教师模型中的高度泛化的知识，从而需要更少的真实训练样本。文中的实验只用了3%的训练样本，就达到了近似教师模型的准确率。我们可以看到这里的 <img src="https://www.zhihu.com/equation?tex=L_1" alt="L_1" eeimg="1"> 项中，两个模型都使用了同样的、较大的温度系数 <img src="https://www.zhihu.com/equation?tex=t" alt="t" eeimg="1"> ，对输出的作用是相同的，未必会使loss变大。</p><blockquote data-pid="NqstxZBX">... 那么loss会很大，<b>这样可以避免我们陷入局部最优解。</b></blockquote><p data-pid="BIe_OADc">为什么loss大就可以避免陷入局部最优呢？我猜作者想表达的是loss很大，从而随机梯度下降的时候梯度很大，步长就会很大，从而更容易跳出局部最优。该文章的评论区也有同样的声音，但可惜这并不正确。我们还以硬标签 <img src="https://www.zhihu.com/equation?tex=y" alt="y" eeimg="1"> 监督训练为例，使用交叉熵损失函数，设softmax的输出为 <img src="https://www.zhihu.com/equation?tex=q" alt="q" eeimg="1"> ，我们可以推导loss对于模型输出 <img src="https://www.zhihu.com/equation?tex=z" alt="z" eeimg="1"> 的梯度：</p><p data-pid="2BwjMfT4"><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L%7D%7B%5Cpartial+z_i%7D%3D%5Csum_%7Bj%7D%5Cfrac%7B%5Cpartial+L%7D%7B%5Cpartial+q_j%7D%5Cfrac%7B%5Cpartial+q_j%7D%7B%5Cpartial+z_i%7D%5C%5C" alt="\frac{\partial L}{\partial z_i}=\sum_{j}\frac{\partial L}{\partial q_j}\frac{\partial q_j}{\partial z_i}\\" eeimg="1"> </p><p data-pid="0W9nwrlY">交叉熵的梯度</p><p data-pid="8iiXkET2"><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L%7D%7B%5Cpartial+q_j%7D%3D-%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+q_j%7D%28y_j%5Clog+q_j%29%3D-%5Cfrac%7By_j%7D%7Bq_j%7D%5C%5C" alt="\frac{\partial L}{\partial q_j}=-\frac{\partial}{\partial q_j}(y_j\log q_j)=-\frac{y_j}{q_j}\\" eeimg="1"> </p><p data-pid="V93wyWEE">softmax的梯度</p><p data-pid="nX7z9h91"><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+q_j%7D%7B%5Cpartial+z_i%7D%3D%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+z_i%7D%5Cleft%28%5Cfrac%7B%5Cexp%28z_j%2FT%29%7D%7B%5Csum_%7Bj%7D%5Cexp%28z_j%2FT%29%7D%5Cright%29%5C%5C" alt="\frac{\partial q_j}{\partial z_i}=\frac{\partial}{\partial z_i}\left(\frac{\exp(z_j/T)}{\sum_{j}\exp(z_j/T)}\right)\\" eeimg="1"> </p><p data-pid="al9ZB2v0">当 <img src="https://www.zhihu.com/equation?tex=j%3Di" alt="j=i" eeimg="1"> 时</p><p data-pid="b-RqW4jU"><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cfrac%7B%5Cpartial+q_i%7D%7B%5Cpartial+z_i%7D%26%3D%5Cfrac%7B%5Cfrac%7B1%7D%7BT%7D%5Cexp%28z_i%2FT%29%5Csum_j%5Cexp%28z_j%2FT%29-%5Cfrac%7B1%7D%7BT%7D%5Cexp%5E2%28z_i%2FT%29%7D%7B%5Cleft%28%5Csum_j%5Cexp%28z_j%2FT%29%5Cright%29%5E2%7D%3D%5Cfrac%7B1%7D%7BT%7Dq_i%281-q_i%29+%5Cend%7Balign%2A%7D%5C%5C" alt="\begin{align*} \frac{\partial q_i}{\partial z_i}&amp;=\frac{\frac{1}{T}\exp(z_i/T)\sum_j\exp(z_j/T)-\frac{1}{T}\exp^2(z_i/T)}{\left(\sum_j\exp(z_j/T)\right)^2}=\frac{1}{T}q_i(1-q_i) \end{align*}\\" eeimg="1"> </p><p data-pid="9Bl1CUvT">当 <img src="https://www.zhihu.com/equation?tex=j%5Cneq+i" alt="j\neq i" eeimg="1"> 时</p><p data-pid="8Oohcnki"><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cfrac%7B%5Cpartial+q_j%7D%7B%5Cpartial+z_i%7D%26%3D%5Cfrac%7B-%5Cfrac%7B1%7D%7BT%7D%5Cexp%28z_i%2FT%29%5Cexp%28z_j%2FT%29%7D%7B%5Cleft%28%5Csum_j%5Cexp%28z_j%2FT%29%5Cright%29%5E2%7D%3D-%5Cfrac%7B1%7D%7BT%7Dq_iq_j+%5Cend%7Balign%2A%7D%5C%5C" alt="\begin{align*} \frac{\partial q_j}{\partial z_i}&amp;=\frac{-\frac{1}{T}\exp(z_i/T)\exp(z_j/T)}{\left(\sum_j\exp(z_j/T)\right)^2}=-\frac{1}{T}q_iq_j \end{align*}\\" eeimg="1"> </p><p data-pid="nDtnDiPv">代入链式法则，最终的梯度为（推导参考了<sup data-text="softmax, CrossEntropyLoss 与梯度计算公式" data-url="https://blog.csdn.net/jiongjiongai/article/details/88324000" data-draft-node="inline" data-draft-type="reference" data-numero="6">[6]</sup><sup data-text="关于Softmax的数值稳定性和梯度反向传播" data-url="https://zhuanlan.zhihu.com/p/92714192" data-draft-node="inline" data-draft-type="reference" data-numero="7">[7]</sup>）</p><p data-pid="NRfYkr-u"><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cfrac%7B%5Cpartial+L%7D%7B%5Cpartial+z_i%7D%26%3D-%5Csum_%7Bj%5Cneq+i%7D%5Cfrac%7By_j%7D%7Bq_j%7D%5Cfrac%7B1%7D%7BT%7D%28-q_iq_j%29-%5Cfrac%7By_i%7D%7Bq_i%7D%5Cfrac%7B1%7D%7BT%7Dq_i%281-q_i%29%5C%5C+%26%3D%5Cfrac%7B1%7D%7BT%7D%5Csum_%7Bj%5Cneq+i%7Dq_iy_j-%5Cfrac%7B1%7D%7BT%7Dy_i%281-q_i%29%5C%5C+%26%3D%5Cfrac%7B1%7D%7BT%7Dq_i%5Csum_jy_j-%5Cfrac%7B1%7D%7BT%7Dy_i%3D%5Cfrac%7B1%7D%7BT%7D%28q_i-y_i%29%5C%5C+%26%3D%5Cfrac%7B1%7D%7BT%7D%5Cleft%28%5Cfrac%7B%5Cexp%28z_i%2FT%29%7D%7B%5Csum_%7Bj%7D%5Cexp%28z_j%2FT%29%7D-y_i%5Cright%29+%5Cend%7Balign%2A%7D%5C%5C" alt="\begin{align*} \frac{\partial L}{\partial z_i}&amp;=-\sum_{j\neq i}\frac{y_j}{q_j}\frac{1}{T}(-q_iq_j)-\frac{y_i}{q_i}\frac{1}{T}q_i(1-q_i)\\ &amp;=\frac{1}{T}\sum_{j\neq i}q_iy_j-\frac{1}{T}y_i(1-q_i)\\ &amp;=\frac{1}{T}q_i\sum_jy_j-\frac{1}{T}y_i=\frac{1}{T}(q_i-y_i)\\ &amp;=\frac{1}{T}\left(\frac{\exp(z_i/T)}{\sum_{j}\exp(z_j/T)}-y_i\right) \end{align*}\\" eeimg="1"> </p><p data-pid="AnF-227X">显然标签 <img src="https://www.zhihu.com/equation?tex=y_i" alt="y_i" eeimg="1"> 与softmax的输出 <img src="https://www.zhihu.com/equation?tex=q_i" alt="q_i" eeimg="1"> 之差不总能增长 <img src="https://www.zhihu.com/equation?tex=T" alt="T" eeimg="1"> 倍，大家可以自己举一些反例，会发现大多数情况下，梯度都不是增大的。那么对于Hinton这篇论文，由于loss的数量级没有变化，所以梯度实际是减小的，所以文章中特意强调了要将系数 <img src="https://www.zhihu.com/equation?tex=%5Cbeta" alt="\beta" eeimg="1"> 设置大一些来补偿，比如设置为 <img src="https://www.zhihu.com/equation?tex=T%5E2" alt="T^2" eeimg="1"> ，在这里给出的<a href="http://link.zhihu.com/?target=https%3A//nni.readthedocs.io/en/stable/sharings/kd_example.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">Pytorch实现</a>中也是这么做的。</p><p data-pid="_zBuLUx6">文章中给出了一个高温情况下的等价，在 <img src="https://www.zhihu.com/equation?tex=T%5Crightarrow+%2B%5Cinfty" alt="T\rightarrow +\infty" eeimg="1"> 时，利用等价无穷小或者是泰勒展开得到：</p><p data-pid="mYFeN9fw"><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7BT%7D%5Cleft%28%5Cfrac%7B%5Cexp%28z_i%2FT%29%7D%7B%5Csum_%7Bj%7D%5Cexp%28z_j%2FT%29%7D-%5Cfrac%7B%5Cexp%28p_i%2FT%29%7D%7B%5Csum_%7Bj%7D%5Cexp%28p_j%2FT%29%7D%5Cright%29%5Capprox+%5Cfrac%7B1%7D%7BT%7D%5Cleft%28%5Cfrac%7B1+%2B+z_i%2FT%7D%7BK%2B%5Csum_%7Bj%7Dz_j%2FT%7D-%5Cfrac%7B1+%2B+p_i%2FT%7D%7BK%2B%5Csum_%7Bj%7Dp_j%2FT%7D%5Cright%29%3D%5Cfrac%7B1%7D%7BKT%5E2%7D%28z_i-p_i%29%5C%5C" alt="\frac{1}{T}\left(\frac{\exp(z_i/T)}{\sum_{j}\exp(z_j/T)}-\frac{\exp(p_i/T)}{\sum_{j}\exp(p_j/T)}\right)\approx \frac{1}{T}\left(\frac{1 + z_i/T}{K+\sum_{j}z_j/T}-\frac{1 + p_i/T}{K+\sum_{j}p_j/T}\right)=\frac{1}{KT^2}(z_i-p_i)\\" eeimg="1"> </p><p data-pid="f3cNkk5g">可以清晰的看出这里是 <img src="https://www.zhihu.com/equation?tex=T%5E2" alt="T^2" eeimg="1"> 的关系。</p><blockquote data-pid="JuDpGzPs">随着训练的进行，我们将t变小，也可以称作降温，类似于模拟退火算法，这也是为什么要把t称作温度参数的原因。变小模型才能收敛。</blockquote><p data-pid="0hsqX7Oh">我不知道将这里的温度系数类比模拟退火算法的温度系数有什么依据（Quora上有个类似的<sup data-text="What is the temperature parameter in deep learning?" data-url="https://www.quora.com/What-is-the-temperature-parameter-in-deep-learning" data-draft-node="inline" data-draft-type="reference" data-numero="8">[8]</sup>），但它们真的是不怎么像。同样也未必是温度系数变小模型才能收敛，需要分情况：如果是模型蒸馏， <img src="https://www.zhihu.com/equation?tex=L_1" alt="L_1" eeimg="1"> 项始终都使用较大的温度；如果是使用真实标签训练，确实选取较小的温度系数，更利于模型收敛。<b>可以这样理解，温度系数较大时，模型需要训练得到一个很陡峭的输出，经过softmax之后才能获得一个相对陡峭的结果；温度系数较小时，模型输出稍微有点起伏，softmax就很敏感地把分布变得尖锐，认为模型学到了知识。</b></p><p data-pid="hD60D6XI">所以，使用一个固定的小于1的温度系数是合理的，这也是那篇文章里提到的推荐系统所做的，它没有降温过程，直接设置了 <img src="https://www.zhihu.com/equation?tex=T%3D0.05" alt="T=0.05" eeimg="1"> 。如果大家在哪篇文章中看到了降温过程，还请在评论区指正。</p><h2>其他场景</h2><p data-pid="JIrhbDBd">这里我们天马行空地设想一个场景：在一些序列生成任务中，比如seq2seq的机器翻译模型，或者是验证码识别的CTC算法<sup data-text="详解CTC" data-url="https://zhuanlan.zhihu.com/p/42719047" data-draft-node="inline" data-draft-type="reference" data-numero="9">[9]</sup>中，输出的每一个时间步都会有一个分布。最终的序列会使用BeamSearch<sup data-text="文本生成解码之 Beam Search" data-url="https://zhuanlan.zhihu.com/p/43703136" data-draft-node="inline" data-draft-type="reference" data-numero="10">[10]</sup>或者Viterbi<sup data-text="如何通俗地讲解 viterbi 算法？" data-url="https://www.zhihu.com/question/20136144/answer/763021768" data-draft-node="inline" data-draft-type="reference" data-numero="11">[11]</sup>等算法搜索Top-K概率的序列。这类方法介于逐时间步argmax的完全贪心策略和全局动态规划的优化策略之间。虽然BeamSearch中我们不需要提前softmax，但<b>假如</b>我们做了带温度系数的softmax，就可以控制输出分布的尖锐程度。对于这类逐步计算累积概率的算法，在每个时间步的概率分布较为均匀时就容易输出不同的结果。所以在这类问题下，高温可能导致输出序列的多样性。</p><p data-pid="ox-KIRb-">对于这类场景，我没有进行严格证明也没有很深的经验，只是一个猜想。这里有类似的说法<sup data-text="What is Temperature in LSTM?" data-url="https://www.quora.com/What-is-Temperature-in-LSTM" data-draft-node="inline" data-draft-type="reference" data-numero="12">[12]</sup>，但都不能作为参考依据。大家感兴趣的话可以将softmax with temperature引入BeamSearch看看会不会对输出的丰富性造成影响。假如算法只依赖每个时间步的概率大小关系，那输出就是确定的，说明我们猜想失败。或者有相关经验的同学也可以在评论区给出参考文献。</p><h2>后话</h2><p data-pid="LxFYVYxm">写完这篇文章才发现，<a href="https://zhuanlan.zhihu.com/p/102038521" class="internal" target="_blank">潘小小【经典简读】知识蒸馏(Knowledge Distillation) 经典之作</a>一文中已有类似的探讨。尽管如此，我相信这篇文章还是可以起到一定的科普作用，让那些和我一样对知识蒸馏不太了解的同学，从温度系数这个关键词入手，能够快速得到想要的答案。</p><p data-pid="NihHh4vv">读完Hinton的文章，有两个强烈的感受：一是感觉他太牛了，3句话让我读了18遍，全文很少用公式，基本没有配图，但把算法讲得清清楚楚；二就是，他的写作中长从句实在太多了，一句话60个单词，读起来很不友好。如果对这篇文章感兴趣，也可以看上面潘小小的那篇解读。文章最后讲到了一种和MOE很像的分布式集成学习方法，在潘的文章中没有介绍，由于这不是今天的主题，所以我也没用笔墨，大家如果对这部分感兴趣也可以来找我讨论。</p><p data-pid="oL3LRydU">说出来很难相信，我其实不是做AI方向的，我是做系统的，所以欢迎大家怼我 (°ー°〃)。写着写着就写了这么多，好了，滚去继续读论文了。</p><p data-pid="i-ZcMfAH"><b>2023年4月26日更新：图一的绘制代码</b></p><div class="highlight"><pre><code class="language-python3"><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib.colors</span> <span class="k">as</span> <span class="nn">colors</span>
<span class="kn">from</span> <span class="nn">matplotlib.font_manager</span> <span class="kn">import</span> <span class="n">FontProperties</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">font</span> <span class="o">=</span> <span class="n">FontProperties</span><span class="p">(</span>
    <span class="n">fname</span><span class="o">=</span><span class="s2">"/System/Library/Fonts/Supplemental/Courier New Bold.ttf"</span><span class="p">,</span>
    <span class="n">weight</span><span class="o">=</span><span class="s1">'bold'</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">'normal'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">10</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">softmax</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">bar_width</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s2">"Set3"</span><span class="p">)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="p">[</span>
    <span class="c1"># np.ones_like(y) / y.shape[0] ,</span>
    <span class="p">[</span><span class="n">y</span><span class="p">,</span> <span class="s2">"origin"</span><span class="p">],</span>
    <span class="p">[</span><span class="n">softmax</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">y</span> <span class="o">/</span> <span class="mi">8</span><span class="p">)),</span> <span class="s2">"T=8.0"</span><span class="p">],</span>
    <span class="p">[</span><span class="n">softmax</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">y</span> <span class="o">/</span> <span class="mi">4</span><span class="p">)),</span> <span class="s2">"T=4.0"</span><span class="p">],</span>
    <span class="p">[</span><span class="n">softmax</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">y</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)),</span> <span class="s2">"T=2.0"</span><span class="p">],</span>
    <span class="p">[</span><span class="n">softmax</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">y</span>    <span class="p">)),</span> <span class="s2">"T=1.0"</span><span class="p">],</span>
    <span class="p">[</span><span class="n">softmax</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)),</span> <span class="s2">"T=0.5"</span><span class="p">],</span>
    <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y</span><span class="p">)],</span> <span class="s2">"argmax"</span><span class="p">],</span>
<span class="p">]</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">width_ratios</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span><span class="o">+</span><span class="mi">2</span><span class="p">)</span>
<span class="n">width_ratios</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.4</span>
<span class="n">width_ratios</span><span class="p">[</span><span class="mi">7</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.4</span>
<span class="n">gridspec</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">width_ratios</span><span class="o">=</span><span class="n">width_ratios</span><span class="p">)</span>
<span class="n">axs</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span><span class="o">+</span><span class="mi">2</span><span class="p">),</span> <span class="n">gridspec_kw</span><span class="o">=</span><span class="n">gridspec</span><span class="p">)</span>
<span class="n">indexes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">7</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">indexes</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">indexes</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
<span class="n">index</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">indexes</span><span class="p">)</span>

<span class="k">for</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">ys</span><span class="p">:</span>
    <span class="n">i</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">bar_width</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="n">cmap</span><span class="p">(</span><span class="n">i</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">ys</span><span class="p">)),</span>
        <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">,</span>
        <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">,</span>
        <span class="n">lw</span><span class="o">=</span><span class="mi">1</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">30</span><span class="p">])</span>
    <span class="k">elif</span> <span class="n">i</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">width_ratios</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">])</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">linestyle</span><span class="o">=</span><span class="s1">':'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s1">'y'</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">3</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">width_ratios</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">tick</span> <span class="ow">in</span> <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">get_yticklabels</span><span class="p">():</span>
            <span class="n">tick</span><span class="o">.</span><span class="n">set_font_properties</span><span class="p">(</span><span class="n">font</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_ticks</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_ticklabels</span><span class="p">([])</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="mf">1.005</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="s2">"lower center"</span><span class="p">,</span> <span class="n">handlelength</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">prop</span><span class="o">=</span><span class="n">font</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s2">"fig.png"</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p></p>