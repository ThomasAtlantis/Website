<h2>引言</h2><p data-pid="aozLx1Bq">在<a href="http://link.zhihu.com/?target=https%3A//book.douban.com/subject/26976457/" class=" wrap external" target="_blank" rel="nofollow noreferrer">TensorFlow实战Google深度学习框架</a>一书的第87页写道「L1正则化的计算公式不可导，而L2正则化公式可导。因为在优化时需要计算损失函数的偏导数，所以对含有L2正则化损失函数的优化要更加简洁。」这让人不禁好奇<b>，L1正则化不可导那么怎么优化，有多复杂？</b>在<a href="http://link.zhihu.com/?target=https%3A//book.douban.com/subject/26708119/" class=" wrap external" target="_blank" rel="nofollow noreferrer">西瓜书</a>的第253页给出了一种方案，即近端梯度下降。我不会那些很深入和fancy的数学，只能简单给出一种我的理解，一种相对好懂的理解。</p><p data-pid="Wv_fG2zY">将神经网络的训练过程看做是对输入和输出之间未知的函数关系 <img src="https://www.zhihu.com/equation?tex=f" alt="f" eeimg="1"> 的拟合过程，这里假设训练数据集固定，那么我们优化的自变量 <img src="https://www.zhihu.com/equation?tex=x" alt="x" eeimg="1"> 就变为网络的权重值。加入对权重大小的L1正则惩罚项，优化目标就可以写作如下的形式：</p><p data-pid="FFGfxxPg"><img src="https://www.zhihu.com/equation?tex=%5Cmin_xf%28x%29%2B%5Clambda%5CVert+x%5CVert_1%5C%5C" alt="\min_xf(x)+\lambda\Vert x\Vert_1\\" eeimg="1"> </p><p data-pid="HmbYY4vM"><a class="member_mention" href="http://www.zhihu.com/people/26845ac513dc82a52d6a6af1afcdaf4a" data-hash="26845ac513dc82a52d6a6af1afcdaf4a" data-hovercard="p$b$26845ac513dc82a52d6a6af1afcdaf4a">@Jason Gu</a> 的回答<a href="https://www.zhihu.com/question/20700829/answer/16395087" class="internal" target="_blank">机器学习中使用正则化来防止过拟合是什么原理？</a>简练直观的说明了正则化防止过拟合的原理：「过拟合，就是拟合函数需要顾忌每一个点，最终形成的拟合函数波动很大。在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的导数绝对值非常大，由于自变量值可大可小，所以只有系数足够大，才能保证导数值很大。」</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-4a354fcda4d4e75f04c00455ec8d2f22_720w.jpg?source=d16d100b" data-size="normal" data-rawwidth="1508" data-rawheight="1024" class="origin_image zh-lightbox-thumb" width="1508" data-original="https://pic1.zhimg.com/v2-4a354fcda4d4e75f04c00455ec8d2f22_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1508'%20height='1024'&gt;&lt;/svg&gt;" data-size="normal" data-rawwidth="1508" data-rawheight="1024" class="origin_image zh-lightbox-thumb lazy" width="1508" data-original="https://pic1.zhimg.com/v2-4a354fcda4d4e75f04c00455ec8d2f22_720w.jpg?source=d16d100b" data-actualsrc="https://pic1.zhimg.com/v2-4a354fcda4d4e75f04c00455ec8d2f22_720w.jpg?source=d16d100b"><figcaption>图片来源 https://en.wikipedia.org/wiki/Overfitting</figcaption></figure><p data-pid="W07tpby9">相比L2正则化，L1正则化不仅仅限制了权重值的大小，还使得很多权重直接变为0，从而使神经网络具有稀疏性，这在网络的稀疏压缩方法中是很重要的步骤。直观地理解，2范数会使得趋近于0的权重更加趋近于0以至于忽略不计，优化器就会减少对这部分参数的优化，而1范数则会一直优化下去。如何更加准确地理解稀疏参数的问题，还需要具体观察L1正则项的优化过程。</p><h2>含有L1正则项的优化方法</h2><p data-pid="yLoZgE0_">下面按照西瓜书上给出的顺序讲解近端梯度下降法（Proximal Gradient Descent），不要问我为什么叫近端，也不要问我软阈值函数为什么软。这些数学完全是我的盲区，甚至都不知道这个公式是哪门数学里定义的。</p><p data-pid="TrfnJ1jI">对于以上公式，如果 <img src="https://www.zhihu.com/equation?tex=f%28x%29" alt="f(x)" eeimg="1"> 可导，根据L-Lipschitz条件，即存在常数 <img src="https://www.zhihu.com/equation?tex=L%3E0" alt="L&gt;0" eeimg="1"> 使得</p><p data-pid="NJMSD-02"><img src="https://www.zhihu.com/equation?tex=%5CVert+%5Cnabla+f%28x%27%29-%5Cnabla+f%28x%29%5CVert%5E2_2+%5Cleq+L%5CVert+x%27-x%5CVert%5E2_2%5Cquad%28%5Cforall+x%2Cx%27%29%5C%5C" alt="\Vert \nabla f(x')-\nabla f(x)\Vert^2_2 \leq L\Vert x'-x\Vert^2_2\quad(\forall x,x')\\" eeimg="1"> </p><p data-pid="F7eD7cql">在 <img src="https://www.zhihu.com/equation?tex=x_k" alt="x_k" eeimg="1"> 附近对 <img src="https://www.zhihu.com/equation?tex=f%28x%29" alt="f(x)" eeimg="1"> 进行二阶泰勒展开</p><p data-pid="bKPy9dWP"><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Chat%7Bf%7D%28x%29%26%5Csimeq+f%28x%5E%7B%28k%29%7D%29%2B%5Clangle%5Cnabla+f%28x%5E%7B%28k%29%7D%29%2Cx-x%5E%7B%28k%29%7D+%5Crangle%2B%5Cfrac%7BL%7D%7B2%7D%5CVert+x-x%5E%7B%28k%29%7D%5CVert_2%5E2%5C%5C+%26%3D%5Cfrac%7BL%7D%7B2%7D%28%5CVert+x-x%5E%7B%28k%29%7D%5CVert%5E2_2+%2B+2%5Clangle%5Cfrac%7B1%7D%7BL%7D%5Cnabla+f%28x%5E%7B%28k%29%7D%29%2Cx-x%5E%7B%28k%29%7D+%5Crangle%2B%5CVert%5Cfrac%7B1%7D%7BL%7D%5Cnabla+f%28x%5E%7B%28k%29%7D%29%5CVert%5E2_2%29%5C%5C+%26%5Cquad%2Bf%28x%5E%7B%28k%29%7D%29-%5Cfrac%7B1%7D%7B2L%7D%5CVert%5Cnabla+f%28x%5E%7B%28k%29%7D%29%5CVert_2%5E2%5C%5C+%26%3D%5Cfrac%7BL%7D%7B2%7D%5CVert+x-%28x%5E%7B%28k%29%7D-%5Cfrac%7B1%7D%7BL%7D%5Cnabla+f%28x%5E%7B%28k%29%7D%29%29%5CVert%5E2_2%2B%5Ctext%7BConst%7D+%5Cend%7Balign%2A%7D%5C%5C" alt="\begin{align*} \hat{f}(x)&amp;\simeq f(x^{(k)})+\langle\nabla f(x^{(k)}),x-x^{(k)} \rangle+\frac{L}{2}\Vert x-x^{(k)}\Vert_2^2\\ &amp;=\frac{L}{2}(\Vert x-x^{(k)}\Vert^2_2 + 2\langle\frac{1}{L}\nabla f(x^{(k)}),x-x^{(k)} \rangle+\Vert\frac{1}{L}\nabla f(x^{(k)})\Vert^2_2)\\ &amp;\quad+f(x^{(k)})-\frac{1}{2L}\Vert\nabla f(x^{(k)})\Vert_2^2\\ &amp;=\frac{L}{2}\Vert x-(x^{(k)}-\frac{1}{L}\nabla f(x^{(k)}))\Vert^2_2+\text{Const} \end{align*}\\" eeimg="1"> </p><p data-pid="AQswu30F">其中 <img src="https://www.zhihu.com/equation?tex=%5Clangle+%5C+%5Ccdot+%5C+%2C+%5C+%5Ccdot+%5C+%5Crangle" alt="\langle \ \cdot \ , \ \cdot \ \rangle" eeimg="1"> 表示希尔伯特内积， <img src="https://www.zhihu.com/equation?tex=%5Ctext%7BConst%7D" alt="\text{Const}" eeimg="1"> 代表与 <img src="https://www.zhihu.com/equation?tex=x" alt="x" eeimg="1"> 无关的常数， <img src="https://www.zhihu.com/equation?tex=x%5E%7B%28k%29%7D" alt="x^{(k)}" eeimg="1"> 表示第 <img src="https://www.zhihu.com/equation?tex=k" alt="k" eeimg="1"> 次迭代的权重值组成的向量。显然上式取得最小值时</p><p data-pid="L89aJP2P"><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+x%5E%7B%28k%2B1%29%7D%26%3D%5Carg+%5Cmin+_x%5Cfrac%7BL%7D%7B2%7D%5CVert+x-%28x%5E%7B%28k%29%7D-%5Cfrac%7B1%7D%7BL%7D%5Cnabla+f%28x%5E%7B%28k%29%7D%29%29%5CVert%5E2_2%5C%5C+%26%3Dx%5E%7B%28k%29%7D-%5Cfrac%7B1%7D%7BL%7D%5Cnabla+f%28x%5E%7B%28k%29%7D%29%5C%5C+%5Cend%7Balign%2A%7D%5C%5C" alt="\begin{align*} x^{(k+1)}&amp;=\arg \min _x\frac{L}{2}\Vert x-(x^{(k)}-\frac{1}{L}\nabla f(x^{(k)}))\Vert^2_2\\ &amp;=x^{(k)}-\frac{1}{L}\nabla f(x^{(k)})\\ \end{align*}\\" eeimg="1"></p><p data-pid="kGxviAU0">于是，对于 <img src="https://www.zhihu.com/equation?tex=f%28x%29" alt="f(x)" eeimg="1"> 的最小化过程可以近似于对 <img src="https://www.zhihu.com/equation?tex=%5Chat+f%28x%29" alt="\hat f(x)" eeimg="1"> 的最小化。如果使用梯度下降法进行优化，则每一步的迭代可以按照以上公式进行。将泰勒展开的方法推广到最上面的公式，我们可以得到含有L1正则项的目标函数的迭代过程</p><p data-pid="PVz3Hgdf"><img src="https://www.zhihu.com/equation?tex=x%5E%7B%28k%2B1%29%7D%3D%5Carg+%5Cmin+_x%5Cfrac%7BL%7D%7B2%7D%5CVert+x-%28x%5E%7B%28k%29%7D-%5Cfrac%7B1%7D%7BL%7D%5Cnabla+f%28x%5E%7B%28k%29%7D%29%29%5CVert%5E2_2%2B%5Clambda%5CVert+x%5CVert_1%5C%5C" alt="x^{(k+1)}=\arg \min _x\frac{L}{2}\Vert x-(x^{(k)}-\frac{1}{L}\nabla f(x^{(k)}))\Vert^2_2+\lambda\Vert x\Vert_1\\" eeimg="1"> </p><p data-pid="WT7kHJdG">对于这个式子，我们可以先计算 <img src="https://www.zhihu.com/equation?tex=z%3Dx%5E%7B%28k%29%7D-%5Cfrac%7B1%7D%7BL%7D%5Cnabla+f%28x%5E%7B%28k%29%7D%29" alt="z=x^{(k)}-\frac{1}{L}\nabla f(x^{(k)})" eeimg="1"> ，然后求解</p><p data-pid="VstC-nk4"><img src="https://www.zhihu.com/equation?tex=x%5E%7B%28k%2B1%29%7D%3D%5Carg+%5Cmin+_x%5Cfrac%7BL%7D%7B2%7D%5CVert+x-z%5CVert%5E2_2%2B%5Clambda%5CVert+x%5CVert_1%5C%5C" alt="x^{(k+1)}=\arg \min _x\frac{L}{2}\Vert x-z\Vert^2_2+\lambda\Vert x\Vert_1\\" eeimg="1"> </p><p data-pid="TOZtXDxn">将上式写作分量形式</p><p data-pid="WzHOSLgw"><img src="https://www.zhihu.com/equation?tex=x%5E%7B%28k%2B1%29%7D_i%3D%5Carg+%5Cmin+_%7Bx_i%7D%5Cfrac%7BL%7D%7B2%7D%28x_i-z_i%29%5E2%2B%5Clambda%7C+x_i%7C%5C%5C" alt="x^{(k+1)}_i=\arg \min _{x_i}\frac{L}{2}(x_i-z_i)^2+\lambda| x_i|\\" eeimg="1"> </p><p data-pid="EtUeDy-a">可以看出公式中不存在 <img src="https://www.zhihu.com/equation?tex=x_i" alt="x_i" eeimg="1"> 之间的交叉项，所以权重向量的各个分量的优化是独立的。由于绝对值项导数不连续，我们对 <img src="https://www.zhihu.com/equation?tex=x_i" alt="x_i" eeimg="1"> 的取值分类讨论。令 <img src="https://www.zhihu.com/equation?tex=g%28x_i%29%3DL%28x_i-z_i%29%5E2%2F2%2B%5Clambda%7C+x_i%7C" alt="g(x_i)=L(x_i-z_i)^2/2+\lambda| x_i|" eeimg="1"> ，当 <img src="https://www.zhihu.com/equation?tex=x_i%3E0" alt="x_i&gt;0" eeimg="1"> 时，</p><p data-pid="ucMSqnGD"><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+g%28x_i%29%26%3D%5Cfrac%7BL%7D%7B2%7D%28x_i-z_i%29%5E2%2B%5Clambda+x_i%5C%5C+%5Cfrac%7B%5Cpartial+g%28x_i%29%7D%7B%5Cpartial+x_i%7D%26%3DL%28x_i-z_i%29%2B%5Clambda%3D0%5C%5C+x%5E%7B%28k%2B1%29%7D_i+%26%3D+z_i-%5Cfrac%7B%5Clambda%7D%7BL%7D%3E0%5CRightarrow+z_i%3E%5Cfrac%7B%5Clambda%7D%7BL%7D+%5Cend%7Balign%2A%7D%5C%5C" alt="\begin{align*} g(x_i)&amp;=\frac{L}{2}(x_i-z_i)^2+\lambda x_i\\ \frac{\partial g(x_i)}{\partial x_i}&amp;=L(x_i-z_i)+\lambda=0\\ x^{(k+1)}_i &amp;= z_i-\frac{\lambda}{L}&gt;0\Rightarrow z_i&gt;\frac{\lambda}{L} \end{align*}\\" eeimg="1"> </p><p data-pid="Bupi6K8s">同理，当 <img src="https://www.zhihu.com/equation?tex=x_i%3C0" alt="x_i&lt;0" eeimg="1"> 时，</p><p data-pid="JeL1PADo"><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+g%28x_i%29%26%3D%5Cfrac%7BL%7D%7B2%7D%28x_i-z_i%29%5E2-%5Clambda+x_i%5C%5C+%5Cfrac%7B%5Cpartial+g%28x_i%29%7D%7B%5Cpartial+x_i%7D%26%3DL%28x_i-z_i%29-%5Clambda%3D0%5C%5C+x%5E%7B%28k%2B1%29%7D_i+%26%3D+z_i%2B%5Cfrac%7B%5Clambda%7D%7BL%7D%3C0%5CRightarrow+z_i%3C-%5Cfrac%7B%5Clambda%7D%7BL%7D+%5Cend%7Balign%2A%7D%5C%5C" alt="\begin{align*} g(x_i)&amp;=\frac{L}{2}(x_i-z_i)^2-\lambda x_i\\ \frac{\partial g(x_i)}{\partial x_i}&amp;=L(x_i-z_i)-\lambda=0\\ x^{(k+1)}_i &amp;= z_i+\frac{\lambda}{L}&lt;0\Rightarrow z_i&lt;-\frac{\lambda}{L} \end{align*}\\" eeimg="1"> </p><p data-pid="eKVG351-">当 <img src="https://www.zhihu.com/equation?tex=x_i%3D0" alt="x_i=0" eeimg="1"> 取得最小值时，即 <img src="https://www.zhihu.com/equation?tex=x%5E%7B%28k%2B1%29%7D%3D0" alt="x^{(k+1)}=0" eeimg="1"> ，在 <img src="https://www.zhihu.com/equation?tex=x_i%3C0" alt="x_i&lt;0" eeimg="1"> 时导数非正， <img src="https://www.zhihu.com/equation?tex=x_i%3E0" alt="x_i&gt;0" eeimg="1"> 时导数非负， <img src="https://www.zhihu.com/equation?tex=x_i%3D0" alt="x_i=0" eeimg="1"> 处导数不存在。</p><p data-pid="-4G1GI6o"><img src="https://www.zhihu.com/equation?tex=%5Cleft%5C%7B%5Cbegin%7Baligned%7D+L%28x_i-z_i%29-%5Clambda%5Cleq+0%5C%5C+L%28x_i-z_i%29%2B%5Clambda%5Cgeq0%5C%5C+x_i%3D0%5C%5C+%5Cend%7Baligned%7D+%5Cright.%5CRightarrow+%7Cz_i%7C%5Cleq%5Cfrac%7B%5Clambda%7D%7BL%7D%5C%5C" alt="\left\{\begin{aligned} L(x_i-z_i)-\lambda\leq 0\\ L(x_i-z_i)+\lambda\geq0\\ x_i=0\\ \end{aligned} \right.\Rightarrow |z_i|\leq\frac{\lambda}{L}\\" eeimg="1"> </p><p data-pid="qqz3ZzXx">这里要注意虽然极小值点的左导数和右导数不相等，但不代表它们都不能为0，只不过不能同时取到0罢了。例如函数 <img src="https://www.zhihu.com/equation?tex=y%3Dx%5E2%2B10%7Cx-5%7C" alt="y=x^2+10|x-5|" eeimg="1"> 在 <img src="https://www.zhihu.com/equation?tex=x%3D5" alt="x=5" eeimg="1"> 处的左导数为0，其图像如下：</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-3ad4f7a6c3fd36087f96a0a3dd411a25_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="1992" data-rawheight="1084" class="origin_image zh-lightbox-thumb" width="1992" data-original="https://pic1.zhimg.com/v2-3ad4f7a6c3fd36087f96a0a3dd411a25_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1992'%20height='1084'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1992" data-rawheight="1084" class="origin_image zh-lightbox-thumb lazy" width="1992" data-original="https://pic1.zhimg.com/v2-3ad4f7a6c3fd36087f96a0a3dd411a25_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-3ad4f7a6c3fd36087f96a0a3dd411a25_720w.jpg?source=d16d100b"></figure><p data-pid="eL06653s">综上所述我们得到迭代公式如下</p><p data-pid="s9VJZO2F"><img src="https://www.zhihu.com/equation?tex=x_i%5E%7B%28k%2B1%29%7D%3D%5Cleft%5C%7B+%5Cbegin%7Baligned%7D+z_i-%5Clambda%2FL%26%2C+%5Cquad+z_i%3E%5Clambda%2FL%5C%5C+0%26%2C%5Cquad+%7Cz_i%7C%5Cleq+%5Clambda%2FL%5C%5C+z_i%2B%5Clambda%2FL%26%2C+%5Cquad+z_i%3C-%5Clambda%2FL%5C%5C+%5Cend%7Baligned%7D+%5Cright.%5C%5C" alt="x_i^{(k+1)}=\left\{ \begin{aligned} z_i-\lambda/L&amp;, \quad z_i&gt;\lambda/L\\ 0&amp;,\quad |z_i|\leq \lambda/L\\ z_i+\lambda/L&amp;, \quad z_i&lt;-\lambda/L\\ \end{aligned} \right.\\" eeimg="1"> </p><p data-pid="H6vTI0ED">这个公式也常被称作迭代软阈值函数。我们可以发现，当 <img src="https://www.zhihu.com/equation?tex=z_i" alt="z_i" eeimg="1"> 的值超出最小负值和最大正值的区间范围时，会被逐渐地拉回到区间内，而一旦进入区间，则该权重的值被设置为0。那么什么时候迭代收敛到一个稳定状态呢？当 <img src="https://www.zhihu.com/equation?tex=+z_i%3E%5Clambda%2FL" alt=" z_i&gt;\lambda/L" eeimg="1"> 时，如果 <img src="https://www.zhihu.com/equation?tex=x_i" alt="x_i" eeimg="1"> 取值保持不变，</p><p data-pid="Gdp_vskY"><img src="https://www.zhihu.com/equation?tex=x_i%5E%7B%28k%2B1%29%7D%3Dz_i-%5Clambda%2FL%3Dx_i%5E%7B%28k%29%7D-%5Cfrac%7B1%7D%7BL%7D%5Cnabla+f%28x%5E%7B%28k%29%7D%29_i-%5Clambda%2FL%3Dx_i%5E%7Bk%7D%5C%5C" alt="x_i^{(k+1)}=z_i-\lambda/L=x_i^{(k)}-\frac{1}{L}\nabla f(x^{(k)})_i-\lambda/L=x_i^{k}\\" eeimg="1"> </p><p data-pid="YakydbHx">解得</p><p data-pid="Lxv-eYde"><img src="https://www.zhihu.com/equation?tex=%5Cnabla+f%28x%5E%7B%28k%29%7D%29_i%3D-%5Clambda%2C%5Cquad+x%5E%7B%28k%29%7D_i%3E0%5C%5C" alt="\nabla f(x^{(k)})_i=-\lambda,\quad x^{(k)}_i&gt;0\\" eeimg="1"> </p><p data-pid="XGOdozPD">同理，当 <img src="https://www.zhihu.com/equation?tex=+z_i%3C-%5Clambda%2FL" alt=" z_i&lt;-\lambda/L" eeimg="1"> 时，</p><p data-pid="8QRI9lrA"><img src="https://www.zhihu.com/equation?tex=%5Cnabla+f%28x%5E%7B%28k%29%7D%29_i%3D%5Clambda%2C%5Cquad+x%5E%7B%28k%29%7D_i%3C0%5C%5C" alt="\nabla f(x^{(k)})_i=\lambda,\quad x^{(k)}_i&lt;0\\" eeimg="1"> </p><p data-pid="MBrjpqe8">亦或 <img src="https://www.zhihu.com/equation?tex=x_i" alt="x_i" eeimg="1"> 取到0时停止在0处。所以我们可以发现网络收敛时，最终各个权重向量的梯度的各个分量的绝对值应该在 <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"> 附近，或者权重值为0。我们知道在极值点处的梯度值趋于0，所以惩罚项的比率值 <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"> 的取值很关键。如果取值过大，网络很可能因为加入L1正则项反而无法达到准确率的要求。这里我们使用<a href="http://link.zhihu.com/?target=http%3A//playground.tensorflow.org/" class=" wrap external" target="_blank" rel="nofollow noreferrer">Tensorflow — Neural Network Playground</a>验证一下这个结论。</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-ff2b13661624327ca03e7fff1d16085a_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="2560" data-rawheight="1326" class="origin_image zh-lightbox-thumb" width="2560" data-original="https://picx.zhimg.com/v2-ff2b13661624327ca03e7fff1d16085a_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='2560'%20height='1326'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="2560" data-rawheight="1326" class="origin_image zh-lightbox-thumb lazy" width="2560" data-original="https://picx.zhimg.com/v2-ff2b13661624327ca03e7fff1d16085a_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-ff2b13661624327ca03e7fff1d16085a_720w.jpg?source=d16d100b"></figure><p data-pid="LzyIlARu">在 <img src="https://www.zhihu.com/equation?tex=%5Clambda%3D0.01" alt="\lambda=0.01" eeimg="1"> 时如图所示的网络时常无法得到有效的分类界面，如果设置 <img src="https://www.zhihu.com/equation?tex=%5Clambda%3D0.03" alt="\lambda=0.03" eeimg="1"> 整个网络的权重值将全部停滞在0处。但是如果设置 <img src="https://www.zhihu.com/equation?tex=%5Clambda%3D0.001" alt="\lambda=0.001" eeimg="1"> 可以发现网络很快收敛且有效。</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-43c97001cc67082f9c83b605f26d4b2b_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="2560" data-rawheight="1328" class="origin_image zh-lightbox-thumb" width="2560" data-original="https://pic1.zhimg.com/v2-43c97001cc67082f9c83b605f26d4b2b_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='2560'%20height='1328'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="2560" data-rawheight="1328" class="origin_image zh-lightbox-thumb lazy" width="2560" data-original="https://pic1.zhimg.com/v2-43c97001cc67082f9c83b605f26d4b2b_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-43c97001cc67082f9c83b605f26d4b2b_720w.jpg?source=d16d100b"></figure><p data-pid="Eh7KTIeI">有趣的是，不同的激活函数对这个因子的敏感程度不同。由于tanh和sigmoid函数都存在梯度消失的问题，如果训练启动后梯度值很快减小，而 <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"> 设置的较大，则权重值很快掉入一个很大的区间陷阱中，从而停滞在0处。可以发现，sigmoid的梯度值更小一些，所以要求的 <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"> 需要更小一些。在这个例子中 <img src="https://www.zhihu.com/equation?tex=%5Clambda%3D0.001" alt="\lambda=0.001" eeimg="1"> 时网络仍然不能稳定得到结果。「Tip：收敛速度一般来说ReLU&gt;tanh&gt;sigmoid，所以使用sigmoid时可以将学习率调大一点。」下图给出了tanh和sigmoid函数的导数值对比。</p><figure data-size="normal"><noscript><img src="https://picx.zhimg.com/v2-9e5c0019a78172a7b50d6753695e9d04_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="994" data-rawheight="551" class="origin_image zh-lightbox-thumb" width="994" data-original="https://picx.zhimg.com/v2-9e5c0019a78172a7b50d6753695e9d04_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='994'%20height='551'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="994" data-rawheight="551" class="origin_image zh-lightbox-thumb lazy" width="994" data-original="https://picx.zhimg.com/v2-9e5c0019a78172a7b50d6753695e9d04_720w.jpg?source=d16d100b" data-actualsrc="https://picx.zhimg.com/v2-9e5c0019a78172a7b50d6753695e9d04_720w.jpg?source=d16d100b"></figure><p data-pid="fR1qR1I4">绘制上图使用的是TensorFlow的下一代框架JAX「JAX is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research.」它多数情况下可以看做高级版Numpy，支持autograd自动求导。下面给出了绘制这个图像的具体程序：</p><div class="highlight"><pre><code class="language-python"><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">grad</span>
<span class="n">d_tanh</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
<span class="n">d_sigmoid</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">d_tanh</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">"d_tanh"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">d_sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">"d_sigmoid"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p data-pid="9Aq4gG9D">如果使用ReLU激活函数，在 <img src="https://www.zhihu.com/equation?tex=%5Clambda%3D0.01" alt="\lambda=0.01" eeimg="1"> 时基本上可以稳定得到结果。而且可以直观地体会到L1正则化带来的权重的稀疏性：</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-489323eedfbff87286fc7c0d724aac7b_720w.jpg?source=d16d100b" data-caption="" data-size="normal" data-rawwidth="1243" data-rawheight="685" class="origin_image zh-lightbox-thumb" width="1243" data-original="https://picx.zhimg.com/v2-489323eedfbff87286fc7c0d724aac7b_720w.jpg?source=d16d100b"></noscript><img src="data:image/svg+xml;utf8,&lt;svg%20xmlns='http://www.w3.org/2000/svg'%20width='1243'%20height='685'&gt;&lt;/svg&gt;" data-caption="" data-size="normal" data-rawwidth="1243" data-rawheight="685" class="origin_image zh-lightbox-thumb lazy" width="1243" data-original="https://picx.zhimg.com/v2-489323eedfbff87286fc7c0d724aac7b_720w.jpg?source=d16d100b" data-actualsrc="https://pic1.zhimg.com/v2-489323eedfbff87286fc7c0d724aac7b_720w.jpg?source=d16d100b"></figure><p data-pid="HkSUCaE2">可以看到第一个隐藏层可以剪掉一个神经元，第二个隐藏层可以剪掉三个神经元，这可以使神经网络的参数量大幅压缩。可以尝试加宽和加深隐藏层，会发现剪枝的结果几乎很稳定，第一层大概保留三到四个神经元，第二层以后都是一个神经元。越往后的层次冗余信息越多这个是已知的结论。但一个神经网络的结构或者说权重的数量和其表征能力是否有确定的函数关系呢？如果能够找到这个关系，就能论证网络压缩的下限，这将具有巨大的意义。</p><h2>结语</h2><p data-pid="m6y_hAhF">含有L1正则项的公式可以使用近端梯度下降法进行优化，倒也不是非常复杂。L1正则化会导致稀疏参数。L1正则项的因子 <img src="https://www.zhihu.com/equation?tex=%5Clambda" alt="\lambda" eeimg="1"> 不能取得太大。想要了解更多严谨的数学概念，可以参考：</p><ul><li data-pid="ZM7oGhsE"><a href="http://link.zhihu.com/?target=https%3A//blog.csdn.net/qq_38290475/article/details/81052206" class=" wrap external" target="_blank" rel="nofollow noreferrer">近端梯度下降算法(Proximal Gradient Algorithm)</a></li><li data-pid="OXigBn9z"><a href="https://zhuanlan.zhihu.com/p/82622940" class="internal" target="_blank">机器学习 | 近端梯度下降法 (proximal gradient descent)</a></li></ul><p></p>