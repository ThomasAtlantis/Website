<div>pytorch加载模型的隐藏小bug<br/>相信很多人像我一样，save模型的时候直接把model的state_dict传过去，在load的时候直接load。这有个隐患。<br/><br/>训练模型和加载模型的时候可能用的不是同一张显卡。load函数默认从存储时候的设备进行加载，于是可能在卡0空闲、卡1爆满的情况下向卡0加载模型仍然爆显存。<br/><br/>简单有效的办法就是在load模型时，加上参数map_location，将其设为目标显卡。注意load函数并不会由于和save时的设备不同而报设备不匹配的错误，尽管在load前可能移动过设备。 <a data-pin-topic="zhihu://topic/20075993/pin20" class="hash_tag" href="https://www.zhihu.com/topic/20075993">#PyTorch</a> <a class="hash_tag" href="https://www.zhihu.com/topic/20106070" data-pin-topic="zhihu://topic/20106070/pin20">#踩坑</a> <a data-pin-topic="zhihu://topic/19559450/pin20" class="hash_tag" href="https://www.zhihu.com/topic/19559450">#机器学习</a> </div>