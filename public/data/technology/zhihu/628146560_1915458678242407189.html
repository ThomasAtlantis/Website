<p data-pid="U8_sgnkx">二者区别非常大，完全不是一个领域。</p><p data-pid="d1-MTYcz">建议对比两份非常清晰易懂的 Tutorial，一个关于生成式信息检索 <a href="https://link.zhihu.com/?target=https%3A//generative-ir.github.io/" class=" wrap external" target="_blank" rel="nofollow noreferrer">SIGIR 2024 Tutorial: Recent Advances in Generative Information Retrieval</a>，另一个关于检索增强生成 <a href="https://link.zhihu.com/?target=https%3A//acl2023-retrieval-lm.github.io/" class=" wrap external" target="_blank" rel="nofollow noreferrer">ACL 2023 Tutorial: Retrieval-based Language Models and Applications</a>，看一下 introduction 就够了。下面我总结了一下我的理解。</p><p data-pid="NfoRnzUJ"><b>生成式检索</b>（Generative Information Retrieval，GenIR）目的在于把检索的多个步骤——包括文档的解析、索引、召回、重排和查询的增强等——合并为一个端到端的 seq2seq 生成模型，输入查询语句，输出检索得到的相关文档 ID 集合，按相关性降序排列。</p><p data-pid="aMuMbPSj">这样做的初衷，一是可以联合优化各个模块，二是在内存和延迟上更高效。传统检索方法在内存中加载并驻留索引，比如倒排索引表，对于大规模文档十分吃力。而 GenIR 只需要驻留一个模型，这个模型是一个检索的 Agent，它记住了哪篇文档大概讲了什么。</p><p data-pid="NErn44FW">生成式检索的生成式意味着，模型对输出解码时将在词表（包含所有 docID 的全集）的概率分布上采样，而与之相对的判别式模型，是用 bi-encoder 把文档和查询映射到相同的 space 上再做匹配，根据匹配分数取 top-k。</p><p data-pid="oSxKwNIp">这里的生成甚至可以是自回归的，比如用 URL 做文档的 docID 时，可以对其进行分词，然后逐个 token 地输出。这时词表就不再是完整的 docID 而是 docID 拆解出的更小单元。关于 docID 如何设计有大量的文章讨论：使用什么形式，是数字还是单词？如何构造具有语义相关性的结构化的数字 ID？如何选择单词，是文档标题、URL还是关键词？生成过程如何保证解码拼接的 docID 一定合法？</p><p data-pid="H2AWEn8n"><b>检索增强生成</b>（Retrieval-Augmented Generation，RAG）目的在于辅助生成模型，增强其在对话过程中的 factual consistency，通俗地讲就是让模型引经据典，不要满嘴跑火车。由于其根本目的在于生成而不是检索，所以 RAG 的检索系统绝大多数情况下都是很松散的传统模式（有点像插件，和 GenIR 合而为一的特点相反），先用查询检索文档，然后对得到的文档重排，再将重排后的结果融合到回答的生成过程。</p><p data-pid="KhkQGp-F">输出的不同也就决定着 RAG 和 GenIR 的训练方式不同，RAG 里很多情况下，检索器和重排器和传统 IR 区别不大，最多就是和生成模型这部分一起联合优化一下。</p><p data-pid="Wjnm2cbu">总的来说，GenIR 和 RAG 的生成内容不同，前者输出检索结果的文档编号，后者还要利用检索到的文档总结提炼回答内容。我们称前者的生成过程是 closed-book 的，即生成过程不再参考外界信息，对文档的记忆全靠离线训练，更像是与 RAG 相对的 SFT 路线；后者是 open-book 的，即照着检索得到的材料做阅读理解。当然，两者的模型架构可以有交集，比如都用 LLaMA，因为都是 seq2seq 这种模式。我们也可以把两者结合，在 RAG 的检索阶段使用 GenIR，或者先用大模型生成 statement，再用 GenIR 做 fact verification、attribution 之类。</p><p data-pid="-YGC761U">这两个方向投稿的会议大概也有差别，GenIR 估计是投 SIGIR、KDD、WWW、CIKM、WSDM之类的，RAG 主要投 AI 三大会，当然也可以投数据挖掘，投 IR 的少一些。两个方向当然都可以投 ACL、EMNLP 等 NLP 会议。</p>